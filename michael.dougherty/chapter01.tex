\chapter{Mathematical Logic and Sets\label{LogicChapter}}

In this chapter we introduce symbolic logic and set theory.
These are not specific to calculus, but are shared among
all  branches of mathematics.  There are various
symbolic logic systems, and indeed mathematical logic is its
own branch of mathematics, but here we look at that portion
of mathematical logic which should be understood by any
professional mathematician or advanced student.  The set theory is a natural
extension of logic, and provides further useful notation
as well as some interesting insights of its own.

The importance of logic to mathematics cannot be overstated.\footnote{
%%% FOOTNOTE
In fact Bertrand Russell (1872--1970)---one of the greatest mathematicians of
the twentieth century---argued successfully that 
mathematics and logic are exactly the same discipline.
Indeed, they seem to be supersets of
each other, implying they are the same sets.
It just happens that to many a lay person, mathematics may be
associated only with numbers and computations while
logic deals with argument. The field of geometry 
belies this categorization, but there are
many other vast mathematical disciplines which are
not so interested in our everyday number systems.
These include graph theory (useful for network design and analysis),
topology (used to study surfaces, relativity), and abstract algebra 
(used for instance in coding
theory) to name a few.  Indeed both mathematics and
logic can be defined as interested in abstract, coherent 
structural systems.  Thus, to a modern mathematician,
logic-versus-mathematics may be considered a ``distinction without
a difference.''
%%% END FOOTNOTE
}
No conjecture in mathematics is considered fact until it
has been logically proven, and  truly valid mathematical analysis
is done only within the rigors of logic.
Because of this dependence,  mathematicians have
carefully developed and formalized logic beyond some of the 
murkier ``common sense" we learn from childhood,
and given it the precision required to explore,
manipulate and communicate mathematical ideas unambiguously.  
Part of that development is the codification
of mathematical logic into symbols.
With logic symbols and their rules for use, we can analyze and rewrite
complicated logic statements much like we do with algebraic statements.




Symbolic logic is a powerful tool for analysis and communication,
but we will not abandon written English altogether.  In fact, most
of our ideas will be expressed in sentences which mix 
English with mathematical expressions including symbolic logic.
We will strive for a pleasant style of mixed prose, but we
will always keep in mind the formal logic upon which we base our arguments,
and resort to the symbolic logic when the logic-in-prose is complicated
or can be illuminated by a symbolic representation. 


Because we will use English phrases as well as symbolic logic,
it is important that we clarify exactly what we mean by
the English versions of our logic statements.
Part of our effort in this chapter is devoted toward that end.





The symbolic language developed here is used throughout the text. 
It is descriptive and precise, and 
learning its correct use forces clarity in thinking and 
presentation.  It is not common for a calculus textbook to 
include a study of logic, since authors have more than
enough to accomplish in trying to offer a respectably complete
treatment of the calculus itself.  However, it is quite
common for teachers and professors to insert some of the logic
notation  into the class lectures because of its
usefulness for presenting and explaining calculus to students.
Unfortunately a casual or ``on the fly'' introduction to these devices
can cause as many problems as it solves.  %\footnote{%
%%% FOOTNOTE
%Perhaps the most common and frustrating error is the confusion
%of ``$\implies$'' with ``$=$.'' Unless homework is graded
%aggressively, this type of confusion can be quite persistent. 
%As we will see, ``$\implies$'' connects statements---mathematical
%equivalences of complete
%sentences---while ``$=$'' is always part of a
%statement.} 
%%% END FOOTNOTE
In this text
we will instead commit early to developing and
using the symbolic logic notation so we can take advantage
of its correct use.

We begin with the first section devoted to truth tables, which 
ultimately define our first group of logic symbols.  Next we 
consider valid logical equivalences and implications, along
with tautologies and contradictions in the second section.
Then we detour briefly in the third section
to consider basic set theory, mostly for
notation though it is not without its own insights.  
Finally we consider our last set of logic symbols,
the quantifiers, with their own analytical techniques.

%
%A whole course could easily be devoted to these two topics,
%logic and set theory, but the fundamentals are intuitive enough,
%and come in quite handy when developing the calculus.


%\newpage
\section[Logic Symbols and Truth Tables]{Logic Symbols and Truth Tables}

\begin{table}

\begin{center}\begin{tabular}{|c|l|l|l|}
\hline
Symbol &Read &Example &Also Read\\
&&&\\
\hline
$\sim$&not&$\sim P$&$P$ is not true\\
&&&$P$ is false\\ 
\hline
$\wedge$&and&$P\wedge Q$&$P$ and $Q$
\\ &&&both $P$ and $Q$ are true\\
\hline
$\vee$&or&$P\vee Q$&$P$ or $Q$  \\
&&&$P$ is true  or $Q$ is true (or both)\\ 
\hline

$\ds{\longrightarrow}$ &implies &$P\longrightarrow Q$
&if $P$ then $Q$\\ 
&&&$P$ only if $Q$\\
\hline
$\ds{\longleftrightarrow}$ & if and only if   
        &$P\longleftrightarrow Q$ &$P$ bi-implies $Q$\\
&&&$P$ iff $Q$\\
%\hline
%$\forall$&for all&$\forall K\in\Re$&for all real numbers $K$\\ 
%$\exists$&there exists&$\exists x\in\Re$&there exists a real number $x$\\
%\hline
%
\hline\end{tabular}\end{center}
\caption{Some basic logic notation.}
\label{logicnotation}\end{table}



The first logic symbols we develop in the text are listed in
Table~\ref{logicnotation}.
In what follows we will 
explain their meanings and give
their English versions, while also pointing out
where casual English interpretations often differ,
from each other as well as their formal meanings.
It is useful to learn to read the symbols above as they would
usually be said out loud.  For instance, $P\wedge Q$ can be read,
``$P$ and $Q$,'' while $P\longrightarrow Q$ is usually  
read ``$P$ implies $Q$.''\footnote{%
%%% FOOTNOTE
Occasionally some of these are
verbalized using what amounts to their typographical
descriptions, so for instance $P\wedge Q$ becomes ``$P$ wedge $Q$,''
while $P\vee Q$ becomes ``$P$ vee $Q$.''}
One reads $\sim P$ as ``not $P$,'' while more elaborate
means for verbalizing, say, $\sim(P\vee Q)$ would include
``it is not the case that $P$ or $Q$.''
In fact, if $P$ is the statement that it is raining, 
then  we can graft the words ``it is the case that'' and
have a new statement with exactly the same meaning:
``it is the case that it is raining.''  This allows us more
flexibility to read negations in a more natural order:
$\sim P$ becomes ``it is not the case that it is raining.''

Now we look again at the symbols in Table~\ref{logicnotation}.
The symbol $\sim$ is called a {\it unary logic operation}
because it operates on one (albeit possibly compound) statement,
say $P$.
 The symbols $\vee,\wedge,\longrightarrow,
\longleftrightarrow$ are called {\it connectives} or
{\it binary logic operations}, connecting two
statements, such as $P$, $Q$. 
Both types will be developed in detail
in this chapter.
%The symbols $\forall$ and $\exists$ are called {\it quantifiers},
%and will be explored more in Section~\ref{SWQ}.  
%We will also use ``q.e.d." (Latin, {\em quod erat
%demonstrandum}), the traditional ending of a proof meaning
%{\em that which was to be proved}. In this section we will
%take just a first look at these symbols in action.
%(In later sections we will add $\equiv$, $\iff$, $\implies$ and
%$!$.)


\subsection{Lexicographical Listings of Possible Truth Values}

In the next subsection we  develop the logic operators listed previously
in  Table~\ref{logicnotation}, page~\pageref{logicnotation}.
These operators connect {\it statements}, such as
$P$, $Q$, etc., forming new, {\it compound} statements
$P\longrightarrow Q$, $\sim P$, $P\wedge Q$,
etc.  In doing so, we analyze the truth or falsity of the compound
statements based upon the truth or falsity of the
underlying, {\it component}  statements $P$, $Q$, etc.\footnote{%%
%% FOOTNOTE
In our analyses, the {\it component} statements will consist of
single letters $P$, $Q$, and so on, 
and be allowed truth value T or F.  {\it Compound} statements
are not necessarily allowed either truth value, but their truth
values are determined by those of the underlying component statements.
For instance, we will see $P\vee(\sim P)$ can only have 
truth value T, and $P\wedge(\sim P)$ can have only F,
while $P\longrightarrow Q$ is sometimes T, sometimes F.%%
%%% END FOOTNOTE
}

We always assume a
particular statement can be either true or false, but
not simultaneously both.\footnotemark
\footnotetext{This is sometimes referred to as the ``law of the
excluded middle."  It is useful in future discussions since it
is often easier to prove $P$ is not false (i.e., ($\sim P$) is false)
 than to prove $P$ is true.}
We signify these possibilities by the {\it truth values,} T or F,
respectively.  Note that for $n$ independent
statements $P_1,\cdots,P_n$ there are
$2^n$ different combinations of T and F.\footnote{%%%
%%% VERY LONG FOOTNOTE
This is a simple counting principle.  For another
example suppose we have 
four shirts and three pairs of pants, and
we want to know how many different combinations
of these we can wear,
assuming we will wear exactly one shirt and one pair of pants.  Since we can
wear any of the shirts with any of the pants,
the choices---for counting purposes---are independent.
We have four choices of shirts, and for each of those we have
three choices of pants.
It is not difficult to see that we have $4\cdot3=12$ possible
combinations to choose from.

If we also include two choices of belts, and assume we will wear exactly
one belt, then we have $4\cdot3\cdot2=24$ possible combinations
of shirt, pants, and belt.

Here, there are 2 choices for the truth values (T or F) of 
each of the $P_1,\cdots,P_n$, so there are $2^n$ possible
truth-value combinations.

Whole textbooks are written regarding this and other
counting principles, but in this text
we will only encounter a few.  For undergraduates, some of these principles
are often found embedded in probability courses or courses relying upon
probability such as genetics, or in combinatorics which appears especially 
in computer sciences and electrical engineering. 
%%% END VERY LONG FOOTNOTE
} 
Thus for a single statement $P$, we have $2^1=2$ truth value
possibilities, T or F.
For two independent statements $P$ and $Q$, we have
$2^2=4$ possible combinations of truth values:
TT,TF,FT,FF, i.e., $P$ and $Q$ both true, $P$ true and $Q$
false, $P$ false and $Q$ true, or $P$ and $Q$ both false.
For three statements $P,Q,R$, the possibilities are $2^3=8$-fold.
To list exhaustively all possible orders, we will employ
a {\it lexicographical order, }as shown in 
Figure~\ref{LexicographicalOrdersFor1,2,3StatementsFigure}.
If there are $n\ge2$ independent statements, then 
for the first we write half ($2^{n-1}$) T's and the same number of F's.
For the next statement we write half ($2^{n-2}$) T's, and the same number
of F's, and then repeat.  If there is a third, we simply
alternate T's with F's twice as fast, i.e., $2^{n-3}$ T's, as
many F's, and then repeat following that pattern until we fill out $2^n$
entries.  The last statement's entries are TFTF$\cdots$TF,
until $2^n$ entries are made. 
Figure~\ref{LexicographicalOrdersFor1,2,3StatementsFigure}
illustrates this pattern, for $n=1,2,3$

\begin{figure}
\begin{center}
\begin{tabular}{|c|}\hline
$P$\\
\hline
T\\
F\\\hline\end{tabular}\qquad\qquad
\qquad \begin{tabular}{|c|c|}\hline
$P$& $Q$\\
\hline
T&T\\
T&F\\
F&T\\
F&F\\\hline \end{tabular}\qquad\qquad\qquad
\begin{tabular}{|c|c|c|}
\hline
$P$&$Q$&$R$\\
\hline
T&T&T\\
T&T&F\\
T&F&T\\
T&F&F\\
\hline
F&T&T\\
F&T&F\\
F&F&T\\
F&F&F\\
\hline
\end{tabular}\end{center}

\caption{Lexicographical ordering of the possibilities for
one, two, or three independent statements' truth values.  The extra 
horizontal line in the third table is for ease of reading only.}
\label{LexicographicalOrdersFor1,2,3StatementsFigure}
\end{figure}

\newpage
\subsection{The Logic Operations}

\begin{table}
\begin{center}

\begin{tabular}{|c||c|}
\hline
$P$&$\sim P$\\ \hline
T&F\\F&T\\ \hline\end{tabular}
\qquad\qquad\qquad
\begin{tabular}{|c|c||c|c|c|c|}
\hline
$P$&$Q$&$P\wedge Q$&$P\vee Q$&$P\rightarrow Q$&$P\leftrightarrow Q$\\
\hline
T&T&T&T&T&T\\
T&F&F&T&F&F\\
F&T&F&T&T&F\\
F&F&F&F&T&T\\
\hline
\end{tabular}
\end{center}
\caption{The basic logic operations defined for all possible 
truth values of their arguments}
\label{TalbeOfAll5LogicOperations}\end{table}

The basic five logic operations we will use in this text are
given in Table~\ref{TalbeOfAll5LogicOperations} for every possible truth
value of underlying component statements.  We say that the operation
$\sim$ takes one {\it argument} (not to be confused with the colloquial
meaning of the term), that argument being $P$
in the table above.  The other operators $\wedge$,
$\vee$, $\rightarrow$ and $\leftrightarrow$ each take two arguments,
which for the table above we dub $P$ and $Q$.





We begin with the logical negation $\sim$, which is a unary operation,
i.e., acting on one (possibly compound)
statement.  For example consider the statement $\sim P$, usually read
``not $P$.''
This is the negation
of the statement $P$.  
Of course $\sim P$ is not independent of $P$, but its 
truth value is based upon that of $P$;
stating that $\sim P$ is true
is the same as stating that $P$ is false, and
stating that $\sim P$ is false is the same
as stating that $P$ is true.  We can completely
describe the relationship between $P$ and $\sim P$
in the following truth table diagram:\footnote{%%
%%% FOOTNOTE
We will always use
double lines to separate the independent component statements
$P$, $Q$, etc., from  compound statements based upon them.
%%% END FOOTNOTE
}

\medskip

\begin{center}\begin{tabular}{|c||c|}\hline
$P$& $\sim P$\\
\hline
T&F\\
F&T\\\hline\end{tabular}\end{center}
\medskip

\noindent This also completely describes the action of the operation $\sim$:
it takes a statement with truth value T and returns a statement
with truth value F, and vice-versa.  For an English example,
if we define a statement $P$ by
$$P:\ \text{\em I will go to the store,}$$
then the resultant statement for $\sim P$ is simply
$$\sim P:\ \text{I will not go to the store.}$$
We can also read $\sim$ as ``it is not the case that,''
so our example above could read
$$\sim P:\ \text{It is not the case that I will go to the store.}$$
Even in an English example as above,
it is not difficult to see that $P$ is true exactly when
$\sim P$ is false, and $P$ is false exactly when $\sim P$
is true.  For truth value computations, we can summarize
the action of $\sim$ as follows:
 
\begin{center}{\sl
$\sim$ switches the truth value of the statement on which it operates,
from {\rm T} to {\rm F} or from {\rm F} to {\rm T}.
}
\end{center}

It will be interesting to see how $\sim$ operates on compound
statements as we proceed.
How it interacts ``with itself'' is rather 
straightforward.  Indeed, it is not difficult to see
that the statement $\sim(\sim P)$ is the same as the 
statement $P$.  We might read
$$\sim(\sim P):\ \text{It is not the case that it is not 
the case that I will go to the store.}$$
Perhaps a better English translation of  $\sim(\sim P)$ here
would be, ``it is not the case that I will not go to the store,''
which clearly states that I will go to the store, i.e., $P$.
In the next section we will look at ways to {\it calculate} when
two logic statements in fact mean the same thing, such
as $P$ and $\sim(\sim P)$.\footnote{%
%%% FOOTNOTE
It will be taken for granted throughout the text that the reader
has some familiarity with the use of parentheses (\,), 
brackets [\,], and similar devices for grouping quantities---logical,
numerical, or otherwise---to be treated as single quantities.
For instance, $\sim(\sim P)$ means that the ``outer'' (or first)
$\sim$ will operate  on the statement $\sim P$, treated as a single,
albeit ``compound'' statement.  Thus we first find $\sim P$, and then
its logical negation is $\sim(\sim P)$.  This type of device is
used throughout the chapter and the rest of the text.%%
%%% END FOOTNOTE
}





We next turn our attention to the binary operation
$\wedge$.  This is called the logical {\it conjunction}, or just
simply {\it and}:  the statement $P\wedge Q$ is usually
read ``$P$ and $Q$.''  This compound statement $P\wedge Q$ is
true exactly when both $P$ and $Q$ are true, and false if a
component statement is false.  Thus its truth table is
given by the following:
\medskip

\begin{center}
\begin{tabular}{|c|c||c|}\hline
$P$& $Q$& $P\wedge Q$\\
\hline
T&T&T\\
T&F&F\\
F&T&F\\
F&F&F\\\hline \end{tabular}
\end{center}
\medskip

\noindent As an operation, $\wedge$ returns T if both statements
it connects have truth value T, and returns F otherwise, i.e., if
either of the statements connected by $\wedge$ is false.
\bex Suppose we set $P$ and $Q$ to be the statements
\begin{align*}
P:&\ \text{I will eat  pizza,}\\
Q:&\ \text{I will drink  soda.}
\end{align*}
Connecting these with $\wedge$ gives
$$P\wedge Q:\ \text{I will eat pizza and I will drink soda.}$$
This is true exactly when I do both, eat pizza and drink soda,
and is false if I fail to do one, or the other, or both.
\label{PWedgeQDiscussion}
\eex
%It is always instructive to observe when a given logical statement
%is false.  For instance, $P\wedge Q$ is false 
%exactly when $P$ or $Q$ (or both) is false.  For these
%particular $P,Q$ we have $P\wedge Q$ false if I fail
%to eat a cheeseburger or I fail to eat fries (or both).
%Indeed, in the next section we will show how to prove
%that $\sim(P\wedge Q)$ is logically the
%same as $(\sim P)\vee(\sim Q)$.\footnotemark
%%
%
%\footnotetext{%
%%% FOOTNOTE
%The above example hints at a difference between
%symbolic logic and even standard English: the use of parentheses (,).
%In mathematics they---along with brackets $[,]$ and
%sometimes braces $\{,\}$---are usually used for grouping
%several objects to be treated as a single object,
%whereas in English parentheses are for remarks interlaced in
%the flow of the text.
%In written English we can group phrases to be considered as
%units by using quotation marks, as in
%\begin{quote}
%{\it It is not the case that ``I will eat a cheeseburger and
%I will eat fries.''}
%\end{quote}
%Without the quotes it is not clear what the modifier
%{\it not the case} actually modifies.  We get a very
%different statement if we instead write (for instance)
%\begin{quote}
%{\it It is not the case that I will eat a cheeseburger,
%and I will eat fries.}
%\end{quote}   
%In mathematics and logic we are not limited in how 
%many levels we can ``nest'' statements.  In fact,
%it is not uncommon to have statements arise
%with levels of complexity as great or greater than the following:
%$$(P\vee Q)\wedge(\sim(P\wedge Q))\wedge[(\sim(P\vee Q))
%\longrightarrow R].$$
%In English we are usually limited to at most two levels
%of nesting, using double and single quotes, as in
%\begin{quote}
%{\it Mark said to me, ``I heard Sue tell the manager, `I am quitting tomorrow'
%just before Sue left work.''}
%\end{quote}
%%% END FOOTNOTE
%}

Next we look at the binary operation $\vee$, called 
the logical {\it disjunction}, or simply {\it or}.
The statement $P\vee Q$ is usually read ``$P$ or $Q$.''
For $P\vee Q$ to be true we only need one of the
underlying component statements to be true; for $P\vee Q$ to
be false we need both $P$ and $Q$ to be false.
The truth table for $P\vee Q$ is thus as follows:


\begin{center}
\medskip

\begin{tabular}{|c|c||c|}\hline
$P$& $Q$&  $P\vee Q$\\
\hline
T&T&T\\
T&F&T\\
F&T&T\\
F&F&F\\\hline \end{tabular}
\medskip
\end{center}

It is important to note that $P\vee Q$ is not an
{\it exclusive or},\footnotemark
\footnotetext{The case where we have $P$ or $Q$ but not both
is called an {\it exclusive or}.  Computer scientists and
electrical engineers know this as {\bf XOR}. For
our purposes $\vee$ will suffice, and anyhow is much simpler
to deal with computationally in symbolic logic manipulations.
{\bf XOR} will appear occasionally in exercises.\label{FootnoteXOR}}
so we still take $P\vee Q$ to be true for the case that both
$P$ and $Q$ are true.
At times it is not interpreted this way in spoken English, but
our standard for a statement being false, i.e., having truth
value F, is that it is in fact contradicted.  If
I state ``$P$ or $Q$,'' to a logician I am only taken to be lying if both
$P$ and $Q$ are false.

\bex For the $P$ and $Q$ from the previous
example, we have
$$P\vee Q:\ \text{I will eat pizza or I will drink soda.}$$
Again, this is still true if I do both, eat pizza and drink soda,
or just do one of these;  it is sufficient that one be true,
but it is not contradicted if both are true.  Note that this is
false exactly when both $P$ and $Q$ are false, i.e., for the case that I
do not eat pizza and do not drink soda.
\eex

Sometimes in spoken English the above example of $P\vee Q$ would 
be considered false if I did both, eat the pizza and drink the soda.
According to abstract logic, doing
both does not technically make the speaker a liar.  For
many reasons, symbolic logic defines the operation
$\vee$ to be inclusive, so that  $P\vee Q$ is considered
true in  the  case in which  $P$ and $Q$ are both true.\footnote{%
%%% FOOTNOTE
To see how English understanding is context-driven,
consider the following situations.
First, suppose
a parent tells the child to ``clean the bedroom or the garage'' before
dinner.  If the child does both, the parent will likely take
the request to be fulfilled. Next, suppose instead
that parent tells the child to
``take a cookie or a brownie'' after dinner, 
and the child takes one of each.
In this second context
the parent may have a very different understanding of the child's
compliance to the parental instructions.
To a logician 
(and perhaps to any self-respecting smart aleck) 
$\vee$ must be context-independent.%
%%% END FOOTNOTE
}


Next we consider $\longrightarrow$.
Arguably the most common and therefore important 
logic statements in mathematics 
are of the form $P\longrightarrow Q$,
read ``$P$ implies $Q$'' or  ``if $P$ then $Q$.''
These are also
the most misunderstood by novice mathematics students,
and so we will discuss them at length.
As before, a truth table summarizes the action of
this (binary) operation:

\begin{center}
\medskip

\begin{tabular}{|c|c||c|}\hline 
$P$ & $Q$& $P\longrightarrow Q$\\
\hline
T&T&T\\
T&F&F\\
F&T&T\\
F&F&T\\\hline\end{tabular} 

\medskip
\end{center}

\noindent
Note that the only circumstance we take $P\longrightarrow Q$ to be
false is when $P$ is true, but $Q$ is false.  As before, our
standard for falsity is when the statement is actually contradicted,
and that can be seen to be exactly when we have
the {\it antecedent} $P$, but not the {\it consequent} $Q$.
In particular, if $P$ is false, then $P\longrightarrow Q$ cannot
be contradicted, so we take those two cases to be true,
dubbing $P\longrightarrow Q$ {\it vacuously true} for those
two cases where $P$ is false.  

In summary, the connection $\longrightarrow$ returns T for all
cases {\it except} when the first statement is true, but the second is 
false.

The importance of the implication extends beyond mathematics and into
philosophy and other studies.  Because of its ubiquity, logical
implication has several syntaxes which all mean the
same to a logician.  It is interesting
to compare the various phrases, but first we will
look at an example in the same spirit as we
had for $\sim$, $\wedge$ and $\vee$.


\bex 
For the $P,Q$ in the previous examples, we have
$$P\longrightarrow Q:\ 
\text{If I will eat pizza then I will drink soda}.$$
It is useful to see when this is clearly
false: when $P$ is true but $Q$ is false, which
for these  $P,Q$ would be the case that I eat 
pizza but do not drink soda. 
In fact, it is important that that is the \,{\bf only}
case in which we consider $P\longrightarrow Q$
to be false.  In particular, if $P$ is false,
then  $P\longrightarrow Q$ is  vacuously true.
The idea is that if I do not eat pizza,
then whether or not I drink soda I do not lie
in stating that, ``If I will eat pizza
then I will drink soda.''
\eex

There are several English phrases
which mean $P\longrightarrow Q$.
Below are five  equivalent ways to write the 
corresponding English version of $P\longrightarrow Q$
for the $P,Q$ in the examples.  (That the fourth and fifth
versions are equivalent will be proved in
the next section.)
\begin{enumerate}
\item My eating pizza implies my drinking soda
   ($P$ implies $Q$).\label{SixWaysForP->Q}
\item If I will eat pizza then I will drink soda
      (if $P$ then $Q$).
\item I will eat pizza {only if} I will drink soda
   ($P$ only if $Q$).
\item I will drink soda or I will not eat pizza
   ($Q$ or not $P$).
\item If I will not drink soda, then I will not eat pizza
   (if not $Q$ then not $P$).
\end{enumerate}
These five ways of stating $P\longrightarrow Q$ might not all be
immediately obvious, and so 
are worth reflection and eventual commitment to memory.
Two other common---and rather elegant---ways
of stating the same thing are given below in 
the abstract:
\medskip

6. My drinking soda is necessary for my eating pizza 
      ($Q$ is {\it necessary} for $P$).\medskip

7. My eating pizza is sufficient for my drinking soda 
       ($P$ is {\it sufficient} for $Q$).\medskip

\noindent The kind of diction in 6 and 7
is very common in philosophical as well as mathematical discussions.
We will return to implication after next discussing bi-implication,
since a very common mistake for novice mathematics students 
is to confuse the two.


The bi-implication is denoted $P\longleftrightarrow Q$, and
often read ``$P$ if and only if $Q$.''  This is sometimes
also abbreviated ``$P$ iff $Q$''.  It states that $P$ implies
$Q$ and $Q$ implies $P$ simultaneously.  Thus truth of $P$ 
gives truth of $Q$, while truth of $Q$ would give truth of $P$.
Furthermore, if $P$ is false, then so must be $Q$, because
$Q$ being true would have forced $P$ to be true as well.  
Similarly $Q$ false 
would imply $P$ false (since if $P$ were instead true, so
would be $Q$).  The truth table for the bi-implication is the
following:
\begin{center}
\medskip

\begin{tabular}{|c|c||c|}\hline 
$P$ &$Q$ &$P\longleftrightarrow Q$\\
\hline
T&T&T\\
T&F&F\\
F&T&F\\
F&F&T\\\hline\end{tabular}
\medskip
\end{center}

An important, alternative
way to describe the operation $\longleftrightarrow$ is
to note that $P\longleftrightarrow Q$  is true exactly when 
$P$ and $Q$ have the same truth values (TT or FF).
Thus the connective $\longleftrightarrow$ can be used to 
detect when the connected statements' truth values match, and when they 
do not.  This will be crucial in the next section.

\bex Consider the statement $P\longleftrightarrow Q$ for
our earlier  $P$ and $Q$, for which we have
$$P\longleftrightarrow Q:\ 
\text{I will eat pizza if and only if I will drink soda.}
$$
This is the idea that I can not have one without the other:
if I have the pizza, I must also have the soda (``only if''),
{\bf and} I will have the pizza if I have the soda (``if'').
This is false for the cases that I have one but not the other.
Importantly it is not false if I have neither.
\label{P<->QExampleAndExplanation}\eex

In fact a bi-implication $P\longleftrightarrow Q$ is well-named as such
since it is actually the same as 
$(P\longrightarrow Q)\wedge(Q\longrightarrow P)$.
(The proof of this fact is given in the next section.)
Note that
we can switch the order of statements connected by $\wedge$ (and),
so we
can instead write 
$(Q\longrightarrow P)\wedge(P\longrightarrow Q)$, i.e.,
$Q\longleftrightarrow P$. 
In prose we can write ``$P$ is necessary and sufficient for $Q$,''
for $P\longleftrightarrow Q$, 
which is then the same as  ``$Q$ is necessary
and sufficient for $P$,'' i.e., $Q\longleftrightarrow P$.

At this point we will make a few more observations concerning
the differences between the
English and formal logic uses of terms in common.
The cases below illustrate how casual English users
are often unclear about when ``if,'' ``only if,'' or
``if and only if'' are meant in both speaking and listening.
Again,  mathematics requires absolute precision
in these things.

The first difference involves the phrase ``only if.''
This is often misunderstood to mean ``if and only if''
in everyday speech. When we combine the two words ``only if,''
the standard logic meaning is not the same as ``if'' modified
by the adverb ``only.''  Taken together, the words ``only if''
have a different, but precise meaning in logic.
Consider the following statements:
\begin{itemize}
\item You can drive that car only if there is gasoline in the tank.
\item You can drive that car only if there is air in the tires.
\item You can drive that car only if the ignition system is working.
\end{itemize}
Clearly it is not the case that you can drive that car if and
only if there is gasoline in the tank, since the gasoline
is {\it necessary but not sufficient} for running the car;
you also  need  the air, ignition, etc., or the car still will not
drive regardless of the state of the gasoline tank.
Similarly a father telling his teenaged child, 
``you can go out with your friends only if your homework 
is finished'' might justifiably find another reason to
keep the child from joining the friends even after
the homework is done. (Sudden severe weather,
inappropriate activities planned, and 
a host of other reasons quickly come to mind.)
Note that these are all mathematical implications $\longrightarrow$:
that you can drive the car would {\it imply} there is
gas, air and ignition; that the child can go out {\it implies}
that the homework is done.  We just have to be careful
we do not read bi-implications (if and only if)
into any of these statements.

The other difference deals with another way to state implications:
if/then.  This is
also often misunderstood to mean if and only if.
Consider the colloquial English statements:\footnote{
%%% FOOTNOTE
This is related by Steven Zucker, Ph.D.\ from Johns Hopkins
University, writing in the appendix of Steven Krantz's 
{\it How to Teach Mathematics}, second edition, 
American Mathematical Society, 1999.
%%% END FOOTNOTE
}  
\begin{enumerate}[(a)]
\item If it stops raining, I'll go to the store.
\item If I win the lottery, I'll buy a new car.
\end{enumerate}

Unfortunately the ``if'' in  statement (a) might be intended to mean
``if and only if.'' Thus by stating (a) the speaker leads
the listener to believe  he  will definitely go to the 
store if it stops raining, but also that he  will go to
the store {\it only if} it stops raining (and thus
{\it will not} go if it does not stop raining).
To the strict logician 
(a) is not violated in the case it does not stop
raining, but the speaker still goes to the store. Recall that
in such a case (a) is vacuously true.

On the other hand, it seems somewhat more likely (b) is understood
the same by the logician and the casual user of English;
though we are tempted to understand the speaker to 
mean if and only if, upon reflection
we would not consider him a liar
for buying the car without having first won the lottery.\footnote{
%%% FOOTNOTE
In everyday English,
context may be important to our interpretations.  For instance, 
if the first person were asked, ``Will you go to the store,''
we might interpret (a) as an if and only if.  For the second
person, if he were asked, ``what will you do if you win the lottery,''
then (b) might be interpreted as an ``if,'' while if he were instead
asked, ``will you buy a new car,'' this answer might be interpreted
as an ``only if.'' 
%%% END FOOTNOTE
} 

In both (a) and (b) the personalities and shared experiences of 
the speaker and listener will likely play roles in 
what was meant by the speaker and what was understood
by the listener.  In mathematics we can not
have this kind of subjectivity. 
\subsection{Constructing Further Truth Tables}
Here we look at truth tables of more complicated compound
statements.  To do so, we first list the underlying 
component statements $P$, $Q$, and so on in lexicographical
order.  We then proceed---working ``inside-out'' and
step by step---to construct
the resulting truth values of the desired compound statement
for each possible truth value combinations of the component
statements.

It will be necessary to recall the actions of each of the
operations introduced earlier.  These are completely summarized
by their truth tables in the previous subsection, but
we can summarize the actions in words:
\begin{description}
\item[$\boldsymbol{\sim}$] changes T to F, and F to T.
\item[$\boldsymbol{\wedge}$] 
    returns F unless both statements it connects are true,
    in which case it returns T.
\item[$\boldsymbol{\vee}$] 
    returns T if either statements it connects are true, and
    F exactly when both statements are false.
\item[$\boldsymbol{\longrightarrow}$] 
    returns T except when the first statement
    is true and the second false.  In particular, if the
    first statement is false, then this returns T (vacuously).
\item[$\boldsymbol{\longleftrightarrow}$] 
    returns T if truth values of both statements
    match, and F if they differ.
\end{description}

 \bex Construct a truth table for $\sim(P\longrightarrow Q)$.
    
    \underline{Solution}: The underlying component statements are
$P$ and $Q$, so we first list these, and then their possible
truth value combinations in lexicographical order.  In
order to construct the resulting truth table values for
$\sim(P\longrightarrow Q)$, we build this statement one step
at a time with the operations, in an ``inside-out'' fashion.
By this we mean that we write the truth table column for
$P\longrightarrow Q$, and then apply the negation to get
the truth table column for $\sim(P\longrightarrow Q)$:
\begin{center}
\begin{tabular}{|c|c||c|c|}
\hline
$P$&$Q$&$P\longrightarrow Q$&$\sim(P\longrightarrow Q)$\\
\hline
T&T&T&F\\
T&F&F&T\\
F&T&T&F\\
F&F&T&F\\\hline
\end{tabular}
\end{center}
This reflects a fact we had before: that we
have $\sim(P\longrightarrow Q)$ true---i.e., we have
$P\longrightarrow Q$ false---exactly when we have
$P$ true but $Q$ false.\eex

Note that in the example above the third column,
which represents $P\longrightarrow Q$,
essentially connects the statements represented
by the first and second columns with the
connective $\longrightarrow$, while the last column
applied the operation $\sim$ to the statement represented by
that third column.  Thus the example reads easily from left to
right without interruption.  It is not always possible 
(or easiest) to do so; often we will add a column connecting
statements from previous columns which are some distance from
where we want to place our new column, though our style 
here will always have our final column representing the desired
compound statement.

\bex Compute the truth table for $(P\vee Q)\longrightarrow(P\wedge Q)$.
  
  \underline{Solution}:  Our ``inside-out'' strategy is 
still the same.  Here we list $P$ and $Q$, construct $P\vee Q$ and
$P\wedge Q$ respectively, and the connect these with $\longrightarrow$:
\begin{center}
\begin{tabular}{|c|c||c|c|c|}
\hline
$P$&$Q$&$P\vee Q$&$P\wedge Q$&$(P\vee Q)\longrightarrow(P\wedge Q)$\\
\hline
T&T&T&T&T\\
T&F&T&F&F\\
F&T&T&F&F\\
F&F&F&F&T\\
\hline
\end{tabular}
\end{center}

\eex

Some texts refer to the $\longrightarrow$ above as
the ``major connective,'' since ultimately the statement
$(P\vee Q)\longrightarrow(P\wedge Q)$ is an implication, albeit
connecting two already-compound statements $(P\vee Q)$ and
$(P\wedge Q)$.  Thus ``major connective'' can be seen
as referring to the last operator whose action was computed
in making the truth table for the statement as a whole.
(In the previous example, $\sim$ would be the major connective,
though we do not refer to unitary operators as 
``connectives.'')

After constructing such a truth table step-by-step, it is 
also instructive to step back and examine the result.
In particular, it is always useful to see which circumstances
render the whole statement false, which here are the second
and third combinations.  In those, we have (the 
antecedent)  $P\vee Q$ true
since one of the $P$, $Q$ is true, 
but (the consequent) $P\wedge Q$ is not true, since
$P$ and $Q$ are not both true.

\bex Construct the truth table for $P\longrightarrow[\sim (Q\vee R)]$.

\underline{Solution} Here we need $2^3=8$ different combinations
of truth values for the underlying component statements
$P$, $Q$ and $R$. Once we list these combinations in lexicographical
order, we then compute $Q\vee R$, $\sim(Q\vee R)$, and then compute
 $P\longrightarrow[\sim (Q\vee R)]$, in essence computing the major
connective $\longrightarrow$.
\begin{center}
\begin{tabular}{|c|c|c||c|c|c|}
\hline
$P$&$Q$&$R$&$Q\vee R$&$\sim(Q\vee R)$&$P\longrightarrow[\sim(Q\vee R)]$\\
\hline
T&T&T&T&F&F       \\
T&T&F&T&F&F       \\
T&F&T&T&F&F      \\
T&F&F&F&T&T       \\
\hline
F&T&T&T&F&T       \\
F&T&F&T&F&T        \\
F&F&T&T&F&T       \\
F&F&F&F&T&T       \\
\hline
\end{tabular}
\end{center}
Note that the last four cases are true vacuously.  The cases where
this is false are when we have $P$, but $\sim(Q\vee R)$ is false,
i.e., when we have $P$ and $Q\vee R$ true, i.e., when $P$ is
true and either $Q$ or $R$ is true.
\eex

\bex Construct the truth table for $P\wedge(Q\vee R)$.

\underline{Solution}: Again we need $2^3=8$ rows.  The column
for $P$ is repeated, though this is not necessary or always
desirable (see the above example), 
but here was done so that the last column represents
the major connective operating on the two columns immediately 
preceding it.

\begin{center}
\begin{tabular}{|c|c|c||c|c|c|}
\hline
$P$&$Q$&$R$&$P$&$Q\vee R$&$P\wedge(Q\vee R)$\\
\hline
T&T&T&T&T&T       \\
T&T&F&T&T&T       \\
T&F&T&T&T&T      \\
T&F&F&T&F&F       \\
\hline
F&T&T&F&T&F       \\
F&T&F&F&T&F        \\
F&F&T&F&T&F       \\
F&F&F&F&F&F       \\
\hline
\end{tabular}
\end{center}
\eex

In fact it is not difficult to spot which entries in the final
column should have value T, since what was required was that
$P$ be true, and at least one of the $Q$ or $R$ also be true.
Later we will see that this has exactly the same truth value, in
all circumstances, as $(P\wedge Q)\vee(P\wedge R)$, which is 
true if we have both $P$ and $Q$ true, or we have both $P$ and $R$
true.  That these two compound statements,
$P\wedge(Q\vee R)$ and $(P\wedge Q)\vee(P\wedge R)$ basically state
the same thing will be explored in Section~\ref{TVLEI}, as will
other ``logical equivalences.''




\subsection{Tautologies and Contradictions, A  First Look}

Two very important classes of compound statements are
those which form tautologies, and those which form
contradictions.  As we will see throughout the text, the tautologies 
loom especially large in our study and use of logic.
We will study both tautologies and contradictions further
in the next section.  Here we introduce the concepts and
begin to develop an intuition for these types of statements.
We begin with the definitions and most obvious examples.


\begin{definition} A compound statement formed by
the component statements $P_1, P_2,\cdots, P_n$
is called a 
{\bf tautology} iff its truth table column consists 
entirely of entries with truth value {\rm T} for each of the 
$2^n$ possible truth value combinations 
(\,{\rm T} and {\rm F}) of the component statements.\end{definition}
\begin{definition} A compound statement  formed by
the component statements $P_1, P_2,\cdots, P_n$
is called a 
{\bf contradiction} iff its truth table column consists 
 entirely of entries with truth value {\rm F} for each of 
the $2^n$  possible truth value combinations 
(\,{\rm T} and {\rm F}) of the component statements.\end{definition}
\bex Consider the statement $P\vee(\sim P)$, which is a 
tautology:

\begin{center}
{\rm
\begin{tabular}{|c||c|c|}
\hline
$P$ &$\sim P$& $P\vee(\sim P)$ \\
\hline
T&F&T\\ F&T&T\\
\hline\end{tabular}
}\end{center}
\label{taut1}
\eex



\bex  Next consider the statement $P\wedge(\sim P)$,
which is a contradiction:
\begin{center}
{\rm
\begin{tabular}{|c||c|c|}
\hline
$P$ &$\sim P$& $P\wedge(\sim P)$ \\
\hline
T&F&F\\ F&T&F\\
\hline\end{tabular}
}\end{center}
\eex 
We see that the statement $P\vee(\sim P)$ is always true,
whereas $P\wedge(\sim P)$ is always false. There are
other interesting tautologies, as well as other
interesting contradictions. For the moment let us concentrate on 
the tautologies.


That the statement $P\vee(\sim P)$ is a tautology---especially
when a particular example is examined---should be obvious
when we consider what the statement says: $P$ is true or $(\sim P)$
is true. If $P$ is the statement that I will eat pizza, then we
get the always true statement
$$P\vee(\sim P): \text{I will eat pizza or I will not eat pizza.}$$
In some contexts, tautologies seem to provide 
no useful information.  Indeed, there are times in formal speech that
declaring a statement to be a tautology is meant to be demeaning.
However, we will see that there are many nontrivial tautologies,
and it can be quite useful to recognize complex statements which are
always true.%
%%% FOOTNOTE
\footnote{In fact, we will essentially  devote the next whole section
to tautologies, though we use different terms there.}
For the moment we will look at the most basic of tautologies.
For instance, the next tautology is
obvious if we can read and understand its symbolic representation.
\bex $(P\wedge Q)\longrightarrow P$ is a tautology:
\medskip
\begin{center}
{\rm
\begin{tabular}{|c|c||c|c|c|}
\hline
$P$&$Q$&$P\wedge Q$&$P$&$(P\wedge Q)\longrightarrow P$\\
\hline
T&T&T&T&T\\
T&F&F&T&T\\
F&T&F&F&T\\
F&F&F&F&T\\ \hline\end{tabular}
}\end{center}
\medskip

\noindent
Note that the last three cases were true vacuously.
\eex

A simple English example shows how the above is a tautology.  
If we take $P$ as before, and $Q$ as the statement, ``I will
drink soda,'' then $(P\wedge Q)\longrightarrow P$
becomes, ``If I will eat pizza and drink soda then I will eat pizza.''
Looking at it abstractly, if we have both $P$ and $Q$ true, then
we have $P$ true.  Note that we cannot replace the implication
$\longrightarrow$ with a bi-implication $\longleftrightarrow$.


In the next section we will be very much interested in tautologies
in which the major connective is the bi-implication $\longleftrightarrow$.
In fact we will develop a variation of the notation for just those
cases.  The following tautology is one such example.

\bex Show that $[\sim(P\vee Q)]\longleftrightarrow[(\sim P)\wedge(\sim Q)]$
is a tautology.\footnote{%
%%% FOOTNOTE
Some texts employ a strict hierarchy of ``precedence'' or ``order of
operations'' on logic operations.  It is akin to arithmetic, where
$4\cdot5^2+3/5$ has us computing $5^2$, multiplying that by 4,
separately computing $3/5$, and then adding our two results.  
With grouping symbols
one might write $\left[4\left(5^2\right)\right]+[3/5]$, but
through conventions designed for convenience the grouping is understood
in the original expression.
For our example above, some texts will simply write
$$\sim(P\vee Q)\longleftrightarrow \,\sim P\wedge\sim Q,$$
understanding the precedence to be $\sim$, then $\wedge$ and $\vee$,
and then $\longrightarrow$ and $\longleftrightarrow$ in the order
of appearance.  Thus we need the first parentheses to 
override the precedence of the $\sim$, else
we would interpret $\sim P\vee Q$ to mean $(\sim P)\vee Q$.
As this text is not for a course in logic {\it per se}, we will
continue to use grouping symbols rather than spend the effort
to develop and practice
a procedure for precedence.  (Besides, the author finds the grouped
statements easier and more pleasing to read and write.)
%%% END FOOTNOTE
}

\underline{Solution}: 
\begin{center}
\begin{tabular}{|c|c||c|c|c|c|c|c|}
\hline
$P$&$Q$&$P\vee Q$&$\sim(P\vee Q)$&$\sim P$&$\sim Q$&$(\sim P)\wedge(\sim Q)$
  &$\ds{\begin{array}{c}[\sim(P\vee Q)]\\
           \longleftrightarrow[(\sim P)\wedge(\sim Q)]\end{array}}$\\
\hline
T&T&T&F&F&F&F&T\\
T&F&T&F&F&T&F&T\\
F&T&T&F&T&F&F&T\\
F&F&F&T&T&T&T&T\\
\hline
\end{tabular}
\end{center}
\eex

Later we will get into the habit of  just pointing out how the two
columns
representing, say,  $\sim(P\vee Q)$ and $(\sim P)\wedge(\sim Q)$
have the same truth values, so when connected by 
$\longleftrightarrow$ we get a tautology.  That will
be more convenient, as the entire statement does not
fit easily into a relatively narrow truth table column heading.


With reflection the various tautologies and contradictions
become intuitive, and easy to identify.  (For the above example
consider the discussion of when $P\vee Q$ is false, 
as in Example~\ref{PWedgeQDiscussion}, page~\pageref{PWedgeQDiscussion}.)
However, not all things which appear to be contradictions are
in fact contradictions.  At the heart of the problem in such
examples is usually the nature of the implication operation
$\longrightarrow$.  Consider the following:


\bex Write a truth table for $P\longrightarrow(\sim P)$ to demonstrate
that it is {\bf not} a contradiction.

\underline{Solution}: Note that there is only one independent statement
$P$, so we need only $2^1=2$ rows.

\begin{center}
\begin{tabular}{|c||c|c|}
\hline$P$&$\sim P$&$P\longrightarrow(\sim P)$\\
\hline
T&F&F\\
F&T&T\\
\hline
\end{tabular}
\end{center}
\eex
Note that the second case is true vacuously.  It is also interesting to note
that the statement $P\longrightarrow(\sim P)$ has the same truth values as
$\sim P$, which can be interpreted as saying $\sim P$ is the same
as $P\longrightarrow(\sim P)$.  That is worth pondering, but for the
moment we will not elaborate.

It is perhaps 
easier to spot contradictions which do not involve the implication.
For instance, $P\longleftrightarrow(\sim P)$ is a contradiction,
but the demonstration of that (by truth tables) is left to the reader.
(Simply replace $\longrightarrow$ with $\longleftrightarrow$ in the
above truth table.)



\newpage
\begin{center}
\underline{\Large{\bf Exercises}}\end{center}

\begin{multicols}{2}
\begin{enumerate}
\item A very useful way to learn the nuances of the
logic operations is to consider when their
compound statements are false.
For each of the following compound statements, discuss
all possible circumstances in which the given statement is false.

For example, $P\longleftrightarrow Q$ is false exactly when
$P$ is true and $Q$ false, or $Q$ true and $P$ false.
\label{QWERTY}
\begin{enumerate}[(a)]
\item $\sim P$\label{ASDF}
\item $P\wedge Q$
\item $P\vee Q$
\item $P\longrightarrow Q$
\item $P\longleftrightarrow Q$
\item $P\longrightarrow(\sim Q)$.\label{JKL;}
\end{enumerate}
\item Repeat \ref{QWERTY}(a)--(f) above, except
      using truth tables for each to answer
      the question of when each statement is false.
      Compare and reconcile your answers to Exercise~\ref{QWERTY}
      above.
\item Consider the statement 
$$(\sim Q)\longrightarrow (\sim P).$$
\begin{enumerate}[(a)]
\item When is it false?  
\item Now consider $P\longrightarrow Q$.  When
is it false?  
\item Do you believe these two compound statements mean the 
same thing?
\item Construct the truth table for the statement
      $(\sim Q)\longrightarrow(\sim P)$.  Then revisit
      your answer to (c).
\end{enumerate}  
\item Construct the truth table for $P$~{\bf XOR}~$Q$.
      (See Footnote~\ref{FootnoteXOR}, page~\pageref{FootnoteXOR}.)
\item Construct the truth table for the statement
      $\sim(P\longleftrightarrow Q)$.
      Compare your answer to the previous exercise.
\item Construct truth tables for the following statements:
  \begin{enumerate}
  \item $(\sim P)\longleftrightarrow(\sim Q)$ (Compare to
        $P\longleftrightarrow Q$.)
  \item $[P\vee(\sim Q)]\longrightarrow P$
  \item $\sim[P\wedge(Q\vee R)]$
  \end{enumerate}
\item Find six other English statements which are equivalent to
      the statement, \begin{quote}
      ``You can go out with your friends only if
      your homework is finished.'' \end{quote}
      (See page~\pageref{SixWaysForP->Q}.  Some of your answers
       may seem very formal.)
\item Construct truth tables for the following statements.
  \begin{enumerate}[(a)]
  \item $\sim(P\wedge Q)$
  \item $P\vee(Q\wedge R)$
  \item $P\vee(Q\vee R)$
  \item $(P\vee Q)\vee R$  (Compare to the previous statement.)
  \item $(P\longrightarrow Q)\wedge(Q\longrightarrow P)$
  \end{enumerate}
\item Decide which are tautologies, which are contradictions,
      and which are neither.  Try to decide using intuition, and then
      check with truth tables.
  \begin{enumerate}[(a)]
  \item $P\longrightarrow P$
  \item $P\longleftrightarrow P$
  \item $P\vee(\sim P)$
  \item $P\wedge(\sim P)$
  \item $P\longleftrightarrow(\sim P)$
  \item $P\longrightarrow(\sim P)$
  \item $((P)\wedge(\sim P))\longrightarrow Q$
  \item $(P\longrightarrow(\sim P))\longrightarrow(\sim P)$
  \item $(P\wedge Q)\longrightarrow P$
  \item $(P\vee Q)\longrightarrow P$
  \item $P\longrightarrow(P\wedge Q)$
  \item $P\longrightarrow(P\vee Q)$
   \end{enumerate}
\item Some confuse implication $\longrightarrow$ with causation,
      interpreting $P\longrightarrow Q$ as ``$P$ causes $Q$.''
      However, the implication is in fact weaker than the layman's
      concept of causation. 
  \begin{enumerate}
  \item Show that $(P\longrightarrow Q)\vee(Q\longrightarrow P)$
      is a tautology.
  \item Explain why, replacing $\longrightarrow$ with the phrase
      ``causes'' clearly does not give us a tautology.
  \item On the other hand, if $P$ true {\bf causes} $Q$ to be
      true, can we say $P\longrightarrow Q$ is true?
  \end{enumerate}
\item Write the lexicographical ordering of the possible
   truth value combinations for four statements $P,Q,R,S$.
\end{enumerate}
\end{multicols}




\newpage
\section{Valid Logical Equivalences as Tautologies
\label{TVLEI}} 

\subsection{The Idea, and Definition, of Logical Equivalence}
In lay terms, two statements are logically equivalent when they 
say the same thing, albeit perhaps in different ways.
To a mathematician, two statements are called logically equivalent
when they will always be simultaneously true or simultaneously 
false. To see these are compatible, consider an example of
a man named John N.\ Smith who
lives alone at 215 North Sentinel Avenue in Miami, Florida, and
has a United States Social Security number 380-38-1630.\footnote{%
%%% FOOTNOTE
This is all, of course, fictitious.}
%%% END FOOTNOTE
Of course there should be exactly one person with this Social
Security number.  Hence, when we ask  any person
the questions, ``are you John N.\ Smith of 215 North Sentinel Avenue
in Miami, Florida?'' and ``is your U.S.\ Social Security number
380-38-1630?'' we are in essence asking the same
question in both cases.  
Indeed, the answers to these two questions will always be
both yes, or both no,  so the statements ``you are John N.\ Smith
of  215 North Sentinel Avenue
in Miami, Florida,'' and ``your U.S.\ Social Security number is 380-38-1630,''
are logically equivalent.
The notation we would use is the following:
\begin{align*}
&\text{you are John N.\ Smith of 215 North Sentinel Avenue in Miami, Florida}\\
\iff
&\text{your U.S.\ Social Security number is 380-38-1630}.
\end{align*}
The motivation for the notation ``$\iff$'' will be explained shortly.

On a more abstract note, consider the statements
$\sim(P\vee Q)$ and $(\sim P)\wedge(\sim Q)$.  
Below we compute both of these compound statements' truth
values in one table:
\begin{center}
\begin{pspicture}(0,0)(14,3)
\rput(7,2){\begin{tabular}{|c|c||c|c|c|c|c|}
\hline
$P$&$Q$&$P\vee Q$&$\sim(P\vee Q)$&$\sim P$&$\sim Q$&$(\sim P)\wedge(\sim Q)$\\
\hline
T&T&T&F&F&F&F\\
T&F&T&F&F&T&F\\
F&T&T&F&T&F&F\\
F&F&F&T&T&T&T\\
\hline\end{tabular}
}
\psline{<->}(6,.9)(6.2,.3)(10.2,.3)(10.4,.9)
\rput(8.2,0){the same}
\end{pspicture}
\end{center}\label{TableForSimPOrQ}
We see that these two statements are both true or
both false, 
under any of the $2^2=4$ possible circumstances,
those being the possible truth value combinations of the
underlying, independent component statements $P$ and $Q$.
Thus the statements $\sim(P\vee Q)$ and $(\sim P)\wedge(\sim Q)$
are indeed logically equivalent in the sense of always 
having the same truth value.
Having established this, we would write
$$
\sim(P\vee Q)\iff(\sim P)\wedge(\sim Q).
%\label{NotPOrQmeansNotPAndNotQ}
$$
Note that in logic, this symbol ``$\iff$'' is similar to the symbol
``$=$'' in algebra and elsewhere.\footnote{%
%%% FOOTNOTE
Some texts use the symbol ``$\equiv$''
for logical equivalence.  However there is another
standard use for this symbol, and so we will reserve that symbol for
that use later in the text.
%%% END FOOTNOTE
} There are a couple of ways
it is read out loud, which we will consider momentarily.  
For now we take the occasion to list the formal definition
of logical equivalence:

\begin{definition}
Given $n$ independent statements $P_1,\cdots,P_n$, and two statements $R,S$
which are compound statements of the $P_1,\cdots,P_n$, we say that
$R$ and $S$ are {\bf logically equivalent}, which we then denote
$R\iff S$, if and only if their truth table columns have the
same entries for each of the $2^n$ distinct combinations of
truth values for the $P_1,\cdots,P_n$.
When $R$ and $S$ are logically equivalent, we will also 
call $R\iff S$ a {\bf valid equivalence}.\end{definition}

Again, this is consistent with the idea that to say
statements $R$ and $S$ are logically equivalent is to 
say that, under any circumstances, they are both
true or both false, so that asking if $R$ is true
is---functionally---exactly the same as asking if $S$ is true.
(Think back to our example of John N.\  Smith's Social Security number.)

Note that if two statements' truth values always match, then
connecting them with $\longleftrightarrow$ yields a 
tautology.  Indeed, the bi-implication yields T if the connected
statements have the same truth value, and $F$ otherwise.
Since two logically equivalent statements will 
have matching truth values in all cases, 
connecting with $\longleftrightarrow$
will always yield T, and we will have a tautology.  
On the other hand, if 
connecting two statements with $\longleftrightarrow$
forms a tautology, then the connected statements must
have always-matching truth values, and thus be equivalent.
This argument yields our first theorem:\footnote{%%%
%%% FOOTNOTE
A {\bf theorem} is a fact which has been proven to be true, 
particularly dealing with mathematics.  We will state 
numerous theorems in this text.  Most we will prove,
though occasionally we will include a theorem which
is too relevant to omit, but whose proof is  too technical
to include in an undergraduate calculus book.  Such 
proofs are left to courses with titles such as
mathematical (or real) analysis, topology, or advanced calculus.

Some theorems are also called lemmas (or, more archaically,
lemmata) when they are mostly useful as steps in larger
proofs of the more interesting results.  Still others
are called corollaries if they are themselves interesting, but
follow with very few extra steps after the 
underlying theorem is proved.
%%% END FOOTNOTE
}

\begin{theorem}
Suppose $R$ and $S$ are compound statements of $P_1\cdots,P_n$.
Then $R$ and $S$ are logically equivalent if and only if
$R\longleftrightarrow S$ is a tautology.
\end{theorem}

The theorem above gives us the motivation behind the 
notation $\iff$.  Assuming $R$ and $S$ are compound
statements built upon component statements $P_1\cdots,P_n$,
then 
\begin{equation}
R\iff S\qquad\text{means that}
\qquad R\longleftrightarrow S\text{ is a tautology}.
\label{EquationRelating<->and<=>}\end{equation}
To be clear, when we write $R\longleftrightarrow S$ we understand that
this might have truth value T or F, i.e., it might be true or false.
However, when we write $R\iff S$, we mean that $R\longleftrightarrow S$
is always true (i.e., a tautology), which partially explains why
we call $R\iff S$ a {\it valid equivalence}.\footnote{%
%%% FOOTNOTE
Depending upon the author, both $R\longleftrightarrow S$ and
$R\iff S$ are sometimes verbalized
``$R$ is equivalent to $S$,'' or ``$R$ if and only if $S$.''
We distinguish the cases by using the term ``equivalent''
for the double-lined arrow, and ``if and only if'' for the
single-lined arrow.  To help avoid confusion, we emphasize this
more restrictive use of ``equivalence'' by writing of ``valid equivalences.''%%
%%% END FOOTNOTE
}


To prove $R\iff S$, we could (but usually will not)
construct $R\longleftrightarrow S$,
and show that it is a tautology. 
We do so below to prove
$$\underbrace{\sim(P\vee Q)}_{\text{``$R$''}}
  \iff
  \underbrace{(\sim P)\wedge(\sim Q)}_{\text{``$S$''}}.$$
\medskip
\begin{center}
\begin{tabular}{|c|c||c|c|c|c|c|c|}
\hline
$P$&$Q$&$P\vee Q$&$\ds{\overbrace{\sim(P\vee Q)}^R}$&$\sim P$&$\sim Q$
             &$\ds{\overbrace{(\sim P)\wedge(\sim Q)}^S}$
      &{$\ds{\overbrace{\begin{aligned}&[\sim(P\vee Q)]\\\longleftrightarrow
                 & [(\sim P)\wedge(\sim Q)]\end{aligned}}^{
                      R\longleftrightarrow S}}$}\\
\hline
T&T&T&F&F&F&F&T\\
T&F&T&F&F&T&F&T\\
F&T&T&F&T&F&F&T\\
F&F&F&T&T&T&T&T\\
\hline
\end{tabular}
\end{center}
\medskip
However, our preferred method will be as in the previous
truth table, where we simply show that the truth table columns 
for $R$ and $S$ have the same entries 
at each horizontal 
level, i.e., for each truth value combination of the component
statements.  That approach saves space and reinforces
our original notion of equivalence (as matching truth
values).  However it is still important to understand the
connection between $\longleftrightarrow$ and $\iff$,
as given in (\ref{EquationRelating<->and<=>}).

\subsection{Equivalences for Negations}
Much of the intuition achieved from studying symbolic logic
comes from examining various logical equivalences.
Indeed we will make much use of these, for the  theorems
we use throughout the text are often stated in one form,
and then used in a different, but logically equivalent form.
When we prove a theorem, we may prove even a third,
logically equivalent form.

The first logical equivalences we will look at here are
the negations of the our basic operations.  
We already looked
at the negations of $\sim P$ and $P\vee Q$.  Below we
also look at negations of
$P\wedge Q$, $P\longrightarrow Q$ and $P\longleftrightarrow Q$.
Historically, (\ref{NOTPORQ}) and (\ref{NOTPANDQ}) below are called
{\it De Morgan's Rules}, but each basic negation is important.
We now list these negations.
\begin{align}
\sim(\sim P)&\iff P\label{NOTNOTP}\\
\sim(P\vee Q)&\iff(\sim P)\wedge(\sim Q)\label{NOTPORQ}\\
\sim(P\wedge Q)&\iff (\sim P)\vee(\sim Q)\label{NOTPANDQ}\\
\sim(P\longrightarrow Q)&\iff P\wedge(\sim Q)\label{NOTP->Q}\\
\sim(P\longleftrightarrow Q)&\iff[P\wedge(\sim Q)]\vee[Q\wedge(\sim P)].
\label{NOTP<->Q}\end{align}
Fortunately, with a correct perspective these are intuitive.
Recall that the negation asserts the original statement is false,
i.e., the negation is true exactly when the original is false.
Thus $\sim R$ is the statement that $R$ is false.
So the statement $\sim (P\vee Q)$ is the statement that
``$P$ or $Q$'' is false.  It is not difficult to
see that this requires $P$ false {\it and}
$Q$ false.  For a specific example,
consider our earlier $P$ and $Q$:
\begin{align*}
P:&\quad\text{I will eat  pizza}\\
Q:&\quad\text{I will drink soda}\\
P\vee Q:&\quad\text{I will eat pizza or I will drink soda}\\
\sim(P\vee Q):&\quad\text{It is not the case that \emph{I will
              eat pizza or I will drink soda}}\\
(\sim P)\wedge(\sim Q):&\quad
\text{It is not the case that I will eat pizza, and 
      it is not the case that I}\\
              &\quad\text{will drink soda}
\end{align*}
That these last two statements essentially have the same content
should be clear, as prescribed by (\ref{NOTPORQ}). 
Now the actual proof of (\ref{NOTPORQ}) is given by truth tables, 
and can be found on page~\pageref{TableForSimPOrQ}.

Next we consider (\ref{NOTP->Q}).  This states that 
$\sim(P\longrightarrow Q)\iff P\wedge(\sim Q)$.
Now we can read $\sim(P\longrightarrow Q)$ as
``it is not the case that $P\longrightarrow Q$,''
or ``$P\longrightarrow Q$ is false.''  Recall that
there was only one case that we considered $P\longrightarrow Q$
to be false, which was the case that $P$ was true but
$Q$ was false, which itself can be translated to 
$P\wedge(\sim Q)$.  For our earlier example, the
negation of the statement ``if I eat pizza then
I will drink soda'' is the statement ``I will eat pizza
but (and) I will not drink soda.''  While this discussion
is correct and may be intuitive, the actual proof
(\ref{NOTP->Q}) is by truth table:
\medskip
\begin{center}
\begin{pspicture}(0,-1.1)(9.05,2.1)
\rput(4.5,1){
\begin{tabular}{|c|c||c|c|c|c|c|}
\hline
$P$&$Q$&$P\rightarrow Q$&$\sim(P\rightarrow Q)$&$P$&$\sim Q$
&$P\wedge(\sim Q)$\\ 
\hline
T&T&T&F&T&F&F\\
T&F&F&T&T&T&T\\
F&T&T&F&T&F&F\\
F&F&T&F&F&T&F\\
\hline
\end{tabular}}
\psline{<->}(4.2,-.08)(4.4,-.608)(7.8,-.608)(8.,-.08)
\rput(6.1,-.908){the same}
\end{pspicture}\end{center}
\medskip

We leave the proof of (\ref{NOTP<->Q}) by truth tables to the exercises.
Recall that $P\longleftrightarrow Q$ states that
we have $P$ true if and only if we also have $Q$ true, which 
we further translated as the idea that
we cannot have $P$ true without $Q$ true, and
cannot have $Q$ true without $P$ true.  Now $\sim(P\longleftrightarrow Q)$
is the statement that $P\longleftrightarrow Q$ is false, which
means that $P$ is true and $Q$ false, or $Q$ is true and $P$ false,
which taken together form the statement
$[P\wedge(\sim Q)]\vee[Q\wedge(\sim P)]$, as reflected in
(\ref{NOTP<->Q}) above.  For our example $P$ and $Q$ from before, 
$P\longleftrightarrow Q$ is the statement ``I will at pizza
if and only if I will drink soda,'' the 
negation of which is ``I will eat pizza and not
drink soda, or I will drink soda and not eat pizza.''

Another intuitive way to look at these negations is to consider
the question of exactly when is someone uttering the original
statement lying?  For instance, if someone states
$P\wedge Q$ (or some English equivalent), when are they lying?
Since they stated ``$P$ \emph{and} $Q$,'' it is not difficult
to see they are lying exactly when at least
one of the statements $P$ and $Q$ is
false, i.e., when $P$ is false or $Q$ is false,\footnote{
%%% FOOTNOTE
Note again that here as always we use the inclusive ``or,'' so when we write
``$P$ is false or $Q$ is false,'' we include the case
both $P$ and $Q$ are false. (See Footnote~\ref{FootnoteXOR},
page~\pageref{FootnoteXOR} for remarks on the exclusive ``or.'')
%%% END FOOTNOTE
} 
i.e., when the truth is $(\sim P)\vee(\sim Q)$.
That is the kind of thinking one should employ when 
examining (\ref{NOTPANDQ}), that is
$\sim(P\wedge Q)\iff(\sim P)\vee(\sim Q)$, intuitively.

\subsection{Equivalent Forms of the Implication}
In this subsection we examine two statements which
are equivalent to $P\longrightarrow Q$.  The first is
more important conceptually, and the second is more
important computationally.  
We list them both now before contemplating them further:
\begin{align}
P\longrightarrow Q&\iff(\sim Q)\longrightarrow(\sim P)
     \label{CONTRAPOSITIVE}\\
P\longrightarrow Q&\iff(\sim P)\vee Q.\label{ALTFORMOFIMPLICATION}
\end{align}
We will combine the proofs into one truth table, where we
compute $P\longrightarrow Q$, followed in turn by
$(\sim Q)\longrightarrow (\sim P)$ and $(\sim P)\vee Q$.
\medskip

\begin{center}
\begin{pspicture}(0,-1)(12,3.1)
%\psframe(0,1)(12,3.1)
\rput(6,2){%
\begin{tabular}{|c|c||c|c|c|c||c|c|c|}
\hline
$P$&$Q$&$P\rightarrow Q$&$\sim Q$&$\sim P$&$(\sim Q)\rightarrow(\sim P)$
                &$\sim P$&$Q$&$(\sim P)\vee Q$\\
\hline
T&T&T&F&F&T&F&T&T\\
T&F&F&T&F&F&F&F&F\\
F&T&T&F&T&T&T&T&T\\
F&F&T&T&T&T&T&F&T\\
\hline
\end{tabular}
}
\psline{<->}(2.33,.9)(2.53,.3)(6.6,.3)(6.6,.9)
\psline{->}(6.6,.3)(10.55,.3)(10.75,.9)
\rput(6.5,0){the same}
\end{pspicture}
\end{center}
The form (\ref{CONTRAPOSITIVE}) is important enough
that it warrants a name:
\begin{definition}
Given any implication $P\longrightarrow Q$, we call
the {\bf contrapositive} the (logically equivalent) statement
$(\sim Q)\longrightarrow (\sim P)$.
\end{definition}
We have proved that $P\longrightarrow Q$, its contrapositive
$(\sim Q)\longrightarrow (\sim P)$, and the other
form $(\sim P)\vee Q$ are equivalent using the truth table
above, but developing the  intuition that these {\it should} be
equivalent can require some effort.  
Using our earlier statement $P$ and $Q$, we can write
\medskip

$\ds{\begin{aligned}
P:&\ \text{I will eat pizza}\\
Q:&\ \text{I will drink soda}\\
P\longrightarrow Q:&\ \text{If I eat pizza, then I will drink soda}\\
(\sim Q)\longrightarrow(\sim P):&\ 
      \text{If I do not drink soda, then I will not eat pizza}\\
(\sim P)\vee Q:&\ \text{I will not eat pizza, or I will drink soda.}
\end{aligned}}$
\medskip

\noindent Perhaps more intuition can be found when $Q$ is a more natural
consequence of $P$.  Consider the following $P,Q$ combination
which might be used by parents communicating to their children.
\medskip

$\ds{\begin{aligned}
P:&\ \text{you leave your room messy}\\
Q:&\ \text{you get spanked}\\
P\longrightarrow Q:&\ 
         \text{if you leave your room messy, then you get spanked}\\
(\sim Q)\longrightarrow(\sim P):&\ 
      \text{if you do not get spanked, then you do (did) not leave your
            room messy}\\
(\sim P)\vee Q:&\ \text{you do not leave your room messy, or you get spanked.}
\end{aligned}}$
\medskip

\noindent A mathematical example could look like the following:
\medskip

$\ds{\begin{aligned}
P:&\ x=10\\
Q:&\ x^2=100\\
P\longrightarrow Q:&\ \text{if }x=10,\text{ then }x^2=100\\
(\sim Q)\longrightarrow(\sim P):&\ \text{if }x^2\ne100,\text{ then }x\ne10\\
(\sim P)\vee Q:&\ x\ne10\text{ or }x^2=100.
\end{aligned}}$
\medskip

The contrapositive is very important because many theorems
are given as implications, and often used in the 
logically equivalent, contrapositive form.
However, it is equally important to avoid confusing
$P\longrightarrow Q$ with either of the statements
$P\longleftrightarrow Q$
or $Q\longrightarrow P$.  For instance, in the second example above,
the child may get spanked without leaving the room messy,
as there are quite possibly other infractions which would 
result in a spanking.  Thus leaving the room messy does
not follow from being spanked, and leaving the room 
messy is not necessarily
connected with the spanking by an ``if and only if.''
In the last, algebraic example above, all the forms of the statement are
true, but $x^2=100$ does not imply $x=10$.  Indeed, it is
possible that $x=-10$.  In fact, the correct bi-implication
is $x^2=100\longleftrightarrow [(x=10)\vee(x=-10)]$.

\subsection{Other Valid Equivalences}
While negations and equivalent 
alternatives to the implication are arguably the most important
of our valid logical equivalences, there are several others.
Some are rather trivial, such as 
\begin{equation}P\wedge P\iff P\iff P\vee P.\end{equation}
Also rather easy to see are the
``commutativities'' of 
$\wedge,\vee$ and $\longleftrightarrow$:
\begin{equation}P\wedge Q\iff Q\wedge P,\qquad\qquad
P\vee Q\iff Q\vee P,\qquad\qquad
P\longleftrightarrow Q\iff Q\longleftrightarrow P.\end{equation}
There are also associative rules.  The latter was in fact a topic in the
previous exercises:
\begin{align}
P\wedge(Q\wedge R)&\iff (P\wedge Q)\wedge R\\
P\vee(Q\vee R)&\iff(P\vee Q)\vee R.\end{align}

However, it is not so clear when we mix together
$\vee$ and $\wedge$.  In fact, these ``distribute over each other''
in the following ways:
\begin{align}
P\wedge(Q\vee R)&\iff (P\wedge Q)\vee(P\wedge R),\\
P\vee(Q\wedge R)&\iff(P\vee Q)\wedge(P\vee R).
\end{align}
We prove the first of these distributive rules below, and
leave the other for the exercises.
\begin{center}
\begin{pspicture}(0,-1)(12,4)
%\psframe(0,-1)(12,4)
\rput(6,2){%
\begin{tabular}{|c|c|c||c|c|c|c|c|}
\hline
$P$&$Q$&$R$&$Q\vee R$&$P\wedge(Q\vee R)$&$P\wedge Q$&$P\wedge R$
    &$(P\wedge Q)\vee(P\wedge R)$\\
\hline
T&T&T&T&T&T&T&T\\
T&T&F&T&T&T&F&T\\
T&F&T&T&T&F&T&T\\
T&F&F&F&F&F&F&F\\
\hline
F&T&T&T&F&F&F&F\\
F&T&F&T&F&F&F&F\\
F&F&T&T&F&F&F&F\\
F&F&F&F&F&F&F&F\\
\hline
\end{tabular}}
\psline{<->}(4.8,.05)(5,-.6)(10,-.6)(10.2,0.05)
\rput(7.5,-.9){the same}
\end{pspicture}
\end{center}

To show that this is reasonable, consider the following:
\begin{align*}
P:&\ \text{I will eat pizza;}\\
Q:&\ \text{I will drink cola;}\\
R:&\ \text{I will drink lemon-lime soda.}
\end{align*}
Then our logically equivalent statements become
\begin{align*}
P\wedge(Q\vee R):\ &\text{I will eat pizza, and
                drink cola or lemon-lime soda;}\\
(P\wedge Q)\vee(P\wedge R):\ &\text{I will eat pizza
and drink cola, or }\\ 
                &\text{I will eat pizza
      and drink lemon-lime soda.}
\end{align*}



Table~\ref{TableOfCommonValidEquivalences},
page~\pageref{TableOfCommonValidEquivalences} gives these
and some further valid equivalences.
It is important to be able to read these and, through reflection
and the exercises, be able to see the reasonableness of each of 
these.  Each can be proved using truth tables.  We prove a 
couple here, and leave the rest for the exercises.

Now we finally prove that
$P\longleftrightarrow Q\iff
(P\longrightarrow Q)\wedge(Q\longrightarrow P),$
justifying the use of the double-arrow $\longleftrightarrow$.
\begin{center}
\begin{pspicture}(0,-1.1)(11,2.4)
\rput(5.45,1.2){
\begin{tabular}{|c|c||c|c|c|c|}\hline
$P$ &$Q$&$P\longleftrightarrow Q$ &$P\longrightarrow Q$&$Q\longrightarrow P$
&$(P\longrightarrow Q)\wedge(Q\longrightarrow P)$\\
\hline
T&T&T&T&T&T\\
T&F&F&F&T&F\\
F&T&F&T&F&F\\
F&F&T&T&T&T\\ 
\hline\end{tabular}}
\psline{<->}(2.55,0.1)(2.75,-.5)(8.7,-.5)(8.9,0.1)
\rput(5.725,-.8){the same}
\end{pspicture}
\end{center}
This was discussed in Example~\ref{P<->QExampleAndExplanation}
on page~\pageref{P<->QExampleAndExplanation}.

A list of the most important valid logical equivalences is
given in Table~\ref{TableOfCommonValidEquivalences},
page~\pageref{TableOfCommonValidEquivalences}.
All can be proved using our truth table techniques.
For another example of such a proof,
we next demonstrate the following interesting equivalence:
$$P\longrightarrow(Q\wedge R)
\iff(P\longrightarrow Q)\wedge(P\longrightarrow R)$$
\begin{center}
\begin{pspicture}(0.1,-1.1)(13.8,4)
\rput(6.9,2){
\begin{tabular}{|c|c|c||c|c|c|c|c|c|}\hline
$P$&$Q$&$R$&$Q\wedge R$& $P\longrightarrow(Q\wedge R)$&
$P\longrightarrow Q$&$P\longrightarrow R$&
$(P\longrightarrow Q)\wedge(P\longrightarrow R)$\\ \hline
T&T&T&T&T&T&T&T\\
T&T&F&F&F&T&F&F\\
T&F&T&F&F&F&T&F\\
T&F&F&F&F&F&F&F\\
\hline
F&T&T&T&T&T&T&T\\
F&T&F&F&T&T&T&T\\
F&F&T&F&T&T&T&T\\
F&F&F&F&T&T&T&T\\
\hline\end{tabular}}
\psline{<->}(5.05,0.05)(5.25,-.595)(11.55,-.595)(11.75,0.05)
\rput(8.4,-.895){the same}
\end{pspicture}
\end{center}
This should be somewhat intuitive: if $P$ is to imply $Q\wedge R$,
that should be the same as $P$ implying $Q$ {\it and} $P$ implying $R$.
This equivalence will be (\ref{P->QANDR}), page~\pageref{P->QANDR}.  
According to 
(\ref{P->QORR}) below it, we can replace $\wedge$ with $\vee$ and
get another valid equivalence.

Still one must be careful about declaring two statements
to be equivalent.  These are all ultimately intuitive,
but intuition must be informed.\footnote{%
%%% FOOTNOTE
It is a truism in mathematics and other fields that, while
one part of learning is discovering what is true, another part is
discovering what is not true, especially when the latter seems
reasonable at first look.%
%%% END FOOTNOTE
} For instance, left to the exercises are some valid equivalences which
may seem counter-intuitive.  These are in fact left off of our 
Table~\ref{TableOfCommonValidEquivalences} because they are somewhat
obscure, but we include them here to illustrate that not all 
equivalences are transparent.  Consider
\begin{align}
(P\vee Q)\longrightarrow R&\iff(P\longrightarrow R)\wedge(Q\longrightarrow R),
   \label{ObscureEquiv:(PveeQ)->R}\\
(P\wedge Q)\longrightarrow R&\iff(P\longrightarrow R)\vee(Q\longrightarrow R).
   \label{ObscureEquiv:(PwedgeQ)->R}\end{align}
Upon reflection one can see how these are reasonable.  For instance, we
can look more closely at (\ref{ObscureEquiv:(PveeQ)->R}) with the
following $P,Q$ and $R$:
\begin{align*}
P&:\text{I eat pizza,}\\
Q&:\text{I eat chicken,}\\
R&:\text{I drink cola.}\end{align*}
Then the left and right sides of (\ref{ObscureEquiv:(PveeQ)->R})
become
\begin{align*}
(P\vee Q)\longrightarrow R&:\text{If I eat pizza or chicken, then I drink cola}
       \\
(P\longrightarrow R)\wedge(Q\longrightarrow R)&:\text{If I eat pizza
   then I drink cola, and if I eat chicken then I drink cola.}
\end{align*}
In fact (\ref{ObscureEquiv:(PwedgeQ)->R}) is perhaps more difficult
to see.  

At the end of the chapter there will be an optional section
for the reader interested in achieving a higher level of 
symbolic logic sophistication.  That section is devoted
to finding and proving valid equivalences (and implications
as seen in the next section) without relying on truth tables.
The technique centers on using a small number of
established equivalences to rewrite compound statements
into alternative, equivalent forms. With those techniques
one can quickly prove (\ref{ObscureEquiv:(PveeQ)->R})
and (\ref{ObscureEquiv:(PwedgeQ)->R}), again without truth
tables.   It is akin to proving trigonometric
identities, or the leap from memorizing single-digit multiplication
tables and applying them to several-digit problems.  For a glance
at the process, we can look at such a proof of the equivalence of 
the contrapositive:  $P\longrightarrow Q\iff(\sim Q)\longrightarrow(\sim P)$.
To do so, we require (\ref{AltP->Q}), that $P\longrightarrow Q
\iff(\sim P)\vee Q$.  The proof runs as follows:
\begin{align*}
P\longrightarrow Q&\iff(\sim P)\vee Q\\
                  &\iff Q\vee(\sim P)\\
                  &\iff [\sim(\sim Q)]\vee(\sim P)\\
                  &\iff (\sim Q)\longrightarrow(\sim P).\end{align*}
The first line used (\ref{AltP->Q}), the second commutativity
(\ref{PVQ<->QVP}), the 
third that $Q\iff\sim(\sim Q)$ (\ref{NotNotP<->P}), and the fourth used
(\ref{AltP->Q}) again but with the part of ``$P$'' played
by $(\sim Q)$ and the part of ``$Q$'' played by $(\sim P)$.
This proof is not much more efficient than a truth table
proof, but for (\ref{ObscureEquiv:(PveeQ)->R})
and (\ref{ObscureEquiv:(PwedgeQ)->R}) the technique is much faster.
However, since this text's ultimate topic is calculus, the techniques of 
the extra section are not required reading.  



\begin{table}
%\begin{center}\underline{\bf{\Large Valid Equivalences}}\end{center}
\begin{align}
P\wedge P&\iff P\iff P\vee P\qquad\qquad
\label{Idempotents}\\
\sim(\sim P)&\iff P\label{NotNotP<->P}\\
\sim(P\vee Q)&\iff(\sim P)\wedge(\sim Q)\label{SimPVeeQ}\\
\sim(P\wedge Q)&\iff(\sim P)\vee(\sim Q)\label{SimPWedgeQ}\\
%P\vee(\sim P)&\iff \T\label{PornotP}\\
%P\wedge(\sim P)&\iff \F\label{PandnotP}\\
\sim(P\longrightarrow Q)&\iff P\wedge(\sim Q)\label{not(P->Q)}\\
\sim(P\longleftrightarrow Q)&\iff
[P\wedge(\sim Q)]\vee[Q\wedge(\sim P)]\label{not(P<->Q}\\
P\vee Q&\iff Q\vee P\label{PVQ<->QVP}\\ 
P\wedge Q&\iff Q\wedge P\\
P\vee(Q\vee R)&\iff (P\vee Q)\vee R\label{veeovervee}\\
P\wedge(Q\wedge R)&\iff (P\wedge Q)\wedge R\label{wedgeoverwedge}\\
P\wedge(Q\vee R)&\iff (P\wedge Q)\vee(P\wedge R)\label{LogicDistrib1}\\
P\vee(Q\wedge R)&\iff (P\vee Q)\wedge(P\vee R)\label{LogicDistrib2}\\
P\longrightarrow Q&\iff(\sim P)\vee Q\label{AltP->Q}\\
P\longrightarrow Q&\iff(\sim Q)\longrightarrow(\sim P)\label{contrap}\\
P\longleftrightarrow Q&\iff(\sim P)\longleftrightarrow(\sim Q)
            \label{BiImpSameAsBiImpOnNegs}\\
P\longrightarrow(Q\wedge R)&\iff(P\longrightarrow Q)\wedge
         (P\longrightarrow R)\label{P->QANDR}\\
P\longrightarrow(Q\vee R)&\iff(P\longrightarrow Q)\vee
         (P\longrightarrow R)\label{P->QORR}\\
(P\longrightarrow Q)\wedge(Q\longrightarrow
P)&\iff P\longleftrightarrow Q\label{2cycleequiv}\\
(P\longrightarrow Q)\wedge(Q\longrightarrow R)\wedge(R\longrightarrow P)
&\iff (P\longleftrightarrow Q)\wedge(Q\longleftrightarrow R)
   \notag\qquad\qquad\qquad\qquad\\
&\hphantom{\iff} \qquad\wedge(P\longleftrightarrow R)
\label{3cycleequiv}
\end{align} 
%\medskip
\caption{Table of common valid logical equivalence.}
\label{TableOfCommonValidEquivalences}
\end{table}

%% JOHN HOWARD



\newpage

\subsection{The Statements $\T$  and $\F$}
Just as there is a need for zero in addition, we have use for
a symbol representing  a statement which is always true, and another
representing a statement which is always false.
For convenience, we will make the following definitions:
\begin{definition} Let $\T$ represent any compound statement which
is a {\bf tautology}, i.e., whose truth value is
always {\rm T}.
Similarly, let $\F$ represent any compound statement which
is a {\bf contradiction}, i.e.,
whose truth value is always {\rm F}.\end{definition}

We will assume there is a universal $\T$ and a universal $\F$,
i.e., statements which are respectively true regardless
of any other statements' truth values, and false
regardless of any other statements' truth values.
In doing so, we consider any tautology to be logically equivalent to
$\T$, and any contradiction similarly equivalent to $\F$.\footnote{%
%%% FOOTNOTE
In fact it is not difficult to see that all tautologies are
logically equivalent.  Consider the tautologies
$P\vee(\sim P)$,
$(P\longrightarrow Q)\longleftrightarrow[(\sim Q)\longrightarrow(\sim P)]$,
and $R\longrightarrow R$.  A truth table for all three 
must contain independent
component statements $P,Q,R$,
and the abridged version of the table would look like
\begin{center}
\begin{tabular}{|c|c|c||c|c|c|}
\hline
$P$&$Q$&$R$&$P\vee(\sim P)$&$(P\longrightarrow Q)
     \longleftrightarrow[(\sim Q)\longrightarrow(\sim P)]$&
     $R\longrightarrow R$\\ \hline
T&T&T&T&T&T\\
T&T&F&T&T&T\\
T&F&T&T&T&T\\
T&F&F&T&T&T\\ \hline
F&T&T&T&T&T\\
F&T&F&T&T&T\\
F&F&T&T&T&T\\
F&F&F&T&T&T\\ \hline
\end{tabular}\end{center}
So when all possible underlying independent component statements are
included, we see the truth table columns of these tautologies
are indeed the same (all T's!).  Similarly all contradictions are
equivalent.
%%% END FOOTNOTE
} 

So, for any given $P_1\cdots,P_n$, we have that $\T$ is 
exactly 
that statement whose column in the truth table consists
entirely of T's, and $\F$ is exactly
that statement whose column in the truth table consists
entirely of F's.
% \footnotetext{$\T$ is unique, in the sense that any statement
%with all T's would be logically equivalent to $\T$.  $\F$
%is similarly unique.}
For example, we can write
\begin{align}P\vee(\sim P)&\iff \T;\label{FirstTautology}\\
P\wedge(\sim P)&\iff\F.\label{FirstContradiction}\end{align}
These are easily seen by observing the truth tables.\medskip

\begin{tabular}{|c||c|c|c|c|}
\hline
$P$&$\sim P$&$P\vee(\sim P)$&$P\wedge(\sim P)$\\
\hline
T&F&T&F\\
F&T&T&F\\
\hline
\end{tabular}
\medskip

\noindent
We see that $P\vee(\sim P)$ is always true, and $P\wedge(\sim P)$ 
is always false.  Anything which is always true we will dub $\T$,
and anything which is always false we will call $\F$.
In the table above, the third column represents $\T$, and the
last column represents $\F$.



From the definitions we can also eventually get the following.
\begin{align}
P\vee \T&\iff \T\label{PVeeT}\\
P\wedge\T&\iff P\label{PWedgeT}\\
P\vee\F&\iff P\label{PVeeF}\\
P\wedge\F&\iff \F.\label{PWedgeF}\end{align} 



To demonstrate how one would prove these, we prove here
the first two, (\ref{PVeeT}) and (\ref{PWedgeT}), using a truth table.  Notice
that all entries for $\T$ are simply T:
\bigskip

\begin{tabular}{|c|c||c|c|}
\hline
$P$&$\T$&$P\vee\T$&$P\wedge\T$\\
\hline
T&T&T&T\\ F&T&T&F\\ \hline\end{tabular}
\bigskip

\noindent Equivalence (\ref{PVeeT}) is demonstrated by the equivalence of 
the second and third columns, while (\ref{PWedgeT})
is shown by the equivalence of the first and fourth columns.
The others are left as exercises.

These are also worth reflecting upon.  Consider the equivalence
$P\wedge\T\iff P$.  When we use $\wedge$ to
connect $P$ to a statement which
is always true, then the truth of the compound 
statement only depends upon the truth of $P$.
There are similar explanations for the rest of 
(\ref{PVeeT})--(\ref{PWedgeF}).

Some other interesting equivalences involving these are the 
following:
\begin{align}
\T\longrightarrow P&\iff P\\
P\longrightarrow\F&\iff \sim P.\label{RedAdButAsEquivalence}\end{align}
We leave the proofs of
these for the exercises.  These are in fact interesting to
interpret.  The first says that if a true statement implies $P$,
that is the same as in fact having $P$.  The second says that if
$P$ implies a false statement, that is the same as having $\sim P$,
i.e., as having $P$ false.  Both types of reasoning are useful
in mathematics and other disciplines.

If a statement contains {\it only} $\T$ or $\F$, then that statement
in fact that statement itself must be a tautology ($\T$) or
a contradiction ($\F$).  This is because there is only one 
possible combination of truth values. For instance, consider
the statement $\T\longrightarrow\F$, which is a contradiction.
One proof is in the table:
\begin{center}
\begin{tabular}{|c|c||c|}
\hline
$\T$&$\F$&$\T\longrightarrow\F$\\
\hline
T&F&F\\
\hline
\end{tabular}
\end{center}
Since the component statement $\T\longrightarrow\F$
always has truth value F, it is a contradiction.  Thus
$\T\longrightarrow\F\iff\F$.

\newpage
\begin{center}\underline{\Large{\bf Exercises}}\end{center}

Some  of these were solved within the section.
It is useful to attempt them here again, in 
the context of the other problems.  Unless otherwise specified, 
all proofs should be via truth tables.

\bigskip
\begin{multicols}{2}
\begin{enumerate}
\item Prove (\ref{NotNotP<->P}): $\sim(\sim P)\iff P$.
\item Prove (\ref{BiImpSameAsBiImpOnNegs}):

$P\longleftrightarrow Q\iff
(\sim P)\longleftrightarrow(\sim Q)
$.
\item Prove the logical equivalence of the 
contrapositive {\rm(\ref{contrap}):} 

$P\longrightarrow Q\iff (\sim Q)\longrightarrow(\sim P)$.
\item Prove (\ref{AltP->Q}):
$P\longrightarrow Q\iff(\sim P)\vee Q$.
\item Show that $P\longrightarrow Q$ and $Q\longrightarrow P$ 
are not equivalent.
\item Prove
De Morgan's Laws (\ref{SimPVeeQ})
and (\ref{SimPWedgeQ}), which are listed again below:
 \begin{enumerate}[(a)]
 \item $\sim(P\vee Q)\iff(\sim P)\wedge(\sim Q).$
 \item $\sim(P\wedge Q)\iff(\sim P)\vee(\sim Q).$
 \end{enumerate}
\item Use truth tables to prove the
{\it distributive-type} laws (\ref{LogicDistrib1}) and
(\ref{LogicDistrib2}):
\begin{enumerate}[(a)]
\item $P\wedge(Q\vee R)\iff(P\wedge Q)\vee(P\wedge R)$.
\item $P\vee(Q\wedge R)\iff(P\vee Q)\wedge(P\vee R)$.
\end{enumerate}
\item Prove (\ref{P->QANDR}):
$P\longrightarrow(Q\wedge R)$

$\iff
(P\longrightarrow Q)\wedge(P\longrightarrow R)$.

\item Prove (\ref{P->QORR}):
$P\longrightarrow(Q\vee R)$

$\iff (P\longrightarrow Q)\vee(P\longrightarrow R)$.

\item Prove (\ref{not(P->Q)}):

$\sim(P\longrightarrow Q)\iff P\wedge (\sim Q)$.
\item Prove (\ref{2cycleequiv}), which we write below as

$P\longleftrightarrow Q\iff
(P\longrightarrow Q)\wedge(Q\longrightarrow P)$.
\newline
Note that this justifies the choice of the double-arrow
notation $\longleftrightarrow$.

\item Prove (\ref{not(P<->Q}):
$
\sim(P\longleftrightarrow Q)$

$\iff
(P\wedge(\sim Q))\vee(Q\wedge(\sim P)).
$

\item Recall the description of {\bf XOR} in 
      footnote~\ref{FootnoteXOR}, page \pageref{FootnoteXOR}.
  \begin{enumerate}[(a)] 
  \item Construct a truth table for \newline $P$ {\bf XOR} $Q$.
  \item Compare to the previous problem.
        Can you make a conclusion?
  \item Find an expression for $P$ {\bf XOR} $Q$ using $P$, $Q$,
        $\sim$, $\wedge$ and $\vee$.
  \end{enumerate}
\item Prove the following:
  \begin{enumerate}[(a)]
  \item (\ref{PVeeT}): $P\vee\T\iff \T$.
  \item (\ref{PWedgeT}): $P\wedge\T\iff P$.
  \item (\ref{PVeeF}): $P\vee\F\iff P$.
  \item (\ref{PWedgeF}): $P\wedge\F\iff\F$. 
  \end{enumerate}
\item For each of the following, find a simple, equivalent statement,
      using truth tables if necessary.
  \begin{multicols}{2}
  \begin{enumerate}[(a)]
  \item $\T\vee\T$
  \item $\F\vee\F$
  \item $\T\vee\F$
  \item $\T\wedge\T$
  \item $\F\wedge\F$
  \item $\T\wedge\F$
  \item $\T\longrightarrow\T$
  \item $\F\longrightarrow\F$
  \item $\T\longrightarrow\F$
  \item $\F\longrightarrow\T$
  \end{enumerate}\end{multicols}
\item Repeat the previous exercise for the following:
  \begin{multicols}{2}
  \begin{enumerate}[(a)]
  \item $\T\longrightarrow P$
  \item $\F\longrightarrow P$
  \item $P\longrightarrow \T$
  \item $P\longrightarrow \F$
  \item $P\longleftrightarrow\T$
  \item $P\longleftrightarrow\F$
  \item $P\longleftrightarrow(\sim P)$
  \item $P\longrightarrow(\sim P)$
  \end{enumerate}\end{multicols}
\item Prove the associative rules (\ref{veeovervee}) and
      (\ref{wedgeoverwedge}), page~\pageref{veeovervee}.
\item Prove (\ref{3cycleequiv}), page~\pageref{3cycleequiv}.
\item Show $(P\vee Q)\rightarrow R
   \iff [(P\rightarrow R)\wedge(Q\rightarrow R)].$
   Try to explain why this makes sense.
\item Show $(P\wedge Q)\rightarrow R
   \iff [(P\rightarrow R)\vee(Q\rightarrow R)].$
\end{enumerate}
\end{multicols}

\newpage
\section{Valid Implications and Arguments\label{ValidImplicationsSection}}
%Used to be Tautologies, Contradictions
%and Compound Statements Implying Other Statements.
%\markright{\ref{TCCS}\ TAUTOLOGIES, CONTRADICTIONS, COMPOUND STATEMENTS}

Most theorems in this text are in the form of implications,
rather than the more rigid equivalences of the last section.
Indeed, our theorems are usually of the form
``hypothesis implies conclusion.''  So we have need
of an analog to our valid equivalences, namely 
a notion of valid implications.
\subsection{Valid Implications Defined}
Our definition of valid implications is similar to our previous
definition of valid equivalences:
\begin{definition} Suppose that $R$ and $S$ are compound
statements of some independent component statements $P_1,\cdots,P_n$.
If  $R\longrightarrow S$ is a tautology (always true),
then we write 
\begin{equation}R\implies S,\end{equation}
which we then call a 
{\bf valid logical
implication}.\footnote{%
%%% FOOTNOTE
In this text we will differentiate between implications
as statements, such as $P\longrightarrow Q$, which may be 
true or false, and {\bf valid} implications which are
declarations that a particular implication is always true.
For example
$R\implies S$ means $R\longrightarrow S$
is a tautology.  (We similarly differentiated $\longleftrightarrow$
from $\iff$.)
%%% END FOOTNOTE
}
\end{definition}
\bex
Perhaps the simplest example is the following:
$P\implies P.$
This seems obvious enough on its face.  
It can be proved using a truth table (note the vacuous case):\footnote{%
%%% FOOTNOTE
 We could also show that $P\longrightarrow P$ is
a tautology by way of previously proved results.
For instance, with $P\longrightarrow Q\iff(\sim P)\vee Q$
((\ref{AltP->Q}), page~\pageref{AltP->Q}), with 
the part of $Q$ played by $P$, we have
$$P\longrightarrow P\iff (\sim P)\vee P\iff\T,$$
the second statement being in effect our original example of a tautology
(Example~\ref{taut1}, page~\pageref{taut1}).%
%%% END FOOTNOTE
}
\begin{center}
\begin{tabular}{|c||c|c|}
\hline
$P$ &$P$& $P\longrightarrow P$ \\
\hline
T&T&T\\ F&F&T\\
\hline\end{tabular}
\end{center}

\noindent
\eex

Thus we see that there are logical 
\index{implication}%
implications which are 
\index{tautology}%
tautologies.  A slightly 
more complicated---and very instructive---example is the following: 
\bex The following is a valid implication: 
\begin{equation}(P\wedge Q)\implies P.\label{P^Q>Peq}\end{equation}    
To prove this, we will
use a truth table to show that the following is a tautology:
$$(P\wedge Q)\longrightarrow P.$$ 
%We do so with a truth table.
\begin{center}
{\rm
\begin{tabular}{|c|c||c|c|c|}
\hline $P$& $Q$ &$P\wedge Q$&$P$& $(P\wedge Q)\longrightarrow P$\\
\hline
T&T&T&T&T\\
T&F&F&T&T\\
F&T&F&F&T\\
F&F&F&F&T\\ \hline\end{tabular}}
\end{center}
Notice that three of the four cases  have the implication true
vacuously.
\label{P^Q>Pex}\eex

The above example is fairly easy to interpret:  if $P$ and $Q$ are true,
then (of course) $P$ is true.  Another intuitive example follows, bascially 
stating that if we have bi-implication then we have implication.

\bex The following is a valid implication:
  \begin{equation} P\longleftrightarrow Q\implies P\longrightarrow Q.
  \label{P<->Q=>P->Q}\end{equation}
As before, we prove that replacing $\implies$ with $\longrightarrow$
gives us a tautology.
\begin{center}
{\rm
\begin{tabular}{|c|c||c|c|c|}
\hline
$P$&$Q$&$P\longleftrightarrow Q$&$P\longrightarrow Q$
    &$(P\longleftrightarrow Q)\longrightarrow(P\longrightarrow Q)$\\
\hline
T&T&T&T&T\\
T&F&F&F&T\\
F&T&F&T&T\\
F&F&T&T&T\\\hline
\end{tabular}}
\end{center}
Also as
before, we see the importance of the vacuous cases in the final implication
In fact, this is just an application of the previous implication
(\ref{P^Q>Peq}), if we remember that
$P\longleftrightarrow Q$ is equivalent to 
$(P\longrightarrow Q)\wedge(Q\longrightarrow P)$:
$$\underbrace{(P\longrightarrow Q)}_{\text{``$P$''}}
\wedge\underbrace{(Q\longrightarrow P)}_{\text{``$Q$''}}\implies
\underbrace{(P\longrightarrow Q)}_{\text{``$P$''}}.$$
The quotes indicate what roles in (\ref{P^Q>Peq}) are
played by the parts of (\ref{P<->Q=>P->Q}).
\eex


Another interesting valid 
\index{implication}%
implication is given next.
(The reader should reflect on its apparent meaning.)
%PageForThreeStatements
\bex $(P\longrightarrow Q)\wedge(Q\longrightarrow R)
\implies(P\longrightarrow R)$.  
%This is often written:
%\begin{equation}
%P\longrightarrow Q\longrightarrow R\implies P\longrightarrow R.
%\end{equation}\label{P->Q->R=>P->R}

Note that to prove this, we must show that the
following statement is a tautology:   
$$[(P\longrightarrow Q)\wedge(Q\longrightarrow R)]
\longrightarrow(P\longrightarrow R).$$
\begin{center}
\begin{tabular}{|c|c|c||c|c|c|c|c|}
\hline
$P$&$Q$&$R$&$P\rightarrow Q$&$Q\rightarrow R$&
$(P\rightarrow Q)\wedge(Q\rightarrow R)$&$P\rightarrow R$&$\ds{\begin{array}{c}
[(P\rightarrow Q)\wedge(Q\rightarrow R)]\\
\longrightarrow(P\rightarrow R)\end{array}}$\\
\hline
T&T&T&T&T&T&T&T\\
T&T&F&T&F&F&F&T\\
T&F&T&F&T&F&T&T\\
T&F&F&F&T&F&F&T\\\hline
F&T&T&T&T&T&T&T\\
F&T&F&T&F&F&T&T\\
F&F&T&T&T&T&T&T\\
F&F&F&T&T&T&T&T\\
\hline
\end{tabular}
\end{center}

\label{ExampleForshortchainofimplications}
\eex
At this stage in the development it is perhaps best to check
such things using truth tables, however unwieldy they can be.  
Of course with practice comes
intuition, informed memory and many shortcuts, but for now we will use
this brute force method of truth tables to determine if an implication is
valid.

For a simple  algebraic perspective of the difference between
a valid implication and a valid equivalence, consider the
following list of algebraic facts:
\begin{align*}
x=5&\implies x^2=25,\\
x=-5&\implies x^2=25,\\
x^2=25&\iff (x=5)\vee(x=-5).\end{align*}
Knowing $x=5$ is not {\it equivalent} to knowing $x^2=25$.
That is because there is an alternative explanation for
$x^2=25$, namely that perhaps $x=-5$.  
But it is true that knowing $x=5$ {\it implies} knowing---at
least in principle---that $x^2=25$
(just as knowing $x=-5$ implies knowing $x^2=25$).
If  an equivalence is desired, a valid one is that knowing $x^2=25$
{\it is} equivalent to knowing that $x$ must be either $5$ or $-5$.

Later in the text we will briefly focus on algebra in earnest, bringing
our symbolic logic to bear on that topic.
In algebra (and in calculus) it is often important to know
when we have an equivalence and when we have only an implication.
For some  algebraic problems, the implication often means
we need to check our answer while the equivalence means
we do not.  For an example of this phenomenon, consider
\begin{align*}\sqrt{x+2}=x&\implies x+2=x^2\iff 0=x^2-x-2
                        \iff 0=(x-2)(x+1)\\
   &\iff(x-2=0)\vee(x-1=0)\iff(x=2)\vee(x=-1).\end{align*}
We lost the  equivalence at the first step, and
so we can only conclude from the logic that 
$$\sqrt{x+2}=x\implies(x=2)\vee(x=-1).$$
All this tells us is that {\it if} there is a a number $x$ so
that  $\sqrt{x+2}=x$, then
the number must be either $x=2$ or $x=-1$ (or perhaps both work; 
recall that we always interpret {\it or} inclusively).
When we check $x=2$ in the original equation, we get $\sqrt4=2$,
which is true.  However, $x=-1$ gives $\sqrt1=-1$, which is
not true.  (Later we will define $\sqrt{z}$ to be only the nonnegative 
square root of $z$, assuming $z\ge0$ lest $\sqrt{z}$ be undefined,
at least as a {\it real} number.  See the next chapter.)
Since we have now solved the original equation we can
say that $\sqrt{x+2}=x\iff x=2$.

For another example, consider how one can solve a linear equation:
$$2x+1=3\iff 2x=2\iff x=1.$$
Here we subtracted 1 from both sides, and then divided by 2, neither
of which break the logical equivalence. We do not have to check the answer
(unless we believe our arithmetic or reasoning may be faulty).
In Chapter~\ref{RealsChapter} we will delve deeper into algebra.

  
\subsection{Partial List of Valid Implications
\label{ValidImplicationsSubsection}} 
Table~\ref{TableOfValidImplications}, 
page~\pageref{TableOfValidImplications}
lists some  basic valid equivalences and implications.
All can be proved using truth tables.  However, it is important
to learn to recognize validity without always resorting
to truth tables. Each can be viewed in light of English
examples.     Still, it is the
rigorous {\it mathematical} framework which gives us the 
precise rules for rewriting and
analyzing statements.\footnotemark
\footnotetext{%
%%% FOOTNOTE
Similarly we use the  rules of algebra
to rewrite and analyze equations in hopes of solving for the
variables.
%%% END FOOTNOTE
}

\begin{table}
%\begin{center}\underline{\bf{\Large Valid Implications}}\end{center} 
\begin{align}
P\wedge Q&\implies P\label{extraneous}\\
P&\implies P\vee Q\label{weaker->weakest}\\
P\longleftrightarrow Q
  &\implies P\longrightarrow Q\label{EquivImpliesImplication}\\
(P\longrightarrow Q)\wedge
  (Q\longrightarrow R)&\implies P\longrightarrow R
        \label{shortchainofimplications}\\
(P\longleftrightarrow Q)\wedge(Q\longrightarrow R)
  &\implies(P\longrightarrow R)\label{BiImp^Imp->Imp}\\
(P\longrightarrow Q)\wedge(Q\longleftrightarrow R)
    &\implies(P\longrightarrow R)\label{Imp^BiImpl->Imp}\\
(P\longleftrightarrow Q)\wedge(Q\longleftrightarrow R)
    &\implies(P\longleftrightarrow R)\label{ChainOfBiImplications}\\
(P\longrightarrow Q)\wedge P&\implies Q\label{(P>Q)^P>Q}\\
(P\longrightarrow Q)\wedge (\sim Q)&\implies\sim P
         \label{P>QandnotQ>notP}\\
(P\vee Q)\wedge(\sim Q)&\implies P \label{PorQandNotQImpliesP}\\
P\longrightarrow\F&\implies\sim P\label{RedAdAbsurd}\\
\T\longrightarrow P&\implies P\label{IfTrueImpliesPThenP}
\end{align}
\caption{Table of Valid Logical Implications.  If we replace
$\Longrightarrow$ with $\longrightarrow$ in each of the above
(perhaps enclosing each side in brackets $[\cdots]$), we would
have tautologies.}
\label{TableOfValidImplications}
\end{table}

It is useful to see why (\ref{extraneous})--(%
\ref{PorQandNotQImpliesP}) are {\it not} equivalences.\footnote{%
%%% FOOTNOTE
The exceptions
in the table are (\ref{RedAdAbsurd}) and (\ref{IfTrueImpliesPThenP}),
which are valid if
we replace $\Longrightarrow$ with $\Longleftrightarrow$,
as was discussed
in the previous section.
See (\ref{RedAdButAsEquivalence}),
page~\pageref{RedAdButAsEquivalence} and the exercises of that section.
%%% END FOOTNOTE
}          
For instance a little reflection should make clear that\footnote{%
%%% FOOTNOTE
Note that it is common to negate a statement by including a
``slash'' through the main symbol, as in $\sim(x=3)\iff x\ne 3$.
What we mean by $R\,\,\,\,\not\!\!\!\!\implies  S$ is that 
it is not true that $R\longrightarrow S$ is a tautology.  
%
%%% END FOOTNOTE
}
$P\,\,\,\,\not\!\!\!\!\implies P\wedge Q$ (unless there is some
underlying relationship between $P$ and $Q$
which is not stated), and so we can not replace
$\implies$ with $\iff$ in (\ref{extraneous}).
Similarly, in (\ref{shortchainofimplications})
having $P\longrightarrow R$ in itself says nothing about $Q$,
so there is no reason to believe 
$(P\longrightarrow Q)\wedge(Q\longrightarrow R)$
is implied by $P\longrightarrow R$.

Implicit in the above discussion is the fact that
having $\iff$ is the same as simultaneously having 
both $\implies$ and $\Longleftarrow$.  Put another
way, $R\iff S$ is the same as 
collectively having both $R\implies S$ and $S\implies R$.\footnote{%
%%% FOOTNOTE
Note that $R\Longleftarrow S$ would be interpreted as
$S\Longrightarrow R$.  We will not make extensive
use of ``$\Longleftarrow$.''%
%%% END FOOTNOTEon
}
In fact it important to note that all of the valid
logical equivalences, for 
instance in Table~\ref{TableOfCommonValidEquivalences},
page~\pageref{TableOfCommonValidEquivalences} can be
also considered to be combinations of two valid implications,
one with $\Longrightarrow$, and the other with $\Longleftarrow$,
replacing $\Longleftrightarrow$.  We do not list them all here,
but rather list the most commonly used implications which are
not equivalences, again except for the last two in
the table.


\subsection[Fallacies and Valid Arguments]%
{Fallacies and Valid Arguments\footnotemark}

\begin{quote}
{\it The name {\bf fallacy} is usually reserved for {\rm typical} 
faults in arguments 
that we nevertheless find persuasive. Studying them is therefore a 
good defense against deception.}
\newline
---Peter Suber, Department of Philosophy, Earlham College, 
Richmond, Indiana, 1996.
\end{quote}


Here we look at some classical argument styles, some of which
are valid, and some of which are invalid and therefore called 
{\it fallacies}.  The valid styles will mostly mirror the valid 
logical implications of Table~\ref{TableOfValidImplications}.

\footnotetext{%
%%% FOONOTE
Valid argument forms are also called {\it rules of inference.}
%%% END FOONOTE
}

A common  method for diagramming simple arguments is to have a
horizontal line separating the {\it premises}\,\footnote{%
%%% FOOTNOTE
Premises are also called {\it hypotheses}.  The singular forms
are premise and hypothesis.%
%%% END FOOTNOTE
} from the {\it conclusions}.  Usually we will have multiple premises and
a single conclusion.  For style considerations,
the conclusion is often announced with the
symbol $\therefore$ which is read ``therefore.''   

In this subsection
we look at several of these arguments, both valid and fallacious.
Many are classical, with classical names.  We will see how to 
analyze arguments for validity.  In all cases here, it will
amount to determining if a related implication is valid.
\newpage
\bex Our first example we consider is the argument form which is 
classically known as {\bf modus ponens}, or {\bf law of
detachment}.  It is outlined as follows:\footnote{%%
%%% FOOTNOTE
As pointed out in J.E.\ Rubin's {\it Mathematical Logic: Applications
and Theory} (Saunders, 1990), 
the idea is that we
can validly ``detach'' the consequent
$Q$ of the conditional $P\longrightarrow Q$ when we also assume
the antecedent $P$.
%%% END FOOTNOTE
}
\begin{center}
\begin{tabular}{c}
$P\longrightarrow Q$\\ $P$\\ 
\hline
$\therefore Q$
\end{tabular}
\end{center}
The idea is that if we {\bf assume} $P\longrightarrow Q$ and $P$ are
true, then we must {\bf conclude} that $Q$ is also true.
This is ultimately an implication. 
The key is that checking
to see if this is valid is the same as checking to see if 
$$(P\longrightarrow Q)\wedge P\implies Q,$$
i.e., that
$$[(P\longrightarrow Q)\wedge P]\longrightarrow Q$$
is a tautology.  We know this to be the case already, as this is
just (\ref{(P>Q)^P>Q}).  To emphasize again how we can thus
show the argument was valid, we again produce the truth table
to show the above statement is a tautology, i.e., has
truth value {\rm T} for all cases:
\begin{center}{\rm
\begin{tabular}{|c|c||c|c|c|c|}
\hline
$P$&$Q$&$P\rightarrow Q$&$(P\rightarrow Q)\wedge P$&$Q$
&$[(P\rightarrow Q)\wedge P]\longrightarrow Q$\\ \hline
T&T&T&T&T&T\\
T&F&F&F&F&T\\
F&T&T&F&T&T\\
F&F&T&F&F&T\\
\hline
\end{tabular}}\end{center}
\label{ModusPonensExample}\eex

Thus
to test the validity of an argument
is to test whether or not the argument, as diagrammed as
an implication, is a tautology.
This gives us a powerful tool to analyze the classical argument styles.
It also connects some of our symbolic logic to this style of diagramming
arguments, so the intuition
of these two flavors of logic can illuminate each other.

To repeat and emphasize
the criterion for validity we list the following definition:
\begin{definition}
A {\bf valid argument} is one which, when diagrammed as an implication,
is a tautology.  In other words, if the premises are
$P_1,P_2\cdots,P_n$ and the conclusion is $Q$, then the argument
is valid if and only if 
$$P_1\wedge P_2\wedge\cdots\wedge P_n\Longrightarrow Q,$$
i.e., $\left[P_1\wedge P_2\wedge\cdots\wedge P_n\right]\longrightarrow
Q$ is a tautology.
If not, then the argument is a {\bf fallacy}.
\end{definition}

Note that the validity of any argument has nothing to do with
the truth or falsity of the conclusion.  Indeed the above argument
is perfectly valid, regardless of whether or not $Q$ is true.
That is because we do not know for sure whether or not the 
premises are true.  What we do know is that, {\it if the premises
are true, then so is the conclusion}.  In other words, the statement
$[(P\rightarrow Q)\wedge P]\longrightarrow Q$ is always true.
(When just one  premises is false, the implication is true vacuously.)
Of course a statement which is always true is also known as a 
tautology.

One example often used to shed light on the law of 
detachment above, and other argument styles as well, is the
following choice for $P$ and $Q$.
\begin{align*}
P:&\text{ It rained}\\
Q:&\text{ The ground is wet}
\end{align*} 
The argument above would then be diagrammed
\begin{center}
\begin{tabular}{c}
If it rained, then the ground is wet.\\
It rained.\\
\hline
$\therefore$ The ground is wet.
\end{tabular}
\end{center}
This is a perfectly valid argument, meaning that if we accept the premises
we must accept the conclusion. In other words, the logic is flawless.
That said, one need not necessarily accept the conclusion
just because the argument is valid, since one can
always debate the truthfulness of the premises.
Again the point is the logic here is valid, even if the premises 
may be faulty.\footnote{%
%%% FOOTNOTE
In mature philosophical discussions, the logic is rarely in question because
the valid models of argument are well known.  When a conclusion
seems unacceptable or just questionable, 
it is usually the premises which then come under scrutiny.
%%% END FOOTNOTE
}

Next we look at an example of an invalid argument, i.e., a fallacy.
The following is called the {\bf fallacy of the converse}:\footnote{%%
%%% FOOTNOTE
The {\it converse} of an implication $R\longrightarrow S$ is 
the statement $S\longrightarrow R$ (or $R\longleftarrow S$
if we want to preserve the order).  An implication and
its converse are not logically equivalent, as a quick check of their
truth tables would reveal.  However, it is a common mistake to 
forget which direction an implication follows, or to just be
careless and mistake an implication for a bi-implication.
The ``fallacy of the converse'' refers to a state of mind where
one mistakenly believes the converse true, based upon the
assumption that the original implication is true.
(Note that if we mistakenly replace $P\longrightarrow Q$ with 
$Q\longrightarrow P$
in Example~\ref{FallacyOfTheConverse}, the new argument is valid.
In fact it is {\it modus ponens}.)
%
%%%% END FOOTNOTE
\label{FootnoteOnConverse}}
\bex Show that the following argument is a fallacy:
\begin{center}
\begin{tabular}{c}
$P\longrightarrow Q$\\ $Q$\\ \hline $\therefore P$ (Invalid)
\end{tabular}\end{center}
As before, we analyze the corresponding implication,
in this case
$[(P\rightarrow Q)\wedge Q]\longrightarrow P$, with a truth table:
\begin{center}
{\rm
\begin{tabular}{|c|c||c|c|c|c|}
\hline
$P$&$Q$&$P\rightarrow Q$&$(P\rightarrow Q)\wedge Q$
&$P$&$[(P\rightarrow Q)\wedge Q]\longrightarrow P$\\
\hline
T&T&T&T&T&T\\
T&F&F&F&T&T\\
F&T&T&T&F&F\\
F&F&T&F&F&T\\ \hline
\end{tabular}
}\end{center}
\label{FallacyOfTheConverse}\eex

It is always useful to review an invalid argument to see which
conditions were problematic.  In the third line,
$P\rightarrow Q$ is vacuously true, and $Q$ is true so the premises
hold true, but the conclusion $P$ is false (which was why
$P\rightarrow Q$ was vacuously true!).
From a more common-sense  standpoint, while $P\rightarrow Q$ is
assumed, 
$P$ may not be the only condition which forces $Q$ to be
true.  (If it were, we would instead have $P\longleftrightarrow Q$.)
Consider again our previous choices for $P$ and $Q$:
\begin{center}
\begin{tabular}{c}
If it rained, the ground is wet.\\
The ground is wet.\\
\hline
$\therefore$ It rained. (Invalid)
\end{tabular}
\end{center}
Even if the premises are correct,
the ground being wet does not guarantee that it rained.
Perhaps it is wet from dew, or a sprinkler, or flooding
from some other source.  Here one can accept the premises, 
but the conclusion given above is not valid.

There is a subtle---perhaps difficult---general point in
this subsection which
bears repeating.  The truth table associated with an argument
reflects the validity or invalidity of the logic of the
{\bf argument}
(really, the underlying implication),
regardless of the truthfulness of the premises.  Indeed,
note how the truth table for the valid form {\it modus ponens}
of Example~\ref{ModusPonensExample} (page~\pageref{ModusPonensExample})
contains cases where the premises, $P\longrightarrow Q$ and $P$,
can have truth value F as well as T.\footnote{%
%%% FOOTNOTE
Another, perhaps more subtle point here, is the extensive
role of the vacuous cases in the underlying implication
of an argument.  If the premises are not true, then
the conclusion can not contradict them.  This gives rise
to some strange forms of argument indeed, for the premises
can be self-contradictory and therefore, taken as a group
of statements joined by $\wedge$, can be equivalent to
$\F$.  (Recall $\F\rightarrow P$ is a tautology.)
However, the only practical uses of arguments come when
we know the premises to be true, or we think they are false and
demonstrate it by showing the valid conclusions they imply are 
demonstrably false.  The latter use is often called {\it proof
by contradiction} or {\it indirect proof}, but there are many
structures which use the same idea.  {\it Modus tollens} 
(Example~\ref{ModusTollensExample}) is one
permutation of the idea behind indirect proof.
%%% END FOOTNOTE
\label{FootnoteOnVacuousCasesInArguments}}

For completeness, we mention that some use the adjective {\it sound}
to describe an argument which is not only valid, but whose
premises (and therefore conclusions) are in fact true.
For this text, we are mostly interested in abstract, valid arguments,
but we will be interested in their ``soundness'' when we
consider applications of our analyses.




The next example is also very common.  It is a valid form
of argument often known by its Latin name {\bf modus tollens}.
\bex Analyze the following {\bf modus tollens} argument.
\begin{center}
\begin{tabular}{c}
$P\longrightarrow Q$\\
$\sim Q$\\
\hline
$\therefore\ \sim P$
\end{tabular}
\end{center}
As before, we analyze the following associated implication 
(which we leave as a single-arrow implication until we establish
it is a tautology):
$$[(P\rightarrow Q)\wedge(\sim Q)]\longrightarrow(\sim P).$$
\begin{center}
{\rm
\begin{tabular}{|c|c||c|c|c|c|c|}
\hline
$P$&$Q$&$P\rightarrow Q$&$\sim Q$&$(P\rightarrow Q)\wedge(\sim Q)$
&$\sim P$&$[(P\rightarrow Q)\wedge(\sim Q)]\longrightarrow(\sim P)$
%\vphantom{$\ds{\frac11}$}
\\
\hline
T&T&T&F&F&F&T\\
T&F&F&T&F&F&T\\
F&T&T&F&F&T&T\\
F&F&T&T&T&T&T\\
\hline
\end{tabular}
}\end{center}
That the final column is all {\rm T}'s thus establishes its
validity.
In fact, we see that the argument above is just a re-diagrammed
version of (\ref{P>QandnotQ>notP}), page~\pageref{P>QandnotQ>notP}.
\label{ModusTollensExample}\eex
A short chapter could be written just on the insights
which can be found studying the above {\it modus tollens} argument.
For instance, 
for a couple of reasons one could make the case that
{\it modus tollens} and {\it modus ponens} are the same type
of argument.  We will see one of these reasons momentarily, but
first we will look at {\it modus tollens}
by itself.  Inserting our previous $P$ and $Q$ into this form,
we would have
\begin{center}
\begin{tabular}{c}
If it rained, the ground is wet.\\
The ground is not wet.\\
\hline
$\therefore$ It did not rain.
\end{tabular}
\end{center}
The validity of the above argument should be intuitive.
A common way of explaining it is that it must not have rained,
because (first premise) if it had rained the ground would be
wet, and (second premise) it is not wet.  Of course that explanation
is probably no simpler than just reading the argument as it stands.

Another way to look at it is to recall the equivalence of the 
implication to the contrapositive ((\ref{contrap}),
page~\pageref{contrap} and elsewhere):
$$P\longrightarrow Q\iff(\sim Q)\longrightarrow(\sim P).$$
Thus we can replace in the {\it modus tollens}
argument the first premise, $P\rightarrow Q$, 
with its contrapositive:
\begin{center}
\begin{tabular}{c}
$(\sim Q)\longrightarrow(\sim P)$\\
$\sim Q$\\ \hline
$\therefore\ \sim P$
\end{tabular}
\end{center}
This is valid by {\it modus ponens}, with the part of 
$P$ there played by $\sim Q$ here, and the part of $Q$ by
$\sim P$.
Further uniting {\it modus ponens} and {\it modus tollens}
is the valid form of argument of the next example.
(How this unites the previous valid forms will be explained
after the example.)
\bex Consider the following form of argument, which is 
valid.\footnote{%
%%% FOOTNOTE
Some texts call this {\it disjunctive syllogism}.%
%%% END FOOTNOTE
}
\begin{center}
\begin{tabular}{c}
$P\vee Q$\\
$\sim P$\\ \hline
$\therefore Q$
\end{tabular}
\end{center}
The proof of this is left as an exercise.  To prove this one
needs to show
$$(P\vee Q)\wedge(\sim P)\Longrightarrow Q,$$
that is, show $[(P\vee Q)\wedge(\sim P)]\longrightarrow Q$
to be a tautology as before.
Of course the idea of this argument style is
that when we assume ``$P$ or $Q$'' to be true, and assume
$P$ is false (by assuming $\sim P$
is true), we are forced to conclude $Q$
must be true.  Note that this is just a re-diagrammed 
version of (\ref{PorQandNotQImpliesP}), page~\pageref{PorQandNotQImpliesP}
except with $P$ and
$Q$ exchanging roles.
For an example, we will use a different pair of statements $P$ and $Q$:
\begin{align*}
P:&\text{ I will eat pizza}\\
Q:&\text{ I will eat spaghetti}\end{align*}
The argument above becomes (after minor colloquial adjustment):
\begin{center}
\begin{tabular}{c}
I will eat pizza or spaghetti.\\
I will not eat pizza.\\ \hline
$\therefore$ I will eat spaghetti.
\end{tabular}
\end{center}



\eex

To see how this unifies both {\it modus ponens} and {\it modus tollens}
as two manifestations of the same principle, recall that
$$P\longrightarrow Q\iff(\sim P)\vee Q.$$
This was (\ref{AltP->Q}), page~\pageref{AltP->Q} and for instance.
Thus the {\it modus ponens} and {\it modus tollens} become, 
respectively,\footnotemark
\begin{multicols}{2}
\begin{center}
\begin{tabular}{c}
$(\sim P)\vee Q$\\
$\sim(\sim P)$\\
\hline
$\therefore Q$
\end{tabular}

\begin{tabular}{c}
$(\sim P)\vee Q$\\
$\sim Q$\\
\hline
$\therefore\ \sim P.$
\end{tabular}
\end{center}
\end{multicols}
Compare these to the original forms, respectively:
\begin{multicols}{2}
\begin{center}
\begin{tabular}{c}
$P\longrightarrow Q$\\
$P$\\
\hline
$\therefore Q$
\end{tabular}

\begin{tabular}{c}
$P\longrightarrow Q$\\
$\sim Q$\\
\hline
$\therefore\ \sim P.$
\end{tabular}
\end{center}
\end{multicols}




For another fallacy, consider the {\bf fallacy of the inverse}:\footnote{%
%%% FOOTNOTE
The {\bf inverse} of an implication $R\longrightarrow S$ is the statement
$(\sim R)\longrightarrow (\sim S)$.  It is {\it not} equivalent to the
original implication.  In fact, it is equivalent to the converse
(see Footnote~\ref{FootnoteOnConverse}, page~\pageref{FootnoteOnConverse}),
a fact which is left to the exercises.  A course on logic would 
emphasize these two statements which are related to the implication.
However, they are a source of some confusion so we do not elaborate
extensively here.  It is much more important to realize that
$R\longrightarrow S$ {\it is} equivalent to its contrapositive
$(\sim S)\longrightarrow(\sim R)$, and {\it not} equivalent to 
these other two related implications, namely the 
converse $S\longrightarrow R$ and inverse $(\sim R)\longrightarrow(\sim S)$.
These facts should become more self-evident as the material is studied
and utilized.
%%% END FOOTNOTE
}
\bex Show that the following statement is a fallacy.
\begin{center}
\begin{tabular}{c}
$P\longrightarrow Q$\\
$\sim P$\\ \hline
$\therefore\ \sim Q$ (Invalid)
\end{tabular}
\end{center}

\underline{Solution}: We check to see if the statement
$$[(P\rightarrow Q)\wedge(\sim P)]\longrightarrow(\sim Q)$$
is a tautology (in which case we could replace the
major operation $\longrightarrow$
with $\Longrightarrow$).
\begin{center}
{\rm
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$P$&$Q$&$P\rightarrow Q$&$\sim P$&$(P\rightarrow Q)\wedge(\sim P)$
&$\sim Q$&$[(P\rightarrow Q)\wedge(\sim P)]\longrightarrow(\sim Q)$\\
\hline
T&T&T&F&F&F&T\\
T&F&F&F&F&T&T\\
F&T&T&T&T&F&F\\
F&F&T&T&T&T&T\\ \hline
\end{tabular}
}\end{center}
We note that the argument---as an implication---is false
in the case $P$ is false and $Q$ is true.  In fact, 
in that case we have both premises of the argument
($P\rightarrow Q$ vacuously, and $\sim P$ obviously),
but not the conclusion.  Let us return to our rain and 
wet grass statements from before:
\begin{center}
\begin{tabular}{c}
If it rained then the grass is wet.\\
It did not rain.\\ \hline
$\therefore$ The grass is not wet. (Invalid)
\end{tabular}
\end{center}
For another example, consider the following obviously fallacious
argument:
\begin{center}
\begin{tabular}{c}
If you drink the hemlock then you will die.\\
You do not drink the hemlock.\\ \hline
$\therefore$ You will not die. (Invalid)
\end{tabular}
\end{center}
There are, of course, other reasons why you might die (or the
grass is wet).  The truth table example which ruins the
bid for the corresponding implication to be a tautology
is the case that you do not drink the hemlock, and still die.
\eex

There are many other forms of argument, both valid and invalid.
The strategy for checking is the same:  to look at the corresponding
implication and see if it is a tautology, the argument being valid
if and only if that implication is a tautology.

Our final example was proved earlier.  It is also the subject of
Example~\ref{ExampleForshortchainofimplications}, 
page~\pageref{ExampleForshortchainofimplications},
and was (\ref{shortchainofimplications}), 
page~\pageref{shortchainofimplications}.  We will not 
rework the truth table here.
\bex
The following is a valid form of argument:
\begin{center}
\begin{tabular}{c}
$P\longrightarrow Q$\\
$Q\longrightarrow R$\\ \hline
$\therefore P\longrightarrow R$
\end{tabular}
\end{center}
For an example, consider the following assignments for $P,Q$ and $R$:
\begin{align*}
P:&\text{ I am paid}\\
Q:&\text{ I will buy you a present}\\
R:&\text{ You will be happy}
\end{align*}
The argument then  becomes the following, the
validity of which is reasonably clear:
\begin{center}
\begin{tabular}{c}
If I am paid, then I will buy you a present.\\
If I will buy you a present, then you will be happy.\\ \hline
$\therefore$ If I am paid, then you will be happy.
\end{tabular}
\end{center}
\eex

Next we look at an example which contains three underlying 
component statements, and three premises.  Before doing so, we 
point out that we can compute the truth tables for 
$P\wedge Q\wedge R$ and $P\vee Q\vee R$ relatively quickly;
the former is true whenever all three are true (and false
if at least one is false), and the latter
is true if any of the three are true (and false only if all
three are false).  The reason we can do this is that
there is no ambiguity in computing, for instance,
$P\wedge(Q\wedge R)$ or $(P\wedge Q)\wedge R$, as these
are known to be equivalent (see page~\pageref{wedgeoverwedge}).
Similarly for $\vee$.\footnote{%%
%%% FOOTNOTE
This is similar to the arithmetic rules that 
$A+B+C=A+(B+C)=(A+B)+C$.  The first expression is not
at first defined {\it per se}, but because the second and third
are the same we allow for the first.  Similarly 
$A\cdot B\cdot C=A\cdot(B\cdot C)=(A\cdot B)\cdot C$.
However, this does not extend to all operations, such as subtraction:
Usually
$A-(B-C)\ne(A-B)-C$, so when we write $A-B-C$ we have to choose one,
and in fact we choose the latter.
Similarly, we have to be careful with other logical operations
besides $\wedge$ and $\vee$.
For instance, $P\longrightarrow Q\longrightarrow R$ would
probably be interpreted $(P\longrightarrow Q)\longrightarrow R$,
but it would depend upon the author.%
%%% END FOOTNOTE
}
To be clearer, we note the truth tables.
\begin{center}
\begin{tabular}{|c|c|c||c|c|}
\hline
$P$&$Q$&$R$&$P\wedge Q\wedge R$&$P\vee Q\vee R$\\
\hline
T&T&T&T&T\\
T&T&F&F&T\\
T&F&T&F&T\\
T&F&F&F&T\\ \hline
F&T&T&F&T\\
F&T&F&F&T\\
F&F&T&F&T\\
F&F&F&F&F\\\hline
\end{tabular}
\end{center}
With this observation, we can more easily analyze arguments
with more than two premises.
\bex Consider the argument 
\begin{center}
\begin{tabular}{c}
$P\longrightarrow (Q\vee R)$\\
$P$\\
$\sim R$\\\hline
$\therefore Q$
\end{tabular}\end{center}
We need to see if the following conditional---which we 
dub $ARG$ (for ``argument'') for space considerations---is a tautology:
$$ARG:\qquad\{[P\longrightarrow(Q\vee R)]\wedge P\wedge(\sim R)\}
       \longrightarrow Q.$$
\begin{center}
\begin{tabular}{|c|c|c||c|c|c|c|c|c|c|c|}
\hline
$P$&$Q$&$R$&$Q\vee R$&$P\rightarrow(Q\vee R)$&$\sim R$&
         $(P\rightarrow(Q\vee R))\wedge P\wedge(\sim R)$
         &$Q$&$ARG$\\
\hline
T&T&T&T&T&F&F&T&T\\
T&T&F&T&T&T&T&T&T\\
T&F&T&T&T&F&F&F&T\\
T&F&F&F&F&T&F&F&T\\
\hline
F&T&T&T&T&F&F&T&T\\
F&T&F&T&T&T&F&T&T\\
F&F&T&T&T&F&F&F&T\\
F&F&F&F&T&T&F&F&T\\
\hline
\end{tabular}
\end{center}
Note that ``$ARG$'' connects the two immediately preceding columns
with the logical operation $\longrightarrow$.  Also notice that
most of the cases are true vacuously (as often happens when
we have more premises to be met, connected by $\wedge$), 
and eventually we see that
the argument is valid.
\eex

Reading more examples, and perhaps some trial and error, 
one's intuition
for what is valid and what is not should develop.  With English
examples one might be able to see that the above argument style is
reasonable.  For instance, consider the following.
\begin{align*}
P&:\text{ I will eat pizza}\\
Q&:\text{ I will drink soda}\\
R&:\text{ I will drink beer}
\end{align*}
Then the premises become
\begin{align*}
P\longrightarrow (Q\vee R)&: \text{ If I eat pizza, then I will
                                   drink soda or beer.}\\
P&: \text{ I will eat pizza.}\\
\sim R&:\text{ I will not drink beer.}\end{align*}
It is reasonable to believe that, after declaring that if I
eat pizza then I will drink soda or beer, and that I indeed will eat 
pizza, but not drink the beer, then I must drink the soda.\footnote{
%%% FOOTNOTE
Or else I was lying when I recited my premises.  The argument, anyhow, is
valid.
%%% END FOOTNOTE
}











\newpage 

\begin{center}\underline{\Large{\bf Exercises}}\end{center}
\bigskip

As before, many of these were proved in the section.  The reader
should attempt to prove these first for himself/herself without referring
back to the section.  Admittedly, there are not that many such
problems which are withing the scope of the text, so we need to
borrow some which have already been presented, to serve as
exercises here.


\begin{multicols}{2}
\begin{enumerate}

\item Consider the statement $P\longrightarrow(\sim P)$.
\begin{enumerate}[(a)]
\item Use a truth table to prove the validity of
$P\longrightarrow(\sim P)
        \implies(\sim P)$.  Is this reasonable?
\item If possible without truth tables, 
and using a short string of known equivalences,
show that in fact
$$P\longrightarrow (\sim P)\iff(\sim P).$$
(See (\ref{AltP->Q}), page~\pageref{AltP->Q} and
(\ref{Idempotents}), page~\pageref{Idempotents}.)
\end{enumerate}


\item Use truth tables to show (\ref{extraneous})
and (\ref{weaker->weakest}):
\begin{align*}
&P\wedge Q\implies P,\\
&P\implies P\vee Q.\footnotemark\end{align*}
\footnotetext{Some would characterize $P\wedge Q\implies
P\implies P\vee Q$ to be a progression from the
strongest statement, $P\wedge Q$, to the weakest,
$P\vee Q$ of the three. A similar form would be
$P\wedge Q\implies P\implies P\vee R$.}

\item Prove the following without using truth tables.
$$P\Longrightarrow(P\vee Q)$$
by proving that $P\longrightarrow(P\vee Q)$
is a tautology.  (Again, 
see (\ref{AltP->Q}), page~\pageref{AltP->Q}.)


\item Prove the following two valid implications:

\begin{enumerate}
\item(\ref{(P>Q)^P>Q}): $ (P\longrightarrow Q)\wedge P
              \implies Q$.
\item (\ref{P>QandnotQ>notP}): $(P\longrightarrow Q)\wedge (\sim Q)
               \implies\sim P$.
 \end{enumerate}



%\footnotetext{The statements $P\longrightarrow Q$ and
%$Q\longrightarrow P$ are called {\it converses} of each other.
%{\bf They are not logically equivalent}, as a moment's
%reflection would make clear. If they
%are both true, then we have $P\longleftrightarrow Q$.
%It is easy to see that, in Exercise \ref{conversesexercise},
%we do have both on the left of the logical equivalence sign. }



\item Prove the following using truth tables.
Note that there are $2^4=16$ different combinations
of truth values for $P$, $Q$, $R$ and $S$.
\begin{multline}
(P\longrightarrow R)\wedge(Q\longrightarrow S)
\\ \implies (P\wedge Q)\longrightarrow(R\wedge S)
\label{good_for_chains_with_wedge},\end{multline}
\begin{multline}(P\longrightarrow R)\wedge(Q\longrightarrow S)
\\ \implies (P\vee Q)\longrightarrow(R\vee S)
\label{good_for_chains_with_vee}\end{multline}
These are useful because we often have a string
of statements $A_1,\cdots,A_n$ connected entirely by $\wedge$
or entirely by $\vee$, and wish to replace them 
with $B_1,\cdots,B_n$ (or just some of them), where
$A_1\implies B_1,\ 
A_2\implies B_2,\  \cdots,\ A_n\implies B_n$.\footnotemark
\footnotetext{Here the parts of $P\longrightarrow R$
and $Q\longrightarrow S$  are played
by the $A_i\Longrightarrow B_i$, but the idea is the same. 
We could instead attach $\T\iff (A_1
\longrightarrow B_1)\wedge\cdots\wedge(A_n
\longrightarrow B_n)$ to the left-hand sides
of (\ref{strong_and_strong=>weak_and_weak}) and 
(\ref{strong_or_strong=>weak_or_weak})
 with the wedge operation, since $\T\wedge U\iff U$.}  
With (\ref{good_for_chains_with_wedge}) and
(\ref{good_for_chains_with_vee}) we can generalize
and write
\begin{gather}
\begin{split}
A_1\wedge A_2\wedge\cdots\wedge A_n
\\\implies B_1\wedge B_2\wedge\cdots\wedge B_n,
\label{strong_and_strong=>weak_and_weak}\end{split}\\
\begin{split}A_1\vee A_2\vee\cdots\vee A_n
\\ \implies B_1\vee B_2\vee\cdots\vee B_n.
\label{strong_or_strong=>weak_or_weak}\end{split}
\end{gather}

Valid implications (\ref{good_for_chains_with_wedge}) and
(\ref{good_for_chains_with_vee}) would be quite laborious 
to prove without truth tables. 
\item For each of the following, decide if you believe it is a valid 
argument or a fallacy.  Then check by constructing the corresponding 
truth table.
{\begin{multicols}{2}
\begin{enumerate}[(a)]
\item \begin{tabular}{c} $P\longrightarrow Q$\\ $Q\longrightarrow P$\\
                           \hline $\therefore P\longleftrightarrow Q$
      \end{tabular}\bigskip
\item \begin{tabular}{c}$P\longleftrightarrow Q$\\ $P$\\\hline $\therefore Q$
      \end{tabular}\bigskip
\item \begin{tabular}{c}$P\longrightarrow Q$\\ $Q$\\ \hline $\therefore P$
      \end{tabular}
\item \begin{tabular}{c}$P\longleftrightarrow Q$\\$Q$\\\hline$\therefore P$
      \end{tabular}\bigskip
\item \begin{tabular}{c}$\sim(P\longrightarrow Q)$\\$P$\\\hline
                        $\therefore\sim Q$
      \end{tabular}\bigskip
\item \begin{tabular}{c}$P$\\$\sim P$\\\hline$\therefore Q$\end{tabular}
\bigskip
\end{enumerate}
\end{multicols}}
(For this last case,
see Footnote~\ref{FootnoteOnVacuousCasesInArguments}, 
page~\pageref{FootnoteOnVacuousCasesInArguments}.)
\item For each of the following, decide if it is valid or
a fallacy.  For those marked ``(Prove),'' offer a truth table
proof of your answer.
\begin{enumerate}[(a)]
\item \begin{tabular}{c}$(P\wedge Q)\longrightarrow R$\\
                         $P$\\$Q$\\\hline$\therefore R$
      \end{tabular}\bigskip
\item \begin{tabular}{c}$(P\wedge Q)\longrightarrow R$\\
                        $P$\\$\sim R$\\\hline$\therefore\ \sim Q$
      \end{tabular}\bigskip
\item \begin{tabular}{c}$(P\wedge Q)\longrightarrow R$\\
                        $\sim P$\\\hline$\therefore\ \sim R$  (Prove)
      \end{tabular}\bigskip
\item \begin{tabular}{c}$P\longrightarrow Q$\\$Q\longrightarrow R$\\
                        $\sim R$\\\hline$\therefore\ \sim P$ (Prove)
      \end{tabular}\bigskip
\item \begin{tabular}{c}$P\longrightarrow Q$\\$Q\longrightarrow R$\\
                        $\sim P$\\\hline$\therefore\ \sim R$
      \end{tabular}\bigskip
\item \begin{tabular}{c}$P\longrightarrow Q$\\$Q\longrightarrow R$\\
                        $R\longrightarrow P$\\\hline
                        $\therefore\ (P\longleftrightarrow R)$
      \end{tabular}\bigskip
\item \begin{tabular}{c}$P\vee Q\vee R$\\$\sim P$\\$\sim R$\\\hline
                        $\therefore Q$ (Prove)
      \end{tabular}\bigskip
\item \begin{tabular}{c}$P\longrightarrow Q$\\$Q\longrightarrow(\sim R)$\\
                        $R$\\\hline$\therefore\ \sim P$
      \end{tabular}\bigskip
\item \begin{tabular}{c}$P\vee Q\vee R$\\$P\longrightarrow Q$\\$\sim Q$\\
                        \hline$\therefore R$
       \end{tabular}\bigskip

\item \begin{tabular}{c}$P\longrightarrow(Q\wedge R)$\\
                        $\sim R$\\
                        \hline
                        $\therefore$\ $\sim P$ (Prove)
       \end{tabular}
\item \begin{tabular}{c}$\F$\\ \hline$\therefore P$\end{tabular}
\item \begin{tabular}{c}$P$\\ \hline $\therefore \T$\end{tabular}
\item \begin{tabular}{c}$\T\longrightarrow P$\\ 
\hline $\therefore P$\end{tabular}
\item \begin{tabular}{c}$P\longrightarrow \F$\\ \hline$\therefore \sim P$
\end{tabular}
\end{enumerate}
\end{enumerate}
\end{multicols}
\newpage
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Quantifiers and Sets\label{QuantifiersAndSets}}

In this section we introduce the last of the logic symbols, which
are the quantifiers.  To use quantifiers, we also need the notion
of set.  This section introduces these two concepts to the extent
required for our study of calculus here.  A later section will extend
this introduction, but is only included for the interested reader.
Even with that section, we would only just begin to delve into
these topics if we were studying them for their own sakes.
Again, what we need for calculus is contained in this section.

\subsection{Sets}
Put simply, a set is a collection of objects, which are then
called {\it elements} or {\it members} of the set.  We give sets names
just as we do variables and statements.  
For an example of notation, consider a set $A$ defined by
$$A=\{2,3,5,7,11,13,17\}.$$
We usually define a particular set by
describing or listing the elements between ``curly braces''
$\{\ \}$ (so the reader understands it is indeed a {\it set}
we are discussing).  The definition of 
$A$ above was by a complete listing, but
some sets are too large for that to be possible, let alone practical.
As an alternative, the set $A$ above can also be written
$$A=\{x\,|\ x\text{ is a prime number less than 18}\}.$$
The above equation is usually read, ``$A$ is the set of all $x$
such that $x$ is a prime number less than 18.''  Here $x$ is
a ``dummy variable,'' used only briefly to describe the set.\footnote{%
%%% FOOTNOTE
``Dummy variables'' are also used to describe the actions of  functions,
as in $f(x)=x^2+1$.  In this context, the function is considered to
be the {\it action} of taking an input number, squaring it, and adding
1.  The $x$ is only there so we can easily trace the action
on an arbitrary input.  We will revisit functions later.
%%% END FOOTNOTE
}
Sometimes it is convenient to simply write
$$A=\{\text{prime numbers between 2 and 17, inclusive}\}.$$
(Usually ``inclusive'' is meant by default, so we here we would include
2 and 17 as possible elements, if they also fit the rest of the description.)
Of course there
are often several ways of describing a list of items.  For instance,
we can replace ``between 2 and 17, inclusive'' with ``less than 18,''
as before. 

Often an ellipsis ``$\cdots$'' is used when a pattern should be
understood from a partial listing.  This is particularly useful if
a complete listing is either impractical or impossible.  For instance,
the set $B$ of integers from 1 to 100 
could be written
$$B=\{1,2,3,\cdots,100\}.$$
To note that an object is in a set, we use the symbol $\in$.  For instance
we may write $5\in B$, read ``5 is an element of $B$.''  To
indicate concisely that 5, 6, 7 and 8 are in $B$, we can write
$5,6,7,8\in B$.

Just as we have use for zero in addition, we also define
the {\it empty set}, or {\it null set} as the set which
has no elements.  We denote that set $\varnothing$.  Note
that $x\in\varnothing$ is always false, i.e., 
$$x\in\varnothing\iff \F,$$
because it is impossible to find any element of any kind inside $\varnothing.$
We will revisit this set in the optional, more advanced section
on sets and quantifiers.


Of course for calculus we are mostly interested in sets of numbers.
While not the most important,
the following three sets will occur from time to time in this text:
\begin{alignat}{3} 
&\text{\underline{Natural Numbers\footnotemark:}}
        &\qquad&&\mathbb{N}&=\{1,2,3,4,\cdots\},\\ 
&\text{\underbar{Integers:}}&&&
        \mathbb{Z}&=\{\cdots,-3,-2,-1,0,1,2,3,\cdots\},\\
&\text{\underline{Rational Numbers:}}&&&\mathbb{Q}&=
        \left\{\left.\frac{p}q\ \right|\ (p,q\in\mathbb{Z})\wedge(q\ne0) 
\right\}.\end{alignat} 
\footnotetext{The natural numbers are also called {\it counting} numbers
in some texts.}%
Here  use the ellipsis to show that the established pattern continues
forever.
The sets  $\mathbb{N,\ Z}$ and $\mathbb{Q}$ are
examples of {\it infinite} sets, i.e., sets that do not have a finite
number of elements.  The rational
numbers are those which are {\it ratios} of integers, except that
division by zero is not allowed, for reasons we will consider
later.\footnote{%%%
%%%% FOOTNOTE
For a hint, think about what should be $x=1/0$.  If we multiply
both sides by zero, we might think we get
$0x=(1/0)\cdot0$, giving $0=1$, which is absurd.  
In fact there was no such $x$, so $x=1/0\rightarrow 0=1$, which
is of the form $P\longrightarrow\F$ which we may recall to be
equivalent to $\sim P$.
%%% END FOOTNOTE
}


\begin{figure} 

\begin{center}
\begin{pspicture}(-6,-1)(6,1)
\psline{<->}(-6,0)(6,0)
\psline(-5,-.15)(-5,.15)
 \rput(-5.1,-.5){$-5$}
\psline(-4,-.15)(-4,.15)
 \rput(-4.1,-.5){$-4$}
\psline(-3,-.15)(-3,.15)
 \rput(-3.1,-.5){$-3$}
\psline(-2,-.15)(-2,.15)
 \rput(-2.1,-.5){$-2$}
\psline(-1,-.15)(-1,.15)
 \rput(-1.1,-.5){$-1$}
\psline(0,-.15)(0,.15)
 \rput(0,-.5){$0$}
\psline(1,-.15)(1,.15)
 \rput(1,-.5){$1$}
\psline(2,-.15)(2,.15)
 \rput(2,-.5){$2$}
\psline(3,-.15)(3,.15)
 \rput(3,-.5){$3$}
\psline(4,-.15)(4,.15)
 \rput(4,-.5){$4$}
\psline(5,-.15)(5,.15)
 \rput(5,-.5){$5$}

\pscircle*(-2.5,0){.06}
 \rput(-2.5,.5){$-2.5$}

\pscircle*(1.414,0){.06}
 \rput(1.414,.5){$\sqrt2$}

\pscircle*(3.1416,0){.06}
 \rput(3.1416,.5){$\pi$}

\pscircle*(4.8,0){.06}
 \rput(4.8,.5){$4.8$}




\end{pspicture}
\end{center}
\caption{The number line representing
the set $\Re$ of real numbers, with a few points plotted.
On this graph, the hash marks fall at the integers. 
\medskip}\label{numberline}\end{figure}



For calculus the most
important set is the set $\Re$ of {\it real numbers}, which cannot be defined
by a simple listing or by a simple reference to $\mathbb{N}$, $\mathbb{Z}$ or
$\mathbb{Q}$.  One intuitive way to describe the real numbers
is to consider the horizontal {\it number line}, where geometric points on 
the line are represented by their {\it displacements} 
(meaning distances, but counted as positive if to the right
and negative if to the left) from a fixed point.
That fixed point is represented by the number 0, since the fixed
point is a displacement of zero units from itself.
In Figure~\ref{numberline}
the  number line representation of $\Re$
is shown.  Hash marks at convenient intervals are often included.
In this case, they are at the integers.  The arrowheads indicate
the number line is an actual line and thus infinite in both directions.  
The points $-2.5$ and $4.8$ on the graph are not integers, but are
rational numbers, since they can be written
$-25/10=-5/2$, and $48/10=24/5$, respectively.
The points $\sqrt2$ and $\pi$ are real, but not rational, and
so are called {\it irrational}.  To summarize,
\begin{definition}The set of all {\bf real} numbers is 
the set $\Re$ of all possible displacements, to the right
or left, of a fixed point $0$ on a line.  
If the displacement is to the right, the number is the positive
distance from $0$.  If to the left, the number is the negative
of the distance from $0$.\footnote{%
%%% FOOTNOTE
It should be noted that we have to choose a direction to call
``right,'' the other then being ``left.'' It will depend
upon our perspective.  When we look at the Cartesian Plane,
the horizontal axis measures displacements as right (positive horizontal)
or left (negative horizontal), and the vertical
axis measures displacements as upward (positive vertical)
or downward (negative vertical).
%%% END FOOTNOTE
}\end{definition}
Thus 
\begin{equation}\Re=\{\text{displacements from $0$ on the number line}\}.
\end{equation}
This is not a rigorous definition, not least because ``right'' and
``left'' require a fixed perspective.  Even worse,
the definition is really a kind of
``circular reasoning,'' since
we are effectively defining the number line in terms of $\Re$, and
then defining $\Re$ in terms of (displacements on) 
the number line.  We will give a 
more rigorous definition in the next chapter.
For now this should do,
since the number line is a simple and intuitive image. 



\subsection{Quantifiers}%
The three quantifiers used by nearly every professional
mathematician are as follow:
\bigskip

\begin{tabular}{rrl} 
{\bf{universal quantifier}: }&$\forall$,  
\qquad&{\it read, ``for all,'' or ``for every;"}\\
{\bf{existential quantifier}: }&$\exists$, 
\qquad&{\it read, ``there exists;''}\\
{\bf{uniqueness quantifier}: }&$!$, \qquad&{\it read, ``unique.''}
\end{tabular} 
\bigskip

\noindent%
The first two---which we have already seen---are the most important, 
and the third usually appears
only after the second.  Statements which use these quantifiers
are usually of the forms:
\begin{align*}
(\forall x\in S)P(x),\qquad&\text{i.e., for all $x\in S$, $P(x)$ is true;}\\
(\exists x\in S)P(x),\qquad&\text{i.e., there exists an $x\in S$ such that 
        $P(x)$ is true;}\\
(\exists!x\in S)P(x),\qquad&\text{i.e., there exists a unique (exactly
one) $x\in S$ such that}\\&\hphantom{i.e., } P(x)\text{\ is true.}
\end{align*} 
Here $S$ is a set and $P(x)$ is some statement about $x$.  
\noindent The meanings of these quickly become straightforward.
For instance, consider
\begin{align*}
(\forall x\in\Re)(x+x=2x)&:\text{ for all }x\in\Re,\ x+x=2x;\\
(\exists x\in\Re)(x+2=2)&:\text{ there exists (an) }x\in\Re
                              \text{ such that }x+2=2;\\
(\exists !x\in\Re)(x+2=2)&:\text{ there exists a unique }x\in\Re
                  \text{ such that }x+2=2.\\
\end{align*}
All three quantified statements above are true.  In fact
they are true under any circumstances, and can thus be considered
tautologies.  Unlike unquantified statements $P,Q,R,$ etc., 
from our first three sections,
a quantified statement is either true always or false always, and
is thus, for our purposes, equivalent to either $\T$ or $\F$.
Each has to be analyzed on its face, 
based upon known mathematical
principles; we do not have a mechanism analogous to
truth tables to analyze these systematically.\footnote{%%%
%%% FOOTNOTE
This is part of what makes quantified statements interesting!
%%% END FOOTNOTE
}
For a couple more short examples, consider the following from algebra:
\begin{align*}
\T&\iff(\forall x\in\Re)(0\cdot x=0);\\
\F&\iff(\exists x\in\Re)(x^2=-1).
\end{align*}
The optional advanced section shows how 
we can still find equivalent or implied statements
from quantified statements in many circumstances.

\subsection{Statements with Multiple Quantifiers}
Many of the interesting statements in mathematics contain more
than one quantifier.  To illustrate the mechanics of 
multiply quantified statements, we will first
turn to a more worldly setting.  Consider the following sets:
\begin{align*}M&=\{\text{men}\},\\
              W&=\{\text{women}\}.\end{align*}
In other words, $M$ is the set of all men, and $W$ the set of all women.
Consider the statement\footnote{%%
%%% FOOTNOTE
Note that this is of the form
$(\forall m\in M)(\exists w\in W)P(m,w),$
that is, the statement $P$ says something about both $m$ and $w$.
We will avoid a protracted discussion of the difference
between statements regarding one variable object---as in $P(x)$
from our previous discussion---and statements which involve
more than one as in $P(m,w)$ here.  Statements of multiple
(variable) quantities will recur in subsequent
examples.
%%% END FOOTNOTE
}
\begin{equation}
(\forall m\in M)(\exists w\in W)[w\text{ loves }m].\label{AllManExistsWoman}
\end{equation}
Set to English, (\ref{AllManExistsWoman}) could be written, ``for
every man there exists a woman who loves him.''\footnote{%%
%%% FOOTNOTE
At times it seems appropriate to translate ``$\forall$'' 
as ``for all,'' and at other times it seems better to 
translate it as ``for every.''  Both mean the same.
%%% END FOOTNOTE
}
So if (\ref{AllManExistsWoman}) is true, 
we can in principle arbitrarily choose a man $m$, 
and then there is a woman $w$ who loves him.
It is important that the man $m$ was quantified {\it first}.
A common syntax that would be used by a logician or mathematician
would be to say here that, once our choice of
a man is {\it fixed}, we can in principle find a woman who
loves him.  Note that (\ref{AllManExistsWoman}) allows that
different men may need different women to love them, and
also that a given man may be loved by more than one woman.

Alternatively, consider the statement
\begin{equation}
(\exists w\in W)(\forall m\in M)[w\text{ loves }m].
\label{ExistsWomanAllMen}
\end{equation}
A reasonable English interpretation would be, ``there exists a
woman who loves every man.''  Granted that is a summary, for
the word-for-word English would read more like, 
``there exists a woman such that, for every man, she loves him.''
This says something very different from (\ref{AllManExistsWoman}),
because that earlier statement does not assert that we can
find a woman who, herself, loves every man, but that 
for each man there is a woman who loves him.\footnote{%
%%% FOOTNOTE
We do not pretend to know if either (\ref{AllManExistsWoman})
or (\ref{ExistsWomanAllMen}) are true.
%%% END FOOTNOTE
}

We can also consider the statement
\begin{equation}(\forall m\in M)(\forall w\in W)[w\text{ loves }m].
\label{ForallMenForallWomen}\end{equation}
This can be read, ``for every man and every woman, the woman loves the man.''
In other words, every man is loved by every woman.  In this case
we can reverse the order of quantification:
\begin{equation}(\forall w\in W)(\forall m\in M)[w\text{ loves }m].
\label{ForallWomenForallMen}\end{equation}
In fact, if the two quantifiers are the same type---both universal
or both existential---then the order does not matter.
Thus
\begin{align*}
(\forall m\in M)(\forall w\in W)[w\text{ loves }m]
       &\iff (\forall w\in W)(\forall m\in M)[w\text{ loves }m],\\
(\exists m\in M)(\exists w\in W)[w\text{ loves }m]
       &\iff (\exists w\in W)(\exists m\in M)[w\text{ loves }m].
\end{align*}
In both representations in the existential
statements, we are stating that there is at least one man and
one woman such that she loves him.  In fact that above equivalence
is also valid if we replace $\exists$ with $\exists !$, though
it would mean then that there is exactly one man and exactly one woman
such that the woman loves the man, but we will not delve too deeply
into uniqueness here.

Note that in cases where the sets are the same, we can combine 
two similar quantifications into one, as in
\begin{equation}
(\forall x\in\Re)(\forall y\in\Re)[x+y=y+x]
\iff (\forall x,y\in\Re)[x+y=y+x].\end{equation}
Similarly with existence.  

However, we repeat the point at the beginning of the subsection,
which is that the order does matter if the types of quantification
are different.


For another, short  example which is algebraic in nature, consider
\begin{equation}
(\forall x\in\Re)(\exists K\in\Re)(x=2K).
\end{equation}
This is read, ``for every $x\in\Re$, there exists $K\in\Re$
such that $x=2K$.''  
That $K=x/2$ exists makes this true,
while it would be false if we were to reverse the
order of quantification:
\begin{equation}(\exists K\in\Re)(\forall x\in\Re)(x=2K).\label{Duhyee}
\end{equation}
Statement (\ref{Duhyee}) claims (erroneously) that there exists $K\in\Re$
so that, for every $x\in\Re$, $x=2K$.  That is impossible, because
no $K$ is half of {\it every} real number $x$.  Indeed, the $K$
which works for $x=100$ is not the same as the $K$ which works
for $x=1,000,000$.









\subsection{Detour: Uniqueness as an Independent Concept}
We will have several statements in the text which include uniqueness.
However, most of those will not require us to rewrite the statements
in  ways which require  actual manipulation of the uniqueness
quantifier.  Still, it is worth noting a couple of interesting points
about this quantifier.

First we note that uniqueness can be formulated
as a separate concept from existence, albeit instead requiring the
universal quantifier.
The idea is that uniqueness states that
\begin{quote}{\it if any two---{\bf{}not necessarily different}---objects 
in $S$ satisfy the same
particular statement $P$, then they must in fact be the same
object. }\end{quote}
Note that there is the vacuous case where nothing satisfies the 
statement $P$, in which case the uniqueness of any such 
hypothetical object is proved but there
is actually no existence.  Consider the following, symbolic 
representation of the uniqueness of an object $x$ which satisfies
$P(x)$:\footnotemark
\begin{equation}
(\forall x,y\in S)[(P(x)\wedge P(y))\longrightarrow x=y].
\label{UniquenessAloneSymbolically}
\end{equation}
\footnotetext{%
%%% FOOTNOTE
The above statement indeed says that any two elements $x,y\in S$
which both satisfy $P$ must be the same.  Note that we use a single
arrow here, because the statement between the brackets [~] is
not likely to be a tautology, but may be true for enough
cases for the entire quantified statement to be true.
Indeed, the symbols $\Longrightarrow$ and $\Longleftrightarrow$
belong {\it between} quantified statements, not {\it inside} them.
%%% END FOOTNOTE
}






Finally we note that a proof of a statement such as $(\exists !x\in S)P(x)$
is thus usually divided into two separate proofs: 
\begin{enumerate}[(1)]
\item{\it Existence:} $(\exists x\in S)P(x)$;
\item {\it Uniqueness:} $(\forall x,y\in S)[P(x)\wedge P(y)\longrightarrow 
                          x=y]$.
\end{enumerate}
For example, in the next chapter we rigorously, axiomatically define the 
set of real numbers $\Re$.  One of the axioms\footnote{
%%% FOOTNOTE
Recall that an {\it axiom} is an assumption,
usually self-evident, from which we can logically argue towards
theorems.  Axioms are also known as {\it postulates}.  
If we attempt to argue only using ``pure logic'' (as a mathematician
does when developing theorems, for instance), it eventually becomes
clear that we still need to make some assumptions
because one can not argue ``from nothing.''  
Indeed,  some ``starting points'' from which to argue
towards the conclusions are required.  These are then called axioms.

The word ``axiomatic'' is often used colloquially  to mean 
clearly evident and therefore not requiring proof.  In 
fact that is not always the case with mathematical axioms.  Indeed,
the postulates required for defining the real numbers seem rather
strange at first.  They were, in fact, developed to be a minimal
number of assumptions required to give the real numbers their apparent
properties which could be observed.  In that case, it seemed we
worked towards a foundation, after seeing the outer structure.
A similar phenomenon can be seem in Einstein's Special Relativity,
where his two simple---yet at the time quite counterintuitive---axioms
were able to completely replace a much larger set of postulates
required to explain many of the electromagnetic phenomena discovered
early during his time, and predict many new phenomena that were later
observed.   
%%% END FOOTNOTE
}
defining the real numbers is the existence of an additive identity:
\begin{equation}
(\exists z\in\Re)(\forall x\in\Re)(z+x=x).\label{82930}
\end{equation}
In fact it follows quickly that such a ``$z$'' must be unique, so we have
\begin{equation}(\exists !z\in\Re)(\forall x\in\Re)(z+x=x).
\label{ExistsUniqueZeroForUniquenessSubsection}\end{equation}
To prove (\ref{ExistsUniqueZeroForUniquenessSubsection}), we need
to prove (1) existence, and (2) uniqueness. In this setting, the
existence is an axiom so there is nothing to prove.  We turn
then to the uniqueness.  A proof is best written in prose,
but it is based upon proving that
$$(\forall z_1,z_2\in\Re)[(z_1\text{ an additive identity}) 
\wedge (z_2\text{ an additive identity})\longrightarrow z_1=z_2].$$
Now we prove this.  Suppose $z_1$ and $z_2$ are additive
identities, i.e., they can stand in for $z$
in (\ref{82930}).  Then under these assumptions, we have
\begin{alignat*}{2}
z_1&=z_2+z_1\qquad\qquad&&\text{(since }z_2\text{ is an additive identity)}\\
   &=z_1+z_2&&\text{(since addition is commutative---order is irrelevant)}\\
   &=z_2&&\text{(since }z_1\text{ is an additive identity)}.
\end{alignat*}
This argument showed that if $z_1$ and $z_2$ are any
real numbers  which act as additive
identities, then $z_1=z_2$.  In other words, if there are
any, there is in fact only one.
Of course, assuming existence we call that unique real number {\it zero}.
(It should be noted that the commutativity used above
is another axiom of the real
numbers.  There are fourteen in all.)

The distinction between existence and uniqueness of an object
with some property $P$ is often summarized
as follows:
\begin{enumerate}[(1)]
  \item Existence asserts that there is at least one such object.
  \item Uniqueness asserts that there is at most one such object.
\end{enumerate}
If both hold, then there is exactly one such object.

\subsection{Negating Universally and Existentially Quantified Statements}
For statements with a single universal or existential quantifier,
we have the following negations.
\begin{align}
\sim[(\forall x\in S)P(x)]&\iff(\exists x\in S)[\sim P(x)],
         \label{NegatingForall}\\
\sim[(\exists x\in S)P(x)]&\iff(\forall x\in S)[\sim P(x)],
         \label{NegatingExists}
\end{align}

The left side of (\ref{NegatingForall}) states that it is not
the case that $P(x)$ is true for all $x\in S$; the 
right side states that there is an $x\in S$ for which
$P(x)$ is false.  We could ask when is it a lie that
for all $x$, $P(x)$ is true?  Exactly when we can find
an $x$ for which $P(x)$ is false, i.e., $\sim P(x)$ is true.


The left side of (\ref{NegatingExists}) states that it is not
the case that there exists an $x\in S$ so that $P(x)$ is true;
the right side says that $P(x)$ is false for all $x\in S$.
When is it a lie that there is an $x$ making $P(x)$ true?
When $P(x)$ is false for all $x$.

Thus when we negate such a statement as
$(\forall x)P(x)$ or $(\exists x)P(x)$, we
change $\forall$ to $\exists$ or vice-versa, and
negate the statement after the quantifiers.

\bex Negate $(\forall x\in S)[P(x)\longrightarrow Q(x)]$.

\underline{Solution}: We will need (\ref{not(P->Q)}), 
page~\pageref{not(P->Q)}, namely $\sim(P\longrightarrow Q)\iff
           P\wedge(\sim Q)$.
\begin{align*}
\sim[(\forall x\in S)(P(x)\longrightarrow Q(x))]&
 \iff(\exists x\in S)[\sim(P(x)\longrightarrow Q(x))]\\
 &\iff(\exists x\in S)[P(x)\wedge(\sim(Q(x)))].\end{align*}
\eex
The above example should also be intuitive.  To say that
it is not the case that, for all $x\in S$, $P(x)\longrightarrow Q(x)$
is to say there exists an $x$ so that we do have $P(x)$, but not
the consequent $Q(x)$.
\bex Negate $(\exists x\in S)[P(x)\wedge Q(x)]$.

\underline{Solution}:  Here we use $\sim(P\wedge Q)\iff(\sim P)\vee(\sim Q)$.
$$\sim[(\exists x\in S)(P(x)\wedge Q(x))]
\iff(\forall x)[(\sim P(x))\vee(\sim(Q(x)))].$$
\eex
This last example shows that if it is not the case that
there exists an $x\in S$ so that $P(x)$ and $Q(x)$ are both true,
that is the same as saying that for all $x$, either
$P(x)$ is false or $Q(x)$ is false.
\subsection{Negating Statements Containing Mixed Quantifiers}
Here we simply apply (\ref{NegatingForall}) and (\ref{NegatingExists})
two or more times, as appropriate.  For a typical case of a statement
first quantified by $\forall$, and then be $\exists$,
we note that we can group these as follows:\footnotemark
$$(\forall x\in S)(\exists y\in T)P(x,y)
\iff(\forall x\in S)[(\exists y\in T)P(x,y)].$$
\footnotetext{%%%
%%% FOOTNOTE
This may not be the most transparent fact, and indeed there are
somewhat deep subtleties involved, but eventually this should be clear.
The subtleties lie in the idea that once a variable is quantified, it
is {\it fixed} for that part of the statement which follows it.
For instance, that part $(\exists y\in T)P(x,y)$ treats $x$ as
if it were ``constant.''
%%% END FOOTNOTE
}
(Here ``$T$'' is another set.)  Thus 
\begin{align*}
\sim[(\forall x\in S)(\exists y\in T)P(x,y)]
&\iff
  \sim\{(\forall x\in S)[(\exists y\in T)P(x,y)]\}\\
&\iff
   (\exists x\in S)\{\sim[(\exists y\in T)P(x,y)]\}\\
&\iff
   (\exists x\in S)(\forall y\in T)[\sim P(x,y)].\end{align*}
Ultimately  we have, in turn, 
the $\forall$'s become $\exists$'s,
the $\exists$'s  become $\forall$'s,
the variables are quantified in the same order as before,
and finally the statement $P$ is replaced by its negation $\sim P$.
The pattern would continue no matter how many universal and
existential quantifiers arise.  (The uniqueness quantifier
is left for the exercises.)
To summarize for the case of two quantifiers,
\begin{align}
\sim[(\forall x\in S)(\exists y\in T)P(x,y)]
&\iff(\exists x\in S)(\forall y\in T)[\sim P(x,y)]\\
\sim[(\exists x\in S)(\forall y\in T)P(x,y)]
&\iff(\forall x\in S)(\exists y\in T)[\sim P(x,y)].\end{align}

\bex Consider the following statement, which is false:
$$(\forall x\in\Re)(\exists y\in \Re)[xy=1].$$
One could say that the statement says every real number $x$ has
a real number reciprocal $y$.  This is false, but before
that is explained, we compute the negation which must be true:
$$\sim[(\forall x\in\Re)(\exists y\in \Re)(xy=1)]
\iff(\exists x\in\Re)(\forall y\in\Re)(xy\ne 1).$$
Indeed, there exists such an $x$, namely $x=0$, such that $xy\ne 1$ for all 
$y$.\footnote{%
%%% FOOTNOTE
Note that in using English, the quantification often
follows after the variable quantified, as in 
Example~\ref{apoisbjh-0u74et} above.  That can become quite confusing
when statements get complicated.  Indeed, much of the motivation
of this section is so that we can use the notation to, in essence,
diagram the logic of such statements, and analyze them to see
if they may be false (by seeing if their negations ring true).%
%%% END FOOTNOTE
}
\label{apoisbjh-0u74et}
\eex
In the above, we borrowed one of the many
convenient mathematical notations for the negations of various
symbols.
Some common negations follow:
\begin{align*}
\sim(x=y)&\iff x\ne y,\\
\sim(x<y)&\iff x\ge y,\\
\sim(x\le y)&\iff x>y,\\
\sim(x\in S)&\iff x\not\in S.\end{align*}
Of course we can negate both sides of any one of these and get,
for example, $x\in S\iff\sim(x\not\in S)$.  Reading one
of these backwards, we can have 
$\sim(x\ge y)\iff x<y$.


\begin{center}\underline{\Large{\bf Exercises}}\end{center}
\bigskip

\begin{enumerate}
\item Consider the sets 
\begin{align*}
P&=\{\text{prisons}\},\\
M&=\{\text{methods of escape}\}.
\end{align*}
For each of the following, write a short English version of the given
statement.
 \begin{enumerate}
 \item $(\forall p\in P)(\exists m\in M)[m\text{ will get you out of }P]$ 
 \item $(\exists m\in M)(\forall p\in P)[m\text{ will get you out of }P]$
 \item $(\exists p\in P)(\forall m\in M)[m\text{ will get you out of }P]$
 \item $(\forall m\in M)(\exists p\in P)[m\text{ will get you out of }P]$
 \item $(\exists m\in M)(\exists p\in P)[m\text{ will get you out of }P]$
 \item $(\forall m\in M)(\forall p\in P)[m\text{ will get you out of }P]$
 \end{enumerate}

\item For parts (a)--(d) above, write the negation of the statment
both symbolically and in English.

\item For each of the following, write the negation of the statement
and decide which is true, the original statement or its negation.
 \begin{enumerate}
 \item $(\exists x\in\Re)(x^2<0)$
 \item $(\forall x\in\Re)(|-x|=x)$
 \item $(\forall x\in\Re)(\exists y\in\Re)(y=2x+1)$
  \end{enumerate}
\item Consider $(\forall c\in C)(\exists b\in B)(b\text{ would buy }c)$.
         Here $C=\{\text{cars}\}$ and $B=\{\text{buyers}\}$.
 \begin{enumerate}
 \item Write in English what this statement says.
 \item Write in English what the negation of the statement should be.
 \item Write in symbolic logic what the negation of the statement
       should be.
 \end{enumerate}
\item Consider the statement $(\forall x,y\in\Re)(x<y\longrightarrow
                                   x^2<y^2)$.
\begin{enumerate}
  \item Write the negation of this statement.
  \item In fact it is the negation that is true.  Can you 
        explain why?
  \end{enumerate}
\item Using the fact that
$$(\exists !x\in S)P(x)
\iff\underbrace{\left[(\exists x\in S)P(x)\vphantom{\frac11}\right]}%
_{\text{existence}}
\bigwedge\underbrace{\left[(\forall x,y\in S)\vphantom{\frac11}
      [(P(x)\wedge P(y))\longrightarrow x=y]\right]}_{\text{uniqueness}},$$
compute the form of the negation of the unique existence:
$$\sim[(\exists !x\in S)P(x)].$$


\end{enumerate}

















\newpage
\section{Sets\label{Sets}}

In this section we introduce set theory, its definitions and
notation, and its connections to the real numbers $\Re$ 
and symbolic logic.

The concept of set is basic to mathematics---so 
basic that students often question how it could be useful.
Any mathematical structure we consider
is arguably either inherent in or imposed upon a set, so it
is important to understand what we mean by the term set.
But set theory is interesting in its own right, for
a couple of reasons which we explore in this section.
First, much of the set theory mirrors what we developed
in our earlier logic sections.  Second, sets can often
be represented visually, which is always useful for
explanatory and analytical purposes.


\subsection{Sets Defined}

In this section we approach set theory visually and intuitively,
while simultaneously introducing all the set-theoretic notation
we will use throughout the text.
To begin we make the following definition:

\begin{definition}A {\bf set} is a well-defined collection of
objects.\end{definition}

By well-defined, we mean that once we define the set, the objects
contained in the set are totally determined, and so any given
object is either in the set or not in the set.  Also, once defined
the set is {\it fixed}, and does not change.  

The objects contained in a set are called {\it elements} or {\it members}
of the set. One way to define a set is to simply list its members, which
is traditionally done
between curly braces $\{\}$. For instance, we could have a set
which contains certain months of the year:
$$A=\{\text{January, March, May, July, August, October, December}\}.$$
To show that a particular object is in a set, we use the
set membership symbol $\in$, which is usually read ``is in''
or ``is an element of.''  For instance, for this set we
can write the true statement
$$\text{July}\in A.$$
It is not uncommon to use a single membership symbol for more than
one member, as in 
$$\text{January, March, May}\in A.$$
To state that an object is {\it not} a 
member of a set we either negate the
statement of membership with our logical negation $\sim$, or
else we use the nonmembership symbol $\notin$:
$$\text{February}\notin A\iff\sim(\text{February}\in A).$$

If two sets $A$ and $B$  have exactly the same elements,
we write $A=B$.  In symbolic logic, we would write
\begin{equation}A=B\iff(\forall x)[(x\in A)\longleftrightarrow (x\in B)].
\label{LogicDefinitionOfSetEquivalence}
\end{equation}
We include the $(\forall x)$ mainly for technical reasons
which will be explored in Section~\ref{SWQ}.

The order we list the elements is not relevant; sets are defined
by exactly which objects are elements, and which are not.
Moreover, it is also irrelevant if objects are listed more than
once in the set.  
Thus 
\begin{align*}
A&=\{\text{August, December, January, July, May, March, October}\}\\
&=\{\text{January, March, May, May, May, July, August, October, December}\}
\end{align*}  
define the same set as before, with exactly 7 elements.




Rather than listing the elements of a set $A$, we can often
define the set more compactly by finding a {\it defining property}
of its elements, as long as it is exactly those elements in the
set---no more and no fewer---which share that property.  For instance,
$$A=\{\text{months of the year which have 31 days}\}$$
also defines the same set as before.  Still another way to define $A$ is to
use a {\it dummy variable} such as $x$:
$$A=\left\{\left.\vphantom{x_0^9}x\ \right|\ 
x\text{ is a month of the year with 31 days}\right\}.$$ 
This is read, ``$A$ is the set of all elements $x$ such that
$x$ is a month of the year with 31 days.'' 
The variable $x$ is called a dummy variable because it is really
only a placeholder in the definition, and disappears into irrelevance
once the 
definition is established.  Any other variable would do equally
well.\footnote{Of course we would not use fixed elements of the set
as ``variables,'' which they are not since each has a unique 
identity.}  

\subsection{Sets of Numbers}

Of course this is a calculus book, and so  we expect to be interested
in sets of numbers.  The following sets will occur often in this text:
\begin{alignat}{3} 
&\text{\underline{Natural Numbers\footnotemark:}}
        &\qquad&&\mathbb{N}&=\{1,2,3,4,\cdots\},\\ 
&\text{\underbar{Integers:}}&&&
        \mathbb{Z}&=\{\cdots,-3,-2,-1,0,1,2,3,\cdots\},\\
&\text{\underline{Rational Numbers:}}&&&\mathbb{Q}&=
        \left\{\left.\frac{p}q\ \right|\ p,q\text{ are integers, }q\ne0 
\right\}.\end{alignat} 
\footnotetext{The natural numbers are also called {\it counting} numbers
in some texts.}
We use the dots ($\cdots$) to show that the established pattern continues,
in these cases forever.\footnotemark\footnotetext{The dots 
are also often used for large finite sets.  For instance,
the set of natural numbers less than or equal to 100 can be given
by $\{x\in\mathbb{N}\ |\ 1\le x\le100\}=\{1,2,3,\cdots,100\}$.} 
The sets  $\mathbb{N,\ Z}$ and $\mathbb{Q}$ are
examples of {\it infinite} sets, i.e., sets that do not have a finite
number of elements.  The rational
numbers are those which are {\it ratios} of integers, except that
division by zero is not allowed (see Principle \ref{NoZeroDivisors},
p.\ \pageref{NoZeroDivisors}).  

For calculus the most
important set is the set $\Re$ of {\it real numbers}, which cannot be defined
by a simple listing.  One way to describe the real numbers
is to consider the horizontal {\it number line}, where geometric points on 
the line are represented by their {\it displacements} 
(meaning ``distances,'' but counted as positive if to the right
and negative if to the left) from a fixed point.
That fixed point is represented by the number 0, since the fixed
point is a displacement of zero units from itself.
In Figure \ref{numberline} the  number line representation of $\Re$
is shown.  Hash marks at convenient intervals are often included.
In this case, they are at the integers.  The arrowheads indicate
the number line is an actual line and thus infinite in both directions.  
The points $-2.5$ and $4.8$ on the graph are not integers, but are
rational numbers, since they can be written
$-25/10=-5/2$ and $48/10=24/5$, respectively.
The points $\sqrt2$ and $\pi$ are real, but not rational, and
so are called {\it irrational}.  To summarize,
\begin{definition}The set of all {\bf real} numbers is 
the set $\Re$ of all possible displacements, to the right
or left, of a fixed point $0$ on a line.  
If the displacement is to the right, the number is the positive
distance from $0$.  If to the left, the number is the negative
of the distance from $0$.\end{definition}
Thus 
\begin{equation}\Re=\{\text{displacements from $0$ on the number line}\}.
\end{equation}
This is not a rigorous definition, not least because ``right'' and
``left'' require a fixed perspective.  Even worse,
the definition is really a kind of
``circular reasoning,'' since
we are effectively defining the number line in terms of $\Re$, and
then defining $\Re$ in terms of (displacements on) 
the number line.  We will give a 
more rigorous definition in Section~\ref{RealNumbersSection}.  
For now this should do,
since the number line is a simple and intuitive image. 

\begin{figure} 

\begin{center}
\begin{pspicture}(-6,-1)(6,1)
\psline{<->}(-6,0)(6,0)
\psline(-5,-.15)(-5,.15)
 \rput(-5.1,-.5){$-5$}
\psline(-4,-.15)(-4,.15)
 \rput(-4.1,-.5){$-4$}
\psline(-3,-.15)(-3,.15)
 \rput(-3.1,-.5){$-3$}
\psline(-2,-.15)(-2,.15)
 \rput(-2.1,-.5){$-2$}
\psline(-1,-.15)(-1,.15)
 \rput(-1.1,-.5){$-1$}
\psline(0,-.15)(0,.15)
 \rput(0,-.5){$0$}
\psline(1,-.15)(1,.15)
 \rput(1,-.5){$1$}
\psline(2,-.15)(2,.15)
 \rput(2,-.5){$2$}
\psline(3,-.15)(3,.15)
 \rput(3,-.5){$3$}
\psline(4,-.15)(4,.15)
 \rput(4,-.5){$4$}
\psline(5,-.15)(5,.15)
 \rput(5,-.5){$5$}

\pscircle*(-2.5,0){.06}
 \rput(-2.5,.5){$-2.5$}

\pscircle*(1.414,0){.06}
 \rput(1.414,.5){$\sqrt2$}

\pscircle*(3.1416,0){.06}
 \rput(3.1416,.5){$\pi$}

\pscircle*(4.8,0){.06}
 \rput(4.8,.5){$4.8$}




\end{pspicture}
%\begin{picture}(250,30)(0,-10) 
%\put(0,0){\vector(1,0){250}}
%\put(0,0){\vector(-1,0){0}} 
%\put(25,-3){\line(0,1){6}}\put(15,-12){$-5$}  
%\put(45,-3){\line(0,1){6}}\put(35,-12){$-4$}     
%\put(65,-3){\line(0,1){6}}\put(55,-12){$-3$}       
%        \put(75,0){\circle*{3}}
%        \put(64,8){$-2.5$}
%\put(85,-3){\line(0,1){6}}\put(75,-12){$-2$}       
%\put(105,-3){\line(0,1){6}}\put(95,-12){$-1$}        
%\put(125,-3){\line(0,1){6}}\put(123,-12){0} 
%\put(145,-3){\line(0,1){6}}\put(143,-12){1}
%        \put(153.3,0){\circle*{3}}
%        \put(145,8){$\sqrt{2}$}
%\put(165,-3){\line(0,1){6}}\put(163,-12){2}
%\put(185,-3){\line(0,1){6}}\put(183,-12){3}
%        \put(187.8,0){\circle*{3}}
%        \put(185,8){$\pi$} 
%\put(205,-3){\line(0,1){6}}\put(203,-12){4}
%        \put(221,0){\circle*{3}}
%        \put(215,8){4.8}
%\put(225,-3){\line(0,1){6}}\put(223,-12){5}
%
%\end{picture}
\end{center}

\caption{The number line representing
the set $\Re$ of real numbers, with a few points plotted.
On this graph, the hash marks fall on the integers. 
\medskip}
%\label{numberline}
\end{figure}


\subsection{Subsets and Set Equality}

When all the elements of a set $A$ are also elements of another set
$B$, we say $A$ is a {\it subset} of $B$.  To express this in set
notation, we write $A\subseteq B$.  In this case we can also
take another perspective, and say $B$ is a {\it superset} of $A$,
written $B\supseteq A$.  Both symbols represent types of
{\it set inclusions}, i.e., they show one set is contained in
another.

A useful graphical device which can
illustrate the notion that $A\subseteq B$ and other set
relations is the {\it Venn Diagram}, as in Figure \ref{SubsetVenn}.
There we see a visual representation of what it means for $A\subseteq B$.
The sets are represented by enclosed areas in which we imagine the
elements reside. In each representation given in Figure \ref{SubsetVenn},
all the elements inside  $A$ are also inside  $B$.
\begin{figure}
\begin{center}\begin{pspicture}(0,0)(11,3) 
\psline(0,0)(0,3)(3,3)(3,0)(0,0)
\pscircle[linewidth=.3mm](1.5,1.5){1.3}
\rput(2,1.5){$B$}
\pscircle[linewidth=.3mm](1,1.3){.5} 
\rput(1,1.3){$A$} 

\psline(4,0)(4,3)(7,3)(7,0)(4,0)
\psline(4.2,.3)(4.4,2.5)(6.5,2.0)(4.2,.3)
\psline(5.3,1.2)(4.5,2)(5.8,1.8)(5.3,1.2)
\rput(5.3,1.63){$A$}
\rput(4.6,1.1){$B$}
\psline(8,0)(8,3)(11,3)(11,0)(8,0)
\pscurve(8.2,.2)(8.4,1.5)(8.2,2.8)(9,2.7)(9.5,2.8)(10.8,1.5)
        (9.8,0.3)(8.5,0.5)(8.2,.2)
\pscurve(9,1)(8.5,1.5)(9.2,2.4)(10.0,2.2)(9.5,2)(9,1) 
\rput(9,2){$A$}
\rput(10,1){$B$}

\end{pspicture}
\end{center}

\caption{Three possible Venn Diagrams illustrating $A\subseteq B$.
What is important is that all elements of $A$ are necessarily contained
in $B$ as well.  We do not necessarily know ``where'' in $A$ are the
elements of $A$, except that they are in the area which is marked by $A$.
Since the area in $A$ is also in $B$, we know the elements of $A$ must
also be contained in $B$ in the illustrations above.}
\label{SubsetVenn}\end{figure}


Using symbolic logic, we can say the following:
\begin{equation}
A\subseteq B\iff (\forall x)(x\in A\longrightarrow x\in B).
\label{LogicDefinitionOfSubset}\end{equation}
  The implication in
(\ref{LogicDefinitionOfSubset}) should seem intuitive.
Perhaps less intuitive are some of the statements which are
logical equivalences of (\ref{LogicDefinitionOfSubset}):
\begin{align*}
A\subseteq B&\iff (\forall x)(x\in A\longrightarrow x\in B)\\
&\iff(\forall x)\left[(\sim(x\in A))\vee(x\in B)\right]\\
&\iff(\forall x)\left[(x\notin A)\vee(x\in B)\right],
\end{align*}
which uses the fact that $P\longrightarrow Q\iff(\sim P)\vee Q$, and
\begin{align*}
A\subseteq B&\iff(\forall x)\left[(\sim(x\in B))\longrightarrow(\sim(x\in A))
               \right]\\
& \iff(\forall x)\left[(x\notin B)\longrightarrow(x\notin A)\right]
\end{align*}
which uses the contrapositive $P\longrightarrow Q\iff
(\sim Q)\longrightarrow(\sim P)$.
Note that we used the shorthand notation
$\sim(x\in A)\iff x\notin A$.

With the logic definitions we can also see a couple of technically 
interesting points about subsets:
\begin{theorem}
For any sets $A$ and $B$, the following hold true:
\begin{align}&A\subseteq A,\qquad\text{and}\label{ASubsetOfA}\\
             &A=B\iff(A\subseteq B)\wedge(B\subseteq A).
                 \label{EqualSetsSubsetsOfEachother}
\end{align}
\end{theorem}
Now we take a moment to remind ourselves of what is meant by {\it theorem:}
\begin{definition}A {\bf theorem} is a statement which we know to be true
because we have a proof of it.\end{definition}
So, one can say that a theorem is a statement which is always true, and
thus somehow equivalent to $\T$.  We will use that fact in
the proof of (\ref{ASubsetOfA}), but for (\ref{EqualSetsSubsetsOfEachother})
we will instead demonstrate the validity of the equivalence ($\iff$).  
For the first statement's proof, we have
$$A\subseteq A\iff
(\forall x)[(x\in A)\longrightarrow(x\in A)]\iff\T.$$
The proof is based upon the fact that $P\longrightarrow P$ is a tautology
(i.e., equivalent to $\T$). 
A glance at a Venn Diagram with a set $A$ can also convince one of this
fact, that any set is a subset of itself.  For the proof of 
(\ref{EqualSetsSubsetsOfEachother}) we offer the following:
\begin{align*}
A=B&\iff(\forall x)[(x\in A)\longleftrightarrow(x\in B)]\\
   &\iff(\forall x)[((x\in A)\longrightarrow(x\in B))\wedge
                    ((x\in B)\longrightarrow(x\in A))]\\
   &\iff\bigl[(\forall x)[(x\in A)\longrightarrow(x\in B)\bigr]\wedge
        \bigl[(\forall x)[(x\in B)\longrightarrow(x\in A)\bigr]\\
   &\iff(A\subseteq B)\wedge(B\subseteq A),\text{ q.e.d.}
\end{align*}
A consideration of Venn diagrams also leads one to believe that
for all the area in $A$ to be contained in $B$ and vice versa,
it must be the case that $A=B$.  That $A=B$ implies they are
mutual subsets is perhaps easier to see.



Note that the above arguments can also be made with supersets
instead of subsets, with $\supseteq$ replacing $\subseteq$ and
$\longleftarrow$ replacing $\longrightarrow$.

One needs to be careful with quantifiers and symbolic logic, as
is discussed later in Section~\ref{SWQ}, but in what we did above the
$(\forall x)$ effectively went along for the ride.

% In order to distinguish when a set $A$ is
%a subset of $B$, but they are not the same sets, we have the following
%definition.\begin{definition}If $A\subseteq B$, and $A\ne B$, we call $A$ a
%{\rm proper} subset of $B$, and write $A\subset B$:\footnotemark 
%\begin{equation}A\subset B\iff(A\subset B)\wedge(A\ne B).\end{equation}
%\end{definition}
%\footnotetext{The notation has changed over the years.
%Many current texts use ``$\subset$'' the way we use 
%``$\subseteq$'' here.  This is unfortunate, because the notations 
%``$\subseteq,\subset$'' here  are strongly analogous
% to the notation $\le, <$ from arithmetic. 
%One has to take care to know how notation is being used
%in a given context. (A few authors
%even use $\subseteq, \subsetneq$!) }
%Of course it is possible to have two sets, $A$ and $B$, where
%neither is a subset of the other.  Then $A$ and $B$ may share
%some elements, or no elements.  In fact, for any given sets
%$A$ and $B$, exactly one of the following will be true:
%
%\begin{description}
%\item[case 1:]  $A=B$;
%\item[case 2:] $A\subset B$, i.e.,  $A$ is a proper subset of $B$;
%\item[case 3:] $B\subset A$, i.e., $B$ is a proper subset of $A$;
%\item[case 4:]  $A$ and $B$ share common elements, but neither is 
%        a subset of the other;
%\item[case 5:] $A$ and $B$ have no common elements. In such a case the
%two sets are said to be {\it disjoint}.  
%\end{description}
%The Venn Diagrams in Figure \ref{Venncases} illustrate these
%five cases visually.
%\begin{figure}
%\begin{center}
%\begin{pspicture}(0,.5)(10,6.5)
%\psline(2.5,4)(2.5,6)(4.5,6)(4.5,4)(2.5,4)
%\pscircle[linewidth=.3mm](3.5,5){.8}
%\rput(3.5,4.5){$A,B$}
%\rput(3.5,3.8){Case 1}
%
%
%\psline(5.5,4)(5.5,6)(7.5,6)(7.5,4)(5.5,4)
%\pscircle[linewidth=.3mm](6.5,5){.8}
%\rput(6.5,4.5){$B$}
%\pscircle[linewidth=.3mm](6.3,5.2){.4}
%\rput(6.3,5.2){$A$}
%\rput(6.5,3.8){Case 2}
%
%\psline(1,1)(1,3)(3,3)(3,1)(1,1)
%\pscircle[linewidth=.3mm](2,2){.8}
%\rput(2,1.5){$A$}
%\pscircle[linewidth=.3mm](1.8,2.2){.4}
%\rput(1.8,2.2){$B$}
%\rput(2,.8){Case 3}
%
%\psline(4,1)(4,3)(6,3)(6,1)(4,1)
%\pscircle[linewidth=.3mm](4.7,2){.6}
%\rput(4.45,1.8){$A$}
%\pscircle[linewidth=.3mm](5.3,2){.6}
%\rput(5.55,1.8){$B$}
%\rput(5,.8){Case 4}
%
%\psline(7,1)(7,3)(9,3)(9,1)(7,1)
%\pscircle[linewidth=.3mm](7.53,2){.43}
%\rput(7.5,1.8){$A$}
%\pscircle[linewidth=.3mm](8.48,2){.43}
%\rput(8.5,1.8){$B$}
%\rput(8,.8){Case 5}
%
%\end{pspicture}
%\end{center}
%\caption{Venn Diagrams for the five possible cases for
%arbitrary sets $A$ and $B$.}\label{Venncases}\end{figure}
%
%Actually, the illustration used for Case 4 is the most general, that is,
%can be used for all five cases.  




Of course, Venn Diagrams can accommodate more than two sets.
For example, we  can illustrate the  chain of  set inclusions 
\begin{equation}
\mathbb{N}\subseteq\mathbb{Z}\subseteq\mathbb{Q}\subseteq\mathbb{R}
\label{NZQR}\end{equation}
using a Venn Diagram, as in Figure~\ref{chain1}.
Note that this is a compact way of writing six different
set inclusions:
$\mathbb{N}\subseteq\mathbb{Z}$, $\mathbb{N}\subseteq\mathbb{Q}$, 
$\mathbb{N}\subseteq\mathbb{R}$, $\mathbb{Z}\subseteq\mathbb{Q}$,
$\mathbb{Z}\subseteq\mathbb{R}$, and $\mathbb{Q}\subseteq\mathbb{R}$.


\begin{figure}
\begin{center}
\begin{pspicture}(0,0)(10,4) 
\psline(3,0)(3,4)(7,4)(7,0)(3,0) 
\pscircle[linewidth=.3mm](5,2){.5} 
\pscircle[linewidth=.3mm](5,2){1.}
\pscircle[linewidth=.3mm](5,2){1.5}
\rput(5,2){$\mathbb{N}$}
\rput(5.5,1.5){$\mathbb{Z}$} 
\rput(4.1,1.2){$\mathbb{Q}$}
\rput(3.5,3.5){$\mathbb{R}$}
\end{pspicture}
\end{center}
\caption{Venn Diagram illustrating  
$\mathbb{N}\subseteq\mathbb{Z}\subseteq\mathbb{Q}\subseteq\mathbb{R}$. 
}\label{chain1}\end{figure}


\subsection{Intervals and Inequalities in $\Re$}

The number line, which we will henceforth dub the {\it real line},
has an inherent order in which the numbers are arranged.
Suppose we have two numbers $a,b\in\Re$.  Then the order
relation between $a$ and $b$ has
three possibilities, each with its own notation:
\begin{enumerate}
\item $a$ is to the left of $b$, written $a<b$ and
      spoken ``$a$ is {\it less than} $b$.''
\item $a$ is to the right of $b$, written $a>b$ and
      spoken ``$a$ is {\it greater than} $b$.''
\item $a$ is at the same location as $b$, written $a=b$
      and spoken ``$a$ equals $b$.''
\end{enumerate}
Figure~\ref{TrichotomyOfReals} shows these three possibilities.
Note that ``less than'' and ``greater than'' refer to relative
positions on the real line, not how ``large'' or ``small'' the 
numbers are.  For instance, $4<5$ but $-5<-4$, though it is
natural to consider $-5$ to be a ``larger'' number than
$-4$.  Similarly $-1000<1$.\footnote{%%
%%%% FOOTNOTE
Later we will refer to a number's {\it absolute size}, in 
which context we will describe $-1000$ as ``larger'' than 1.%
%%%% END FOOTNOTE
}
Of course if $a<b\iff b>a$.  We have further notation which
describes when $a$ is left of or at $b$, and when $a$ is
right of or at $b$:
\begin{enumerate}\setcounter{enumi}{3}
\item $a$ is at or left of $b$, written $a\le b$ and spoken
      ``$a$ is less than or equal to $b$.''
\item $a$ is at or right of $b$, written $a\ge b$ and spoken
      ``$a$ is greater than or equal to $b$.''
\end{enumerate}

\begin{figure}
\begin{center}
\begin{pspicture}(-5,-1)(5,2)
%\psline[linewidth=1.5pt]{<->}(-5,0)(5,0)
\psline{<->}(-5,0)(5,0)
\pscircle*(0,0){.08}
%\psline(0,-.15)(0,.15)
\rput(0,-.3){$b$}
\rput(4.8,.25){$\Re$}
\rput(-2.5,1){$\boxed{a<b}$}
  \psline{->}(-2.5,.76)(-2.5,0)
  \psline{->}(-2.5,.76)(-4.5,0)
  \psline{->}(-2.5,.76)(-1.,0)
\rput(0,1){$\boxed{a=\vphantom{>}b}$}
  \psline{->}(0,.7)(0,0)
\rput(2.5,1){$\boxed{a>b}$}
  \psline{->}(2.5,.76)(2.5,0)
  \psline{->}(2.5,.76)(4.5,0)
  \psline{->}(2.5,.76)(1,0)


\end{pspicture}
\end{center}
\caption{For any two real numbers $a$ and $b$, we have 
the three cases concerning their relative positions on
the real line: $a<b$, $a=b$, $a>b$.  Arrows indicate
the possible positions of $a$ for the three cases.}
\label{TrichotomyOfReals}
\end{figure}

Using inequalities, we can describe {\it intervals} in $\Re$,
which are exactly the {\it connected} subsets of $\Re$, meaning
those sets which can be represented by darkening the 
real line at only those points which are in the subset, and
where doing so can be theoretically accomplished without
lifting our pencils as we darken.  In other words, these
are ``unbroken'' subsets of $\Re$.
Later we will see that intervals are subsets of particular interest
in calculus.


Intervals can be classified as finite or infinite (referring to
their lengths), and open, closed or half-open (referring to 
their ``endpoints'').  The finite intervals are of
three types: closed, open and half-open.  Intervals of 
these types, with real {\it endpoints} $a$ and $b$,
where $a<b$ (though the idea extends to work with $a\le b$)
are shown below respectively by graphical 
illustration, in {\it interval notation},
and using earlier set-theoretic notation:

\begin{tabular}{llll}
open:&\begin{pspicture}(0,0)(4,1)
    \psline[linewidth=.1cm](1.2,0.1)(2.8,0.1)
      \pscircle[fillstyle=solid,fillcolor=white](1.2,.1){.12}
      \pscircle[fillstyle=solid,fillcolor=white](2.8,.1){.12}
      \psline{<->}(0,0.1)(4,0.1)
      \psline(1.2,-.05)(1.2,.25)
        \rput(1.2,-.2){$a$}
      \psline(2.8,-.05)(2.8,.25)
        \rput(2.8,-.2){$b$}
     \end{pspicture}
      &\qquad
      $(a,b)$
      &\qquad
      $\left\{\left.x\in\Re\,\vphantom{\frac22}\right|\,a< x< b\right\}$\\
closed:&\begin{pspicture}(0,0)(4,1)
    \psline[linewidth=.1cm](1.2,0.1)(2.8,0.1)
      \pscircle[fillstyle=solid,fillcolor=black](1.2,.1){.12}
      \pscircle[fillstyle=solid,fillcolor=black](2.8,.1){.12}
      \psline{<->}(0,0.1)(4,0.1)
      \psline(1.2,-.05)(1.2,.25)
        \rput(1.2,-.2){$a$}
      \psline(2.8,-.05)(2.8,.25)
        \rput(2.8,-.2){$b$}
     \end{pspicture}
      &\qquad
      $[a,b]$
      &\qquad
      $\left\{\left.x\in\Re\,\vphantom{\frac22}\right|\,a\le x\le b\right\}$\\
half-open:&\begin{pspicture}(0,0)(4,1)
    \psline[linewidth=.1cm](1.2,0.1)(2.8,0.1)
      \pscircle[fillstyle=solid,fillcolor=black](1.2,.1){.12}
      \pscircle[fillstyle=solid,fillcolor=white](2.8,.1){.12}
      \psline{<->}(0,0.1)(4,0.1)
      \psline(1.2,-.05)(1.2,.25)
        \rput(1.2,-.2){$a$}
      \psline(2.8,-.05)(2.8,.25)
        \rput(2.8,-.2){$b$}
     \end{pspicture}
      &\qquad
      $[a,b)$
      &\qquad
      $\left\{\left.x\in\Re\,\vphantom{\frac22}\right|\,a\le x< b\right\}$\\
half-open:&\begin{pspicture}(0,0)(4,1)
    \psline[linewidth=.1cm](1.2,0.1)(2.8,0.1)
      \pscircle[fillstyle=solid,fillcolor=white](1.2,.1){.12}
      \pscircle[fillstyle=solid,fillcolor=black](2.8,.1){.12}
      \psline{<->}(0,0.1)(4,0.1)
      \psline(1.2,-.05)(1.2,.25)
        \rput(1.2,-.2){$a$}
      \psline(2.8,-.05)(2.8,.25)
        \rput(2.8,-.2){$b$}
     \end{pspicture}
      &\qquad
      $(a,b]$
      &\qquad
      $\left\{\left.x\in\Re\,\vphantom{\frac22}\right|\,a< x\le b\right\}$\\
\end{tabular}

\medskip
\noindent Note that $a<x<b$ is short for $(a<x)\wedge(x<b)$, i.e.,
$(x>a)\wedge(x<b)$.  The others are similar.


We will concentrate on the open and closed intervals in 
calculus.  For the finite open interval above, 
we see that we do not include the endpoints $a$ and $b$ in the set, 
denoting
this fact with parentheses in the interval notation and an ``open''
circle at each endpoint on the graph.  What is crucial to calculus is
that immediately surrounding any point $x\in(a,b)$ are only
other points still inside
the interval;  if we pick a point $x$ {\it anywhere} in the interval $(a,b)$,
we see that just left and just right of $x$ are only points in the
interval.  Indeed, we have to travel some distance---albeit possibly
short---to leave the interval from a point $x\in(a,b)$.
 Thus no point inside of $(a,b)$ is on the boundary, and so each point
in $(a,b)$ is ``safely'' on the interior of the interval.
This will be crucial to the concepts of continuity, limits
and (especially) derivatives later in the text.  

For a closed interval
$[a,b]$, we {\it do} include the endpoints $a$ and $b$, which
are not surrounded  by other points in the interval.  For instance,
immediately left of $a$ is outside the interval $[a,b]$, 
though immediately right
of $a$ is on the interior.\footnote{%
%%% FOOTNOTE
For a closed interval $[a,b]$, later we will sometimes refer to the 
{\it interior} of the interval, meaning all points whose
immediate neighbors left and right are also in the interval.
This means that the interior of $[a,b]$ is simply $(a,b)$.
%%% END FOOTNOTE
}  We denote this
fact with brackets in the interval notation, and a ``closed'' circle
at each endpoint when we sketch the graph. 
Half-open (or half-closed) intervals are simple extensions of
these ideas, as illustrated above.

For infinite intervals, we have either one or no endpoints.
If there is an endpoint it is either not included in the interval
or it is, the former giving an open interval and the latter a closed
interval.
An open interval which is infinite in one direction will
be written $(a,\infty)$ or $(-\infty,a)$, depending upon the
direction in which it is infinite.  Here $\infty$ {(infinity)}
means
that we can move along the interval to the right ``forever,''
and $-\infty$ means we can move left without end.  
For infinite closed intervals the notation is similar: 
$[a,\infty)$ and $(-\infty,a]$.
The whole real line is also considered an interval,
which we denote $\Re=(-\infty,\infty)$.\footnote{%%
%%%%%
For technical reasons which will be partially explained later,
$\Re$ is considered to be both an open and a closed interval.
Roughly, it is open because every point is interior, but 
closed because every point that can be approached as close
as we want from the interior is contained in the interval.
Those are the topological factors which characterize open and
closed intervals as such. Topology as a subject is rarely taught
before the junior level of college, or even graduate school,
though advanced calculus usually includes some topology of $\Re$.
%%%%
}  When an interval
continues {\it without bound} in a direction, we also darken
the arrow in that direction.  Thus we have the following:


\begin{tabular}{llll}
open:&\begin{pspicture}(0,0)(4,1)
    \psline[linewidth=.1cm]{->}(2,0.1)(4,0.1)
      \pscircle[fillstyle=solid,fillcolor=white](2,.1){.12}
      \psline{<->}(0,0.1)(4,0.1)
      \psline(2,-.05)(2,.25)
        \rput(2,-.2){$a$}
     \end{pspicture}
      &\qquad
      $(a,\infty)$
      &\qquad
      $\left\{\left.x\in\Re\,\vphantom{\frac22}\right|\,x>a\right\}$\\
open:&\begin{pspicture}(0,0)(4,1)
    \psline[linewidth=.1cm]{->}(2,0.1)(0,0.1)
      \pscircle[fillstyle=solid,fillcolor=white](2,.1){.12}
      \psline{<->}(0,0.1)(4,0.1)
      \psline(2,-.05)(2,.25)
        \rput(2,-.2){$a$}
     \end{pspicture}
      &\qquad
      $(-\infty,a)$
      &\qquad
      $\left\{\left.x\in\Re\,\vphantom{\frac22}\right|\,x<a\right\}$\\
closed:&\begin{pspicture}(0,0)(4,1)
    \psline[linewidth=.1cm]{->}(2,0.1)(4,0.1)
      \pscircle[fillstyle=solid,fillcolor=black](2,.1){.12}
      \psline{<->}(0,0.1)(4,0.1)
      \psline(2,-.05)(2,.25)
        \rput(2,-.2){$a$}
     \end{pspicture}
      &\qquad
      $[a,\infty)$
      &\qquad
      $\left\{\left.x\in\Re\,\vphantom{\frac22}\right|\,x\ge a\right\}$\\
closed:&\begin{pspicture}(0,0)(4,1)
    \psline[linewidth=.1cm]{->}(2,0.1)(0,0.1)
      \pscircle[fillstyle=solid,fillcolor=black](2,.1){.12}
      \psline{<->}(0,0.1)(4,0.1)
      \psline(2,-.05)(2,.25)
        \rput(2,-.2){$a$}
     \end{pspicture}
      &\qquad
      $(-\infty,a]$
      &\qquad
      $\left\{\left.x\in\Re\,\vphantom{\frac22}\right|\,x\le a\right\}$\\
\end{tabular}

\bigskip
\noindent Note that we never use brackets to enclose an infinite
``endpoint,'' since $-\infty,\infty$ are not actual boundaries
but rather are concepts of unending continuance.  Indeed, 
$-\infty,\infty\notin\Re$, i.e., they are not points on the
real line, so they can not be boundaries of subsets
of $\Re$; there are no elements ``beyond'' them.

\subsection{Most General Venn Diagrams}
Before we get to the title of this subsection,
we will introduce a notion which we will have occasional use
for, which is the concept of {\it proper subset}.
\begin{definition}If $(A\subseteq B)\wedge (A\ne B$), we call $A$ a
{\bf proper} subset of $B$, and write $A\subset B$\footnotemark. 
\end{definition}
%%%%  BEGIN FOOTNTOE
\footnotetext{The notation has changed over the years.
Many current texts use ``$\subset$'' the way we use 
``$\subseteq$'' here.  This is unfortunate, because the notations 
``$\subseteq,\subset$'' here  are strongly analogous
 to the notations $\le, <$ from arithmetic. 
One has to take care to know how notation is being used
in a given context. (A few authors
even use $\subseteq, \subsetneq$!) }
%%%%  END FOOTNOTE
Thus $A\subset B$ means $A$ is contained in $B$, but $A$ is not all of $B$.
Note that $A\subset B\implies A\subseteq B$
(just as $P\wedge Q\implies P$).  When we have that $A$ is a
subset of $B$ and are not
interested in emphasizing whether or not $A\ne B$ (or are not sure if
this is true), we will use the ``inclusive'' notation $\subseteq$.
In fact, the inclusive case is less complicated logically
(just as $P\vee Q$ is easier than $P\text{ XOR }Q$)
and so we will usually opt for it even when we do know that
$A\ne B$.  We mention the exclusive case here mainly because
it is useful in explaining the most general Venn Diagram
for two sets $A$ and $B$.


%Consider  
%Figure~\ref{GeneralVennForTwoSets}, which has four distinct, disjoint 
%areas, labeled I, II, III and IV.  Also recall that the Venn
%Diagram only requires 
%all elements of $A$ to be within the borders of the
%figure for $A$,
%and all the elements of $B$ to be within the borders of our figure 
%for $B$.  Figure~\ref{GeneralVennForTwoSets}  excludes none of the 
%possibilities.  
%Case~1 is exactly the case where all elements of $A$ and $B$ are
%in Area~IV; Case~2 is when all elements of $A$ are in Area~IV,
%while some other elements of $B$ are in Area~III; in Case~3, 
%all elements of $B$ are in Area~IV, while some other elements
%of $A$ are in Area~II; in Case~4, Areas~II, III and IV all have
%elements; and in Case~5, Area~IV is empty.  Without further
%information, we therefore draw any two sets $A$ and $B$ as in
%Figure~\ref{GeneralVennForTwoSets}. 
%
%\begin{figure}
%\begin{center}
%\begin{pspicture}(0,0.25)(4,3.25)
%\psline(0,.25)(0,3.25)(4,3.25)(4,.25)(0,.25) 
%\pscircle[linewidth=.3mm](1.4,1.75){1.0}
%\pscircle[linewidth=.3mm](2.6,1.75){1.0}
%\rput(0.3,2.7){I}
%\rput(1.3,1.5){II}
%\rput(2.7,1.5){III}
%\rput(2,1.75){IV}
%\rput(1.5,2.5){$A$}
%\rput(2.5,2.5){$B$}
%\rput(.25,.5){$U$}
%\end{pspicture}
%\end{center}
%\caption{Most general Venn Diagram for two arbitrary sets $A$ and $B$.
%Here $U$ is some superset of both $A$ and $B$.}
%\label{GeneralVennForTwoSets} \end{figure}

Of course it is possible to have two sets, $A$ and $B$, where
neither is a subset of the other.  Then $A$ and $B$ may share
some elements, or no elements.  In fact, for any given sets
$A$ and $B$, exactly one of the following will be true:

\begin{description}
\item[case 1:]  $A=B$;
\item[case 2:] $A\subset B$, i.e.,  $A$ is a proper subset of $B$;
\item[case 3:] $B\subset A$, i.e., $B$ is a proper subset of $A$;
\item[case 4:]  $A$ and $B$ share common elements, but neither is 
        a subset of the other;
\item[case 5:] $A$ and $B$ have no common elements. In such a case the
two sets are said to be {\it disjoint}.  
\end{description}
Even if we do not know which of the five cases is correct, we can
use a single illustration which covers all of these.  That
illustration is given in Figure~\ref{GeneralVennForTwoSets},
with the various regions labeled. (We will explain the meaning
of $U$ in the next subsection.)
To see that this covers all cases, we take them in turn:
\begin{description}
\item[case 1:]  $A=B$:  all elements of $A$ and $B$ are in Region IV;
            there are no elements in Regions II and III.
\item[case 2:] $A\subset B$: there are elements in Regions III and IV,
            and no elements in Region II.
\item[case 3:] $B\subset A$: there are elements in Regions II and IV,
            and no elements in Region III.
\item[case 4:]  $A$ and $B$ share common elements, but neither is 
        a subset of the other:  there are elements in Region II, III and IV. 
\item[case 5:] $A$ and $B$ have no common elements:  there are no elements
        in Region IV.
\end{description}
\begin{figure}
\begin{center}
\begin{pspicture}(0,0.25)(4,3.25)
\psline(0,.25)(0,3.25)(4,3.25)(4,.25)(0,.25) 
\pscircle[linewidth=.3mm](1.4,1.75){1.0}
\pscircle[linewidth=.3mm](2.6,1.75){1.0}
\rput(0.3,2.7){I}
\rput(1.3,1.5){II}
\rput(2.7,1.5){III}
\rput(2,1.75){IV}
\rput(1.5,2.5){$A$}
\rput(2.5,2.5){$B$}
\rput(.25,.5){$U$}
\end{pspicture}
\end{center}
\caption{Most general Venn diagram for two arbitrary sets $A$ and $B$.
Here $U$ is some superset of both $A$ and $B$.}
\label{GeneralVennForTwoSets} \end{figure}
Note that whether or not Region I has elements is irrelevant in the
discussion above, though it will become important shortly.

The most general Venn diagram for three sets is given in 
Figure~\ref{GeneralVennForThreeSets}, though we will not
exhaustively show this to be the most general.
It is not important that the sets are represented by circles,
but only that there are sufficiently many separate regions and
that every case of an element being, or not being, in
$A$, $B$ and $C$ is represented.  Note that there are
three sets for an element to be or not to be a member of,
and so there are $2^3=8$ subregions needed.
 

\begin{figure}
\begin{center}
\begin{pspicture}(0,0)(10,4)
\psline(3,0)(3,4)(7,4)(7,0)(3,0)
\pscircle[linewidth=.3mm](4.4,2.5){1}
\pscircle[linewidth=.3mm](5.6,2.5){1}
\pscircle[linewidth=.3mm](5,1.4){1}
\rput(4,3){$A$}
\rput(6,3){$B$}
\rput(5,.7){$C$}
\rput(3.25,.25){$U$}
\end{pspicture}
\end{center}
\caption{The most general Venn Diagram for three sets
$A$, $B$ and $C$.}\label{GeneralVennForThreeSets}\end{figure}


\subsection{Set Operations}
When we are given two sets $A$ and $B$, it is natural to 
combine or compare their memberships with each other and the universe
of all elements of interest. 
In particular, we form new sets called the union and intersection of
$A$ and $B$, the difference of $A$ and $B$ (and of $B$ and $A$), 
and the complement
of $A$ (and  of $B$). 
The first three are straightforward, but the
fourth requires some clarification.  Usually $A$ and
$B$ contain only objects of a certain class like
numbers, colors, etc.  Thus we take elements of $A$ and
$B$ from a specific {\it universal set} $U$ of objects rather
than an all-encompassing universe of all objects.  It is  
unlikely in mathematics that we
would need, for instance, to mix numbers with persons and
planets and verbs, so we find it convenient to 
limit our universe $U$ of considered objects.
With that in mind (but without presently defining $U$),  
the notations for these new sets are as follow:
\begin{definition}
\begin{align}
A\cup B&=\left\{x\ \left.\vphantom{x_0^9}\right|\  
(x\in A)\vee(x\in B)\right\}\label{A_union_B}\\
A\cap B&=\left\{x\ \left.\vphantom{x_0^9}\right|\
(x\in A)\wedge(x\in B)\right\}\\
A-B&=\left\{x\ \left.\vphantom{x_0^9}\right|\
(x\in A)\wedge(x\not\in B)\right\}\\
A'&=\left\{x\in U \left.\vphantom{x_0^9}\right|\
(x\not\in A)\right\}.\label{U_-_A} 
\end{align}
\end{definition} 
These are read ``$A$ union $B$,''  ``$A$ intersect $B$,'' 
``$A$ minus $B$," and ``$A$ complement,'' respectively.  
Note that in the first three, we could have also written
$\left\{\left.x\in U\vphantom{\frac11}\right|\cdots\right\}$, 
but since $A,B\subseteq U$, there it is unnecessary.
Also note that one could define the complement in the following way,
though (\ref{U_-_A}) is more convenient for symbolic logic
computations:
\begin{equation}A'=\left\{x\ \left.\vphantom{x_0^9}\right|\
(x\in U)\wedge(x\not\in A)\right\}=U-A.\label{U_-_A--2}\end{equation}
These operations are illustrated
by the Venn diagrams of 
Figure~\ref{VDunion&intersection&difference}, where we
also construct $B'$ and  $B-A$.  Note the connection between
the logical $\vee$ and $\wedge$, and the set-theoretical
$\cup$ and $\cap$.\footnote{%%%
%%% FOOTNOTE
The set-theoretical ``$-$'' could be interpreted as ``$\wedge\sim\cdots\in$,''
and if we always assume we know what is the universal set, we
can interpret the complement symbol ``$\,'\,$'' as ``$\sim\cdots\in$.''}
%%% END FOOTNOTE




\begin{figure}

\begin{center}
\begin{pspicture}(0.5,-3.5)(11.5,3.0)
\psline(.5,.25)(.5,2.5)(3.5,2.5)(3.5,.25)(.5,.25)
\pscircle[linewidth=.3mm, fillstyle=solid,fillcolor=gray](1.5,1.375){.75}
\pscircle[linewidth=.3mm, fillstyle=solid,fillcolor=gray](2.5,1.375){.75}
\pscircle[linewidth=.3mm](1.5,1.375){.75}
\rput(1.2,1.675){$A$}
\rput(2.8,1.675){$B$}
\rput(2,0){$A\cup B$}
\rput(.75,.5){$U$}
\psarc[linecolor=white,fillstyle=solid,fillcolor=gray](5.5,1.375){.75}{-50}{50}
\psarc[linecolor=white,fillstyle=solid,fillcolor=gray]%
(6.5,1.375){.75}{130}{230}

%\pscustom[linestyle=none,fillstyle=solid, fillcolor=gray]{%
%\pscurve(6,.8)(5.75,1.35)(6,1.92)%
%\pscurve[liftpen=2](6.00,.8)(6.25,1.35)(6,1.92)%(6,1.92)(6.22,1.31)(6.00,.8)}
%}
\psline(4.5,.25)(4.5,2.5)(7.5,2.5)(7.5,.25)(4.5,.25)
\pscircle[linewidth=.3mm](5.5,1.375){.75}
\pscircle[linewidth=.3mm](6.5,1.375){.75}
\rput(5.2,1.675){$A$}
\rput(6.8,1.675){$B$}
\rput(6,0){$A\cap B$}
\rput(4.75,.5){$U$}


\psline(8.5,.25)(8.5,2.5)(11.5,2.5)(11.5,.25)(8.5,.25) 
\pscircle[linewidth=.3mm, fillstyle=solid, fillcolor=gray](9.5,1.375){.75}
\pscircle[linewidth=.3mm, fillstyle=solid,fillcolor=white](10.5,1.375){.75}
\pscircle[linewidth=.3mm](9.5,1.375){.75}
\rput(9.2,1.675){$A$}
\rput(10.8,1.675){$B$}
\psline(8.5,.25)(8.5,2.5)(11.5,2.5)(11.5,.25)(8.5,.25)
\rput(10,0){$A-B$}
\rput(8.75,.5){$U$}


\psframe[fillstyle=solid,fillcolor=gray](.5,-3.25)(3.5,-1.0)
\pscircle[linewidth=.3mm, fillstyle=solid,fillcolor=white](1.5,-2.125){.75}
\pscircle[linewidth=.3mm](2.5,-2.125){.75}
\pscircle[linewidth=.3mm](1.5,-2.125){.75}
\rput(1.2,-1.825){$A$}
\rput(2.8,-1.825){$B$}
\rput(2,-3.5){$A'$}
\rput(.75,-3){$U$} 

\psframe[fillstyle=solid,fillcolor=gray](4.5,-3.25)(7.5,-1.0)
\pscircle[linewidth=.3mm, fillstyle=solid,fillcolor=white](6.5,-2.125){.75}
\pscircle[linewidth=.3mm](5.5,-2.125){.75}
\rput(5.2,-1.825){$A$}
\rput(6.8,-1.825){$B$}
\rput(6,-3.5){$B'$}
\rput(4.75,-3){$U$}

\psline(8.5,-3.25)(8.5,-1.0)(11.5,-1.0)(11.5,-3.25)(8.5,-3.25)
\pscircle[linewidth=.3mm, fillstyle=solid, fillcolor=gray](10.5,-2.125){.75}
\pscircle[linewidth=.3mm, fillstyle=solid,fillcolor=white](9.5,-2.125){.75}
\pscircle[linewidth=.3mm](10.5,-2.125){.75}
\rput(9.2,-1.825){$A$}
\rput(10.8,-1.825){$B$}
\psline(8.5,-3.25)(8.5,-1.0)(11.5,-1.0)(11.5,-3.25)(8.5,-3.25)
\rput(10,-3.5){$B-A$}
\rput(8.75,-3){$U$}

\end{pspicture}\end{center}
\caption{Some Venn Diagrams involving two sets $A$ and $B$
inside a universal set $U$, which is represented by the whole ``box.''
}\label{VDunion&intersection&difference}\end{figure} 


\bex
Find $A\cup B$,  $A\cap B$, $A-B$ and $B-A$  if

\begin{align*}
A&=\{1,2,3,4,5,6,7\}\\
B&=\{5,6,7,8,9,10\}.
\end{align*}

\underline{Solution}: Though not necessary (and often impossible),
we will list these set elements in a table from which we can
easily compare the membership.
$$\begin{array}{rrrrrrrrrrrrrrl}
A&=&\{&1,&2,&3,&4,&5,&6,&7,&  &  &  &\}&,\\
B&=&\{&  &  &  &  &5,&6,&7,&8,&9,&10&\}&.\end{array}$$
Now we can compare the memberships using the operations defined earlier.
\begin{align*}
A\cup B&=\{1,2,3,4,5,6,7,8,9,10\},\\
A\cap B&=\{5,6,7\},\\
A-B&=\{1,2,3,4\},\\
B-A&=\{8,9,10\}.\end{align*}
\eex
The complements depend upon the identity of the assumed universal
set.  If in the above example we had $U=\mathbb{N}$, 
then $A'=\{8,9,10,11,\cdots\}$
and $B'=\{1,2,3,4,11,12,13,14,15\cdots\}$.  If instead we took
$U=\mathbb{Z}$ we have $A'=\{\cdots,-3,-2,-1,0,8,9,10,11,\cdots\}$,
for instance.  (We leave $B'$ to the interested reader.)

Just as it is important to have a zero element in $\mathbb{R}$
for arithmetic and other purposes, it is also useful in set
theory to define a set which contains no elements:
\begin{definition}
The set with no elements is called the {\bf empty set},\footnotemark
\footnotetext{It is also called the {\it{null set}}.
Some older texts use empty braces $\emptyset=\{\,\}$.} 
 denoted $\emptyset$.
\end{definition}
One reason we need such a device is for cases of intersections
of disjoint sets.  If $A=\{1,2,3\}$ and $B=\{4,5,6,7,8,9,10\}$,
then $A\cup B=\{1,2,3,\cdots,10\}$, while $A\cap B=\emptyset$.
Notice that regardless of the set $A$, we will always have
$A-A=\emptyset$, $A-\emptyset=A$,
 $A\cup\emptyset=A$, $A\cap\emptyset=\emptyset$,
and $\emptyset\subseteq A$.  The last statement is 
true because, after all, every element of $\emptyset$ is 
also an element of $A$.\footnotemark
%%% FOOTNOTE
\footnotetext{This is precisely because there are no
elements of $\emptyset$; the statement $x\in\emptyset\longrightarrow 
x\in A$ is vacuously true because $x\in\emptyset$ is false,
regardless of $x$.}
%%% END FOOTNOTE
Note also that $\emptyset'=U$ and $U'=\emptyset$.

The set operations for two sets $A$ and $B$ can only give us 
finitely many combinations of the areas enumerated in 
Figure~\ref{GeneralVennForTwoSets}.  In fact, since each
such area is either included or not, there are
$2^4=16$ different diagram shadings possible for 
the general case as in Figure~\ref{GeneralVennForTwoSets}.
The situation is more interesting if we have three 
sets $A$, $B$ and $C$.
Using Figure~\ref{GeneralVennForThreeSets}, we can 
prove several interesting set equalities.
First we have some fairly obvious commutative laws (\ref{union_commutes}),
(\ref{intersection_commutes}) and
associative laws (\ref{union_associates}), (\ref{intersection_associates}):
\begin{align}
A\cup B&=B\cup A \label{union_commutes}\\
A\cap B&=B\cap A\label{intersection_commutes}\\
A\cup(B\cup C)&=(A\cup B)\cup C\label{union_associates}\\
A\cap(B\cap C)&=(A\cap B)\cap C\label{intersection_associates}\end{align} 
Next are the following two {\it distributive laws},
which are the set-theory analogs to the logical equivalences
(\ref{LogicDistrib1}) and (\ref{LogicDistrib2}) (also
(\ref{dm3}) and (\ref{dm4})).

\begin{align}
A\cap(B\cup C)&=(A\cap B)\cup(A\cap C),\label{cap_over_cup}\\
A\cup(B\cap C)&=(A\cup B)\cap(A\cup C).\label{cup_over_cap}
\end{align}
\bex
We will show how to prove (\ref{cap_over_cup}) using our previous
symbolic logic, and then give a visual proof using Venn diagrams.
Similar techniques can be used to prove (\ref{cup_over_cap}).
For the proof that $A\cap(B\cup C)=(A\cap B)\cup(A\cap C)$, we
use definitions, (\ref{LogicDistrib1}) and the fact
that $(x\in A)\iff(x\in A)\wedge(x\in A)$ to get the following:
\begin{align*}
x\in A\cap(B\cup C)
   &\iff (x\in A)\wedge(x\in B\cup C)\\
   &\iff (x\in A)\wedge[(x\in B)\vee(x\in C)]\\
   &\iff [(x\in A)\wedge(x\in B)]\vee[(x\in A)\wedge(x\in C)]\\
   &\iff [x\in (A\cap B)]\vee[x\in (A\cap C)] \\
   &\iff x\in[(A\cap B)\cup(A\cap C)], \text{ q.e.d.}
\end{align*} 
We proved that  $(\forall x)[(x\in A\cap(B\cup C))\longleftrightarrow
                  (x\in(A\cap B)\cup(A\cap C))]$,
which is the definition for the sets in question to be equal.
The visual demonstration of  $A\cap(B\cup C)=(A\cap B)\cup(A\cap C)$
is given in Figure~\ref{3_Sets_Venn_Example_Figure_0}, where
we construct both sets of the equality in stages.

To construct the left-hand side of the equation, 
in the first box  we color $A$,
then $B\cup C$ in the second, and finally 
we take the area from the first, remove the area from
the second, and are left with the difference
$A-(B\cup C)$.  
To construct the right-hand side of the equation, 
we  color $A-B$ and $A-C$ in separate boxes.
Then we color the intersection of these, which
is the area colored in the previous two boxes.
This gives us our Venn Diagram for $(A-B)\cap(A-C)$.
We see that the left- and right-hand sides are the same,
and conclude the equality is valid. 
\label{3-set_Venn_Example0}
\eex

\begin{figure}
\begin{center}

\begin{pspicture}(0,-.5)(10.5,7)
\psframe(0,4)(3,7)
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](1.05,5.95){.75}
\pscircle[linewidth=.3mm](1.95,5.95){.75} 
\pscircle[linewidth=.3mm](1.5,5.05){.75}
\rput(1.5,3.75){$A$}
\rput(.6,6.3){$A$}
\rput(2.3,6.3){$B$}
\rput(1.5,4.6){$C$}

\psframe(3.75,4)(6.75,7)
\pscircle[linewidth=.3mm](4.80,5.95){.75}
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](5.7,5.95){.75}
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](5.25,5.05){.75}
\pscircle[linewidth=.3mm](4.80,5.95){.75}
\pscircle[linewidth=.3mm](5.7,5.95){.75}
\rput(5.25,3.75){$B\cup C$}
\rput(4.35,6.3){$A$}
\rput(6.05,6.3){$B$}
\rput(5.25,4.6){$C$}

\psframe(7.5,4)(10.5,7) 
\psarc[linewidth=0pt,%
linecolor=gray,fillstyle=solid,fillcolor=gray](8.55,5.95){.75}{-54}{54}
\psarc[linewidth=0pt,linecolor=gray,fillstyle=solid,fillcolor=gray]%
(9.45,5.95){.75}{126}{234}
\psarc[linewidth=0pt,linecolor=gray,fillstyle=solid,fillcolor=gray]%
(9,5.05){.75}{68}{165}
\psarc[linewidth=0pt,linecolor=gray,fillstyle=solid,fillcolor=gray]%
(8.55,5.95){.75}{248}{345}

\pscircle[linewidth=.3mm](8.55,5.95){.75}
\pscircle[linewidth=.3mm](9.45,5.95){.75}
\pscircle[linewidth=.3mm](9,5.05){.75}
%\pscircle[linewidth=.3mm](8.55,5.95){.75}
%\pscircle[linewidth=.3mm](9.45,5.95){.75}
%\pscircle[linewidth=.3mm](9,5.05){.75}
\rput(9,3.75){$A\cap(B\cup C)$}
\rput(8.1,6.3){$A$}
\rput(9.8,6.3){$B$}
\rput(9,4.6){$C$}


\psframe(0,0)(3,3)
\psarc[linewidth=0pt,%
linecolor=gray,fillstyle=solid,fillcolor=gray](1.05,1.95){.75}{-54}{54}
\psarc[linewidth=0pt,linecolor=gray,fillstyle=solid,fillcolor=gray]%
(1.95,1.95){.75}{126}{234}
%\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](1.05,1.95){.75}
%\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=white](1.95,1.95){.75}
\pscircle[linewidth=.3mm](1.5,1.05){.75}
\pscircle[linewidth=.3mm](1.05,1.95){.75}
\pscircle[linewidth=.3mm](1.95,1.95){.75}
\rput(1.5,-.25){$A\cap B$}
\rput(.6,2.3){$A$}
\rput(2.3,2.3){$B$}
\rput(1.5,.6){$C$}


\psframe(3.75,0)(6.75,3)
\psarc[linewidth=0pt,linecolor=gray,fillstyle=solid,fillcolor=gray]%
(5.25,1.05){.75}{68}{165}
\psarc[linewidth=0pt,linecolor=gray,fillstyle=solid,fillcolor=gray]%
(4.8,1.95){.75}{248}{345}

%\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](4.80,1.95){.75}
\pscircle[linewidth=.3mm](5.25,1.05){.75}
\pscircle[linewidth=.3mm](4.80,1.95){.75}
\pscircle[linewidth=.3mm](5.7,1.95){.75}
\rput(5.25,-.25){$A\cap C$}
\rput(4.35,2.3){$A$}
\rput(6.05,2.3){$B$}
\rput(5.25,.6){$C$}


\psframe(7.5,0)(10.5,3)
\psarc[linewidth=0pt,%
linecolor=gray,fillstyle=solid,fillcolor=gray](8.55,1.95){.75}{-54}{54}
\psarc[linewidth=0pt,linecolor=gray,fillstyle=solid,fillcolor=gray]%
(9.45,1.95){.75}{126}{234}
\psarc[linewidth=0pt,linecolor=gray,fillstyle=solid,fillcolor=gray]%
(9,1.05){.75}{68}{165}
\psarc[linewidth=0pt,linecolor=gray,fillstyle=solid,fillcolor=gray]%
(8.55,1.95){.75}{248}{345}
%\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](8.55,1.95){.75}
%\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=white](9.45,1.95){.75}
%\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=white](9,1.05){.75}
\pscircle[linewidth=.3mm](8.55,1.95){.75}
\pscircle[linewidth=.3mm](9.45,1.95){.75}
\pscircle[linewidth=.3mm](9,1.05){.75}
\rput(9,-.25){$(A\cap B)\cup(A\cap C)$}
\rput(8.1,2.3){$A$}
\rput(9.8,2.3){$B$}
\rput(9,.6){$C$}


\end{pspicture}
\end{center}
\caption{Venn Diagrams for Example~\ref{3-set_Venn_Example0}
verifying one of the distributive laws, specifically    
$A\cap(B\cup C)=(A\cap B)\cup(A\cap C)$.  It is especially important
to note how one constructs the third box in each line from the first two.}
\label{3_Sets_Venn_Example_Figure_0}\end{figure} 

The next two are distributive in nature also:
\begin{align}
A-(B\cup C)&=(A-B)\cap(A-C)\label{de_Morgan's_sets_1}\\
A-(B\cap C)&=(A-B)\cup(A-C).\label{de_Morgan's_sets_2} 
\end{align}
Finally,  if we replace $A$ with $U$, we get the set-theoretic
version of {\it de Morgan's Laws:}
\begin{align}
(B\cup C)'&=B'\cap C'\label{complement_of_union}\\
(B\cap C)'&=B'\cup C'\label{complement_of_intersection}.
\end{align}
Note that these are very much like our earlier de Morgan's laws,
and indeed use the previous versions (\ref{dm1}) and (\ref{dm2})
in their proofs.  For instance, assuming $x\in U$ where $U$ is fixed,
we have 
\begin{align*}x\in(B\cup C)'&\iff \sim(x\in B\cup C)\\
&\iff\sim((x\in B)\vee(x\in C))\\
&\iff[\sim(x\in B)]\wedge[\sim(x\in C)]\\
&\iff[x\in B']\wedge[x\in C']\\
&\iff x\in B'\cap C',\text{ q.e.d.}\end{align*}
That proves (\ref{complement_of_union}), and (\ref{complement_of_intersection})
has a similar proof.  It is interesting to prove these using Venn Diagrams
as well (see exercises).
\bex
Another example of how to prove these using logic and
Venn diagrams is in order.  We will prove (\ref{de_Morgan's_sets_1})
using both methods.  First, with symbolic logic:
\begin{align*}
x\in A-(B\cup C)&\iff(x\in A)\wedge[\sim(x\in B\cup C)]\\
 &\iff(x\in A)\wedge[\sim((x\in B)\vee(x\in C))]\\
 &\iff(x\in A)\wedge[(\sim(x\in B))\wedge(\sim(x\in C))]\\
 &\iff(x\in A)\wedge(\sim (x\in B))\wedge(\sim(x\in C))\\
 &\iff(x\in A)\wedge(\sim (x\in B))\wedge(x\in A)\wedge(\sim(x\in C))\\
 &\iff[(x\in A)\wedge(\sim (x\in B)]\wedge[(x\in A)\wedge(\sim(x\in C))]\\
 &\iff(x\in A-B)\wedge(x\in A-C)\\
 &\iff x\in (A-B)\cap(A-C),\text{ q.e.d.}
\end{align*}
If we took the steps above in turn, we used the definition
of set subtraction, the definition of union, (\ref{SimPVeeQ}),
associative property of $\wedge$, added a redundant $(x\in A)$,
regrouped, used the definition of set subtraction, and finally
the definition of intersection.

Now we will see how we can use Venn diagrams to prove 
(\ref{de_Morgan's_sets_1}).
As before, we will do this by constructing Venn Diagrams for the
sets $A-(B\cup C)$ and $(A-B)\cap(A-C)$ separately,
and verify that we get the same sets.  We do this in 
Figure~\ref{3_Sets_Venn_Example_Figure}.
\label{3-set_Venn_Example}\eex
\begin{figure}
\begin{center}

\begin{pspicture}(0,-.5)(10.5,7)
\psframe(0,4)(3,7)
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](1.05,5.95){.75}
\pscircle[linewidth=.3mm](1.95,5.95){.75} 
\pscircle[linewidth=.3mm](1.5,5.05){.75}
\rput(1.5,3.75){$A$}
\rput(.6,6.3){$A$}
\rput(2.3,6.3){$B$}
\rput(1.5,4.6){$C$}

\psframe(3.75,4)(6.75,7)
\pscircle[linewidth=.3mm](4.80,5.95){.75}
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](5.7,5.95){.75}
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](5.25,5.05){.75}
\pscircle[linewidth=.3mm](4.80,5.95){.75}
\pscircle[linewidth=.3mm](5.7,5.95){.75}
\rput(5.25,3.75){$B\cup C$}
\rput(4.35,6.3){$A$}
\rput(6.05,6.3){$B$}
\rput(5.25,4.6){$C$}

\psframe(7.5,4)(10.5,7) 
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](8.55,5.95){.75}
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=white](9.45,5.95){.75}
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=white](9,5.05){.75}
\pscircle[linewidth=.3mm](8.55,5.95){.75}
\pscircle[linewidth=.3mm](9.45,5.95){.75}
\pscircle[linewidth=.3mm](9,5.05){.75}
\rput(9,3.75){$A-(B\cup C)$}
\rput(8.1,6.3){$A$}
\rput(9.8,6.3){$B$}
\rput(9,4.6){$C$}


\psframe(0,0)(3,3)
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](1.05,1.95){.75}
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=white](1.95,1.95){.75}
\pscircle[linewidth=.3mm](1.5,1.05){.75}
\pscircle[linewidth=.3mm](1.05,1.95){.75}
\pscircle[linewidth=.3mm](1.95,1.95){.75}
\rput(1.5,-.25){$A-B$}
\rput(.6,2.3){$A$}
\rput(2.3,2.3){$B$}
\rput(1.5,.6){$C$}


\psframe(3.75,0)(6.75,3)
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](4.80,1.95){.75}
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=white](5.25,1.05){.75}
\pscircle[linewidth=.3mm](4.80,1.95){.75}
\pscircle[linewidth=.3mm](5.7,1.95){.75}
\rput(5.25,-.25){$A-C$}
\rput(4.35,2.3){$A$}
\rput(6.05,2.3){$B$}
\rput(5.25,.6){$C$}


\psframe(7.5,0)(10.5,3)
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=gray](8.55,1.95){.75}
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=white](9.45,1.95){.75}
\pscircle[linewidth=.3mm,fillstyle=solid,fillcolor=white](9,1.05){.75}
\pscircle[linewidth=.3mm](8.55,1.95){.75}
\pscircle[linewidth=.3mm](9.45,1.95){.75}
\pscircle[linewidth=.3mm](9,1.05){.75}
\rput(9,-.25){$(A-B)\cap(A-C)$}
\rput(8.1,2.3){$A$}
\rput(9.8,2.3){$B$}
\rput(9,.6){$C$}


\end{pspicture}
\end{center}
\caption{Venn Diagrams for Example~\ref{3-set_Venn_Example}
verifying that    
$A-(B\cup C)=(A-B)\cap(A-C)$.}
\label{3_Sets_Venn_Example_Figure}\end{figure} 

\newpage
\begin{center}
\underline{\Large{\bf Exercises}}\end{center}
\bigskip
\begin{multicols}{2}
\begin{enumerate}
\item  Draw all the $2^4=16$ possible shadings for 
Figure~\ref{GeneralVennForTwoSets}, page~\pageref{GeneralVennForTwoSets}. 
Then use 
the sets $A$, $B$ and $U$, together with
unions, intersections, complements and set
differences ($\cup,\cap,\,',-$) to write a corresponding expression
for the each of the
shaded areas. Note that Figure~\ref{VDunion&intersection&difference}
illustrates six of them.  Also note that there may be more than
one way of representing a set.  For example, 
$A'=U-A$.

\item Use symbolic logic {\bf and} Venn Diagrams
(as in Examples \ref{3-set_Venn_Example0} and \ref{3-set_Venn_Example})
to prove the other set equalities:
\begin{enumerate}
\item (\ref{union_associates}): $A\cup(B\cup C)=(A\cup B)\cup C$ 
\item (\ref{intersection_associates}): $A\cap(B\cap C)=(A\cap B)$ 
\item (\ref{de_Morgan's_sets_2}): $A-(B\cap C)=(A-B)\cup(A-C)$ 
\item (\ref{cap_over_cup}): $A\cap(B\cup C)=(A\cap B)\cup(A\cap C)$
\item (\ref{complement_of_union}): $(B\cup C)'=B'\cap C'$ 
\item (\ref{complement_of_intersection}): $(B\cap C)'=B'\cup C'$
\end{enumerate}

\item Use Venn Diagrams to draw and 
determine a simpler way of writing the following sets:
\begin{enumerate}
\item $A-(B-A)=$
\item $A-(A-B)=$
\item $(A-B)\cap(B-A)=$
\end{enumerate}

\item Answer each of the following.
\begin{enumerate} 
\item If $A\subseteq B$, what is $A-B$?
\item If $A\subset B$, what can you say about $B-A$?
\item Referring to Figure~\ref{VDunion&intersection&difference},
what are $U'$ and $\emptyset'$?
\item Suppose $A\subseteq B$.  How are $A'$ and $B'$ related?
\item Suppose $A-B=A$.  What is $A\cap B$?
\item Suppose $A-B=B$.  What is $B$?  What is $A$? 
\item Is it possible that $A\subseteq B$ and $B\subseteq A$?
\end{enumerate}


\item The set of  {\it irrational numbers} is the set
$$\mathbb{I}=\left\{\left.\vphantom{X_9^9}x\in\Re\ \right| 
\    x\text{ is not rational}\right\}.$$ 
Using previously-defined sets and set notation, find a concise
definition of  $\mathbb{I}$.

\item Many books define the {\it symmetric difference}
between two sets $A$ and $B$ by
\begin{equation}
A\vartriangle B=(A-B)\cup(B-A).\end{equation}
\begin{enumerate}
\item Use a Venn Diagram to show that 
$A\vartriangle B=(A\cup B)-(A\cap B)$.
\item Is it true that $A\vartriangle B=B\vartriangle A$?
\item Use a Venn Diagram to show 
that $A\vartriangle(B\vartriangle C)
=(A\vartriangle B)\vartriangle C$.
\item Calculate $A\vartriangle A$,
$A\vartriangle U$, and $A\vartriangle\emptyset$.
\item If $A\subseteq B$, what is $A\vartriangle B$?
      What is $B\vartriangle A$?
\item Define $A\vartriangle B$ in a manner similar to 
the definitions (\ref{A_union_B})--(\ref{U_-_A}). That is,
replace the dots with a description of $x$ in the following:
$$A\vartriangle B=\left\{\left.\vphantom{X_0^9} x\
\right|\ \cdots\right\}.$$ 
\end{enumerate}
\item Redraw the Venn Diagram of Figure~\ref{GeneralVennForThreeSets} 
and label each of the eight disjoint areas I--VIII.
Then use the sets $U$, $A$, $B$ and $C$, together with the 
operations $\cup$, $\cap$, $-$ and $'$, to find a definition
of each of these sets I--VIII.

\item How many different shading combinations are there for 
the general Venn Diagram for 3 sets $A$, $B$ and $C$?
(See Figure~\ref{GeneralVennForThreeSets}.)  Speculate
about how many combinations there are for 4 sets, 5 sets,
and $n$ sets.  Test your hypothesis for $n=0$ and $n=1$.


\item A useful concept in set theory is {\it cardinality}
of a set $S$, which we denote $n(S)$, which can 
be defined to be the number of elements
in a set if the set is finite.
Thus $n(\{1,2,3,8,9,10\})=6$.
\begin{enumerate}
\item Use a Venn Diagram to show that
\begin{equation}n(S\cup T)=n(S)+n(T)-n(S\cap T).
\end{equation}
\item Show that if $n(S\cup T)=n(S)+n(T)$, then
$S\cap T=\varnothing$.
\end{enumerate}
\label{CardinalityHomework}
\end{enumerate}

\end{multicols}
\newpage





\section{Logic Epilogue\label{LogicEpilogue}}

Most mathematical statements begin with hypotheses,
i.e., assumptions,  
and end in conclusions which follow from the assumptions 
through valid logic.  
Thus, any mathematics which is done correctly
essentially states a tautology. 
The valid logic connecting 
assumptions to conclusions constitutes a proof
that the assumptions imply the conclusions.
Once a mathematical statement has been proven 
to be a tautology (i.e., always true), it is called a 
{\it theorem}.\footnotemark
\footnotetext{There are also lemmas and corollaries,
but these are also proved, and are technically
theorems in their own rights.  Often the term 
theorem refers to tautologies the author finds
the most interesting, while lemmas are preliminary
results (often where most of the work is done
in proving the theorems) which lead to theorems, and corollaries are
results which follow quickly from theorems.}\hphantom{. }%

Because a theorem is a tautology, it is logically
equivalent to $\T$.  This is quite useful.  Recall
\begin{equation}
P\iff P\wedge\T.\end{equation}
The content of every theorem resides in $\T$.
%For instance, there is a well-known theorem of Pythagoras:
%\begin{description}
%\item[\bf Pythagorean Theorem:]{\it Suppose $a$ and $b$ are
%the lengths of the  legs of a right triangle, 
%and $c$ the length of its hypotenuse.
%Then $a^2+b^2=c^2$.} 
%\end{description}
%Now suppose we are given a right triangle with legs  
%$a=3$ and $b=4$, and we want to find $c^2$.   
%This is a very simple problem, but let us see how
%the logic comes into play:%%
%
%
%(Right Triangle Legs 3,4)
%$\wedge$ (Right Triangle Legs $a,b$, hypotenuse $c\ \longrightarrow
%           c^2=a^2+b^2)\implies c^2=3^2+4^2$.
%
%
%$$\left(   \right) 
%$$
%

%
%There is a false distinction between theoretical and 
%applied (or theoretical and computational) mathematics.   
%The distinction is false because in both cases, we
%work from assumptions, through logic, to conclusions.

\begin{figure}
\begin{center}
%\begin{pspicture}(0,0)(4,4)
%\psline(.9,3)(.9,4)(3.1,4)(3.1,3)(.9,3) 
%\rput(2,3.5){Assumptions}
%\psline{->}(2,3)(2,1)
%\pstextpath[c]{\psline[linecolor=white](2.2,2.8)(2.2,1.2)}
%{Logic}
%\pstextpath[c]{\psline[linecolor=white](1.8,1.2)(1.8,2.8)}
%{Logic} 
%\pscurve{->}(.9,3)(.5,2)(1,1)
%\pstextpath[c]{\pscurve[linecolor=white](0.8,1)(0.3,2)(0.7,3)}{Logic}
%\pscurve{->}(3.1,3)(3.5,2)(3,1)
%\pstextpath[c]{\pscurve[linecolor=white](3.3,3)(3.7,2)(3.2,1)}
%{Logic}
%\psline(1,0)(1,1)(3,1)(3,0)(1,0)
%\rput(2,.5){Conclusions} 
\begin{pspicture}(0,0)(12,2)
%\rput(2.5,2.5){\underline{To Prove Prove Assumptions $\implies$ Conclusions}:}
\rput(6,1.5){(Assumptions)\qquad$\iff$\qquad[\ (Assumptions)\qquad
              $\wedge$\qquad$\T$\ ]
                 \qquad
                $\implies$ \qquad (Conclusions)}
\rput{90}(8.,.8){$\iff$}
\rput(8.,0){Theorems}
\end{pspicture}
\end{center}
\caption{A model for mathematical reasoning.  To show the conclusions
         follow from the assumptions, one usually attaches some
         theorems which are relevant to the assumptions.  This is valid since
         every statement $P$ is equivalent to $P\wedge\T$, and
         every theorem is equivalent to $\T$.}\end{figure} 





\section{Epilogue: Proofs Without Truth Tables}

Truth tables are the most exhaustive tools for studying
compound statements, because they simply check every case.
This makes them useful tools to fall back upon if necessary,
but as we learn and memorize more equivalences
it is useful to put what we already showed to work in other proofs.
For instance, recall the following equivalences, here written
with our new notation:
\begin{enumerate}
\item $P\longrightarrow Q\iff(\sim P)\vee Q$,
\item $\sim[P\vee Q]\iff(\sim P)\wedge(\sim Q)$,
\item $\sim(\sim P)\iff P$.
\end{enumerate}
We will use these principles, in exactly the  order above, to
produce a very concise proof in the following example.
\bex Prove without truth tables that $\sim(P\longrightarrow Q)
\iff P\wedge(\sim Q)$.

\underline{Solution}:
\begin{align*}
\sim(P\longrightarrow Q)&\iff\sim[(\sim P)\vee Q]\\  
                        &\iff[\sim(\sim P)]\wedge(\sim Q)\\
                        &\iff P\wedge(\sim Q),\qquad\text{q.e.d.}
\end{align*}
\eex



To show a statement is a tautology it is enough to show that
it is logically equivalent to $\T$.  We can use this 
fact to prove valid logical equivalences (i.e., show
$\longleftrightarrow$ gives a tautology) and 
valid logical implications (i.e., show $\longrightarrow$
gives a tautology).  There are often easier ways to 
prove valid equivalences, but for valid implications this 
method can be useful, as in the example below.

\bex We revisit Example \ref{P^Q>Pex}, proving
(\ref{P^Q>Peq}) using  this notation.  That is, we wish
 to show that $(P\wedge Q)\implies P$ by showing
$$(P\wedge Q)\longrightarrow P\iff\T.$$ 

\begin{proof}\begin{align*}
P\wedge Q\longrightarrow P&\iff\sim(P\wedge Q)\vee P\\
&\iff(\sim P)\vee(\sim Q)\vee P\\
&\iff(\sim P)\vee P\vee(\sim Q)\\
&\iff \T\vee(\sim Q)\\
&\iff \T,\ {\rm q.e.d.}\end{align*}
\end{proof}\eex\bigskip

This kind of proof requires much practice, but also can be
very satisfying when completely mastered.  The highly interested student
can work all of the exercises, and perhaps continue by attempting,
without truth tables, 
proofs of the remaining valid equivalences and implications listed in the 
next subsection.  We leave a couple of more difficult examples
for later in the next (and final) subsection.

\bex Here we expand the simple statement $P\longleftrightarrow Q$
using, respectively, the obvious equivalent statement, followed by 
(\ref{AltP->Q}), several applications of the distributive
rule (\ref{LogicDistrib1}),  and finally (\ref{PandnotP}).     
\begin{align*}
P\longleftrightarrow Q&\iff
(P\longrightarrow Q)\wedge(Q\longrightarrow P)\\
&\iff [(\sim P)\vee Q]\wedge[(\sim Q)\vee P]\\
&\iff\left\{\vphantom{\frac11}[(\sim P)\vee Q]
          \wedge(\sim Q)\right\}\bigvee
\left\{\vphantom{\frac11} [(\sim P)\vee Q]\wedge P\right\}\\ 
&\iff\left\{\vphantom{\frac11}[(\sim P)\wedge(\sim Q)]\vee[Q\wedge(\sim Q)]
       \right\}\bigvee
        \left\{\vphantom{\frac11}[(\sim P)\wedge P]\vee[Q\wedge P)]\right\}\\
&\iff \left\{\vphantom{\frac11}[(\sim P)\wedge(\sim Q)]\vee\F\right\}
       \bigvee
        \left\{\vphantom{\frac11}\F\vee[Q\wedge P)]\right\}\\
&\iff \{(\sim P)\wedge(\sim Q)\}\vee\{P\wedge Q\}.
\end{align*} 
Though we had a good understanding of $P\longleftrightarrow Q$ 
before---that it is true if and only if $P$ and $Q$ are both
true or both false---we might not have been as quick to
declare  obvious what we just derived when read backwards:
\begin{equation}
(P\wedge Q)\vee((\sim P)\wedge(\sim Q))\iff P\longleftrightarrow Q.
\end{equation}
\eex

\bex {\bf (A Rather Sophisticated Proof)}\ \  We will prove (\ref{shortchainofimplications}) using
its predecessors, particularly (\ref{extraneous}) and the
distributive laws.  The proof is not as straightforward as
using truth tables, but it is worthwhile to follow along. 
\begin{align*}
P\longrightarrow Q\longrightarrow R
&\iff
(P\longrightarrow Q)\wedge(Q\longrightarrow R)\\
&\iff [(\sim P)\vee Q]\wedge[(\sim Q)\vee R]\\
&\iff \Bigl\{[(\sim P)\vee Q]\wedge(\sim Q)]\Bigr\} 
\vee \Bigl\{[(\sim P)\vee Q]\wedge R\Bigr\}\\ 
&\iff
\Bigl\{[(\sim P)\wedge(\sim Q)]\vee[Q\wedge(\sim Q]\Bigr\} 
\vee \Bigl\{[(\sim P)\vee Q]\wedge R\Bigr\}\\ 
&\iff\Bigl\{[(\sim P)\wedge(\sim Q)]\vee\F\Bigr\}
\vee \Bigl\{[(\sim P)\vee Q]\wedge R\Bigr\}
\\
&\iff\Bigl\{\underbrace{(\sim P)\wedge(\sim Q)}_{A}\Bigr\}
\vee\Bigl\{ 
\underbrace{[(\sim P)\vee Q]}_{B}\wedge \underbrace{R}_{C}\Bigr\}\\
&\iff\Bigl\{\underbrace{[(\sim P)\wedge(\sim Q)}_A]\vee[\underbrace{
(\sim P)\vee Q]}_B\Bigr\}
\wedge\Bigl\{\underbrace{[(\sim P)\wedge(\sim Q)]}_A\vee \underbrace{R}_C
\Bigr\}\\
&\iff (A\vee B)\wedge\Bigl\{\Bigl((\sim P)\vee R\Bigr)\wedge 
\Bigl((\sim Q)\vee R\Bigr)\Bigr\}\\
&\iff\Bigl\{\underbrace{\Bigl(A\vee B\Bigr)\vee\Bigl((\sim Q)\vee R\Bigr)
}_D\Bigr\}
\wedge\Bigl\{(\sim P)\vee R\Bigr\}\\
&\iff \Bigl\{\underbrace{\Bigl(A\vee B\Bigr)\vee\Bigl((\sim Q)\vee R\Bigr)
}_D\Bigr\}\wedge\Bigl\{P\longrightarrow R\Bigr\}
\\&\implies P\longrightarrow R, \text{  q.e.d.} 
\end{align*}  
Here we used some labels and substitutions
$A,B,C,D$ for clarity and  space considerations.
Our final step was of the form $D\wedge(P\longrightarrow R)
\implies P\longrightarrow R$, i.e., (\ref{extraneous}).
Note also that it is enough to have $\implies$ all the way
down in such a proof, since we could then follow the ``arrows'' 
from the first statement through to the last.
However it is best to work with equivalences as long as possible,
to avoid losing too much information in early steps where
the goal is not so easily concluded.
\eex
Clearly this can be a messy process.  Fortunately, we rarely need
to expand things out so far.  What is important is that we now
have a language of logic and some very general 
valid equivalences and implications 
which \begin{enumerate}
\item we can prove using truth tables---a
process with an obvious road map---if necessary;
\item  will become more and more 
natural for us to see as valid by inspection, and be prepared to use; 
\item and (later) will make
shorter, clearer work of our mathematical arguments. 
\end{enumerate} 

\bigskip

Some further observations and insights are introduced in the
exercises.  Many are very important and will be referred to throughout the
text. 







\begin{center}\underline{\Large{\bf Exercises}}\end{center}
\bigskip
\begin{multicols}{2}
\begin{enumerate}

\item Show without truth tables the following.\footnotemark
\hphantom{. }(\underline{Hint}: There is a very easy way
and a very long way.) 
\begin{multline}
(P\longrightarrow Q)\wedge((\sim P)\longrightarrow(\sim Q))
\\ \iff P\longleftrightarrow Q.\end{multline}
Does this make sense?  (You should relate this to English statements
of these forms.)
\label{conversesexercise}
\end{enumerate}
\end{multicols}







